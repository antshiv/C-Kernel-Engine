{
  "_schema_version": "1.0",
  "_description": "C-Kernel-Engine optimization and kernel tracking metadata",
  "_updated": "2025-12-29",

  "_usage": {
    "view_report": "make report",
    "quick_status": "make opt-status",
    "pending_work": "make opt-pending",
    "test_coverage": "make test-coverage",
    "validate_json": "make meta-check",
    "how_to_update": [
      "1. Edit this file when you add/modify a kernel",
      "2. Update 'opt_level' array when adding SIMD/blocking/parallel",
      "3. Update 'status' to 'done' when implementation is complete",
      "4. Add to 'optimizations_pending' for planned improvements",
      "5. Run 'make report' to see updated status"
    ],
    "opt_level_values": ["scalar", "simd_avx", "simd_avx2", "simd_avx512", "simd_avx512bf16", "simd_amx", "blocked", "parallel", "fused"],
    "status_values": ["done", "partial", "not_started"]
  },
  "optimization_levels": {
    "none": "Not implemented",
    "scalar": "Scalar C implementation",
    "simd_avx": "AVX1 vectorized (256-bit, no FMA)",
    "simd_avx2": "AVX2 vectorized (256-bit with FMA)",
    "simd_avx512": "AVX-512 vectorized",
    "simd_avx512bf16": "AVX-512 BF16 native instructions",
    "simd_amx": "Intel AMX (matrix accelerator)",
    "fused": "Fused with adjacent operations",
    "blocked": "Cache-blocked for L1/L2/L3",
    "parallel": "OpenMP multi-threaded"
  },
  "dtypes": {
    "fp32": "32-bit float",
    "bf16": "Brain Float 16",
    "fp16": "IEEE Float 16",
    "int8": "8-bit integer",
    "int4": "4-bit integer",
    "q4_0": "GGML Q4_0 quantization",
    "q4_k": "GGML Q4_K quantization",
    "q8_0": "GGML Q8_0 quantization"
  },
  "kernels": {
    "gemm": {
      "description": "General matrix multiplication C = A @ B.T",
      "category": "compute",
      "critical_path": true,
      "variants": {
        "fp32": {
          "forward": {
            "status": "done",
            "opt_level": [
              "blocked",
              "parallel",
              "simd_avx",
              "simd_avx512"
            ],
            "file": "gemm_kernels.c",
            "notes": "AVX1 fallback (no FMA) for Sandy/Ivy Bridge CPUs"
          },
          "backward_dx": {
            "status": "done",
            "opt_level": [
              "simd_avx",
              "simd_avx512",
              "blocked",
              "parallel"
            ],
            "file": "gemm_kernels.c",
            "func": "gemm_nn_*"
          },
          "backward_dw": {
            "status": "done",
            "opt_level": [
              "simd_avx",
              "simd_avx512",
              "blocked",
              "parallel"
            ],
            "file": "gemm_kernels.c",
            "func": "gemm_tn_*"
          }
        },
        "bf16": {
          "forward": {
            "status": "done",
            "opt_level": [
              "blocked",
              "parallel",
              "simd_avx2",
              "simd_avx512",
              "simd_avx512bf16"
            ],
            "file": "gemm_kernels_bf16.c"
          },
          "backward_dx": {
            "status": "done",
            "opt_level": [
              "simd_avx512"
            ],
            "file": "gemm_kernels_bf16.c",
            "func": "gemm_nn_bf16"
          },
          "backward_dw": {
            "status": "done",
            "opt_level": [
              "simd_avx512"
            ],
            "file": "gemm_kernels_bf16.c",
            "func": "gemm_tn_bf16"
          }
        },
        "fp16": {
          "forward": {
            "status": "done",
            "opt_level": [
              "simd_avx2",
              "simd_avx512"
            ],
            "file": "gemm_kernels_f16.c"
          },
          "backward_dx": {
            "status": "done",
            "opt_level": [
              "scalar"
            ],
            "file": "gemm_kernels_f16.c"
          }
        },
        "q4_0": {
          "forward": {
            "status": "done",
            "opt_level": [
              "blocked",
              "simd_avx512"
            ],
            "file": "gemm_kernels_q4_0.c"
          },
          "backward_dx": {
            "status": "done",
            "opt_level": [
              "scalar"
            ],
            "file": "gemm_kernels_q4_0.c"
          }
        },
        "q4_k": {
          "forward": {
            "status": "done",
            "opt_level": [
              "blocked",
              "simd_avx512"
            ],
            "file": "gemm_kernels_q4k.c"
          },
          "backward_dx": {
            "status": "done",
            "opt_level": [
              "simd_avx512"
            ],
            "file": "gemm_kernels_q4k.c"
          }
        },
        "q8_0": {
          "forward": {
            "status": "done",
            "opt_level": [
              "blocked",
              "simd_avx512"
            ],
            "file": "gemm_kernels_q8_0.c"
          },
          "backward_dx": {
            "status": "done",
            "opt_level": [
              "scalar"
            ],
            "file": "gemm_kernels_q8_0.c"
          }
        }
      },
      "tests": {
        "reference": "PyTorch torch.mm",
        "test_files": [
          "unittest/test_gemm.py",
          "unittest/test_quant_kernels.py"
        ]
      },
      "optimizations_pending": [
        {
          "name": "amx_bf16",
          "description": "Intel AMX for BF16 GEMM",
          "priority": "high"
        },
        {
          "name": "pack_matrix",
          "description": "Pack B matrix for better cache access",
          "priority": "medium"
        }
      ]
    },
    "gemm_microkernel": {
      "description": "High-performance GEMM with 8x8 register-blocked microkernels (oneDNN/BLIS style)",
      "category": "compute",
      "critical_path": true,
      "variants": {
        "fp32": {
          "forward": {
            "status": "done",
            "opt_level": ["simd_avx", "simd_avx2", "simd_avx512", "blocked"],
            "file": "gemm_microkernel.c",
            "funcs": ["gemm_microkernel", "gemm_microkernel_blocked", "gemm_microkernel_blocked_bt"],
            "notes": "8x8 AVX1 (mul+add), 8x8 AVX2/FMA, 8x16 AVX-512. Cache blocking: MC=64, NC=256, KC=256"
          }
        }
      },
      "tests": {
        "reference": "PyTorch torch.mm",
        "test_files": ["unittest/test_gemm_microkernel.py"]
      },
      "performance": {
        "speedup_vs_naive": "4.31x average",
        "speedup_vs_pytorch": "1.44x FASTER than MKL",
        "gflops_1024x1024": "31.48 GFLOPS (AVX1 + packing)",
        "gflops_512x512": "24.22 GFLOPS (AVX1 + packing)",
        "accuracy": "max_diff < 1e-04"
      },
      "optimizations_done": [
        "8x8 register blocking",
        "Matrix packing (A and B)",
        "Software prefetching",
        "K-loop unrolling by 4"
      ],
      "optimizations_pending": [
        {
          "name": "avx2_fma_packed",
          "description": "FMA version of packed microkernel",
          "priority": "medium"
        }
      ]
    },
    "gemm_fused": {
      "description": "Fused GEMM operations (GEMM+activation, SwiGLU MLP)",
      "category": "compute",
      "critical_path": true,
      "variants": {
        "fp32": {
          "forward": {
            "status": "done",
            "opt_level": ["simd_avx", "simd_avx512", "parallel", "fused"],
            "file": "gemm_fused_kernels.c",
            "funcs": ["gemm_bias_relu_fused", "gemm_bias_gelu_fused", "gemm_bias_silu_fused", "gemm_swiglu_fused"],
            "accuracy": {
              "relu": "3e-05 max_diff",
              "silu": "3e-05 max_diff",
              "swiglu": "1e-03 max_diff (2 GEMMs)",
              "gelu": "2e-02 max_diff (QuickGELU approx)"
            }
          }
        }
      },
      "notes": "Based on oneDNN post-ops pattern. gemm_swiglu_fused computes TWO GEMMs + SwiGLU in one pass (LLaMA MLP style). Intermediates stay in registers, avoiding DRAM round-trips."
    },
    "attention": {
      "description": "Multi-head attention with causal mask",
      "category": "compute",
      "critical_path": true,
      "variants": {
        "fp32": {
          "forward": {
            "status": "done",
            "opt_level": [
              "scalar",
              "parallel"
            ],
            "file": "attention_kernels.c",
            "funcs": [
              "attention_forward_causal_head_major_gqa",
              "attention_forward_causal_head_major_gqa_flash",
              "attention_forward_decode_head_major_gqa_flash"
            ],
            "notes": "Flash attention with online softmax (O(1) memory per query). GQA support. KV cache decode path with OpenMP."
          },
          "backward": {
            "status": "done",
            "opt_level": [
              "scalar"
            ],
            "file": "attention_kernels.c"
          }
        },
        "bf16": {
          "forward": {
            "status": "done",
            "opt_level": [
              "scalar"
            ],
            "file": "attention_kernels.c",
            "func": "*_bf16"
          },
          "backward": {
            "status": "done",
            "opt_level": [
              "scalar"
            ],
            "file": "attention_kernels.c"
          }
        }
      },
      "tests": {
        "reference": "PyTorch F.scaled_dot_product_attention",
        "test_files": [
          "unittest/bf16/test_attention_bf16.py"
        ]
      },
      "optimizations_done": [
        {
          "name": "flash_attention",
          "description": "Online softmax algorithm - O(1) memory per query, no TÃ—T matrix"
        },
        {
          "name": "kv_cache",
          "description": "KV cache for autoregressive decoding (head-major layout)"
        },
        {
          "name": "gqa",
          "description": "Grouped Query Attention support"
        }
      ],
      "optimizations_pending": [
        {
          "name": "simd_flash",
          "description": "SIMD vectorize the flash attention inner loops (dot product, output accumulation)",
          "priority": "high"
        },
        {
          "name": "paged_attention",
          "description": "Paged KV cache for long sequences",
          "priority": "medium"
        }
      ]
    },
    "rmsnorm": {
      "description": "Root Mean Square Layer Normalization",
      "category": "normalization",
      "critical_path": true,
      "variants": {
        "fp32": {
          "forward": {
            "status": "done",
            "opt_level": [
              "simd_avx",
              "simd_avx512"
            ],
            "file": "rmsnorm_kernels.c",
            "notes": "AVX-512 with FMA, AVX fallback. Vectorized sum-of-squares reduction and normalize."
          },
          "backward": {
            "status": "done",
            "opt_level": [
              "simd_avx",
              "simd_avx512"
            ],
            "file": "rmsnorm_kernels.c",
            "notes": "AVX-512 with FMA, AVX fallback. Vectorized gradient computation."
          }
        },
        "bf16": {
          "forward": {
            "status": "done",
            "opt_level": [
              "simd_avx512"
            ],
            "file": "rmsnorm_kernels_bf16.c",
            "notes": "AVX-512 with BF16<->FP32 conversion. Compute in FP32 for precision."
          },
          "backward": {
            "status": "done",
            "opt_level": [
              "simd_avx512"
            ],
            "file": "rmsnorm_kernels_bf16.c",
            "notes": "AVX-512 with BF16<->FP32 conversion."
          }
        },
        "int8": {
          "forward": {
            "status": "done",
            "opt_level": [
              "scalar"
            ],
            "file": "rmsnorm_kernels_int8.c"
          },
          "backward": {
            "status": "done",
            "opt_level": [
              "scalar"
            ],
            "file": "rmsnorm_kernels_int8.c"
          }
        },
        "int4": {
          "forward": {
            "status": "done",
            "opt_level": [
              "scalar"
            ],
            "file": "rmsnorm_kernels_int4.c"
          },
          "backward": {
            "status": "done",
            "opt_level": [
              "scalar"
            ],
            "file": "rmsnorm_kernels_int4.c"
          }
        }
      },
      "tests": {
        "reference": "PyTorch custom rmsnorm",
        "test_files": [
          "unittest/bf16/test_rmsnorm_bf16.py"
        ]
      },
      "optimizations_pending": [
        {
          "name": "fused_residual",
          "description": "Fuse with residual add",
          "priority": "medium"
        }
      ]
    },
    "layernorm": {
      "description": "Layer Normalization",
      "category": "normalization",
      "variants": {
        "fp32": {
          "forward": {
            "status": "done",
            "opt_level": [
              "simd_avx2",
              "simd_avx512"
            ],
            "file": "layernorm_kernels.c"
          },
          "backward": {
            "status": "done",
            "opt_level": [
              "parallel"
            ],
            "file": "layernorm_kernels.c"
          }
        },
        "bf16": {
          "forward": {
            "status": "done",
            "opt_level": [
              "scalar"
            ],
            "file": "layernorm_kernels_bf16.c"
          },
          "backward": {
            "status": "done",
            "opt_level": [
              "scalar"
            ],
            "file": "layernorm_kernels_bf16.c"
          }
        }
      },
      "tests": {
        "reference": "PyTorch nn.LayerNorm",
        "test_files": []
      }
    },
    "softmax": {
      "description": "Softmax activation",
      "category": "activation",
      "variants": {
        "fp32": {
          "forward": {
            "status": "done",
            "opt_level": [
              "simd_avx",
              "simd_avx2",
              "simd_avx512"
            ],
            "file": "softmax_kernels.c",
            "notes": "Causal softmax. AVX-512/AVX2: fast vectorized exp. AVX1: vectorized max/normalize, scalar exp."
          },
          "backward": {
            "status": "done",
            "opt_level": [
              "simd_avx",
              "simd_avx512"
            ],
            "file": "softmax_kernels.c",
            "notes": "Vectorized dot product and gradient computation."
          }
        },
        "bf16": {
          "forward": {
            "status": "done",
            "opt_level": [
              "simd_avx512"
            ],
            "file": "softmax_kernels_bf16.c",
            "notes": "AVX-512 vectorized BF16<->FP32 conversion, delegates to FP32 SIMD kernel."
          },
          "backward": {
            "status": "done",
            "opt_level": [
              "simd_avx512"
            ],
            "file": "softmax_kernels_bf16.c",
            "notes": "AVX-512 vectorized BF16<->FP32 conversion."
          }
        }
      },
      "tests": {
        "reference": "PyTorch F.softmax",
        "test_files": []
      },
      "optimizations_pending": []
    },
    "gelu": {
      "description": "GELU activation",
      "category": "activation",
      "variants": {
        "fp32": {
          "forward": {
            "status": "done",
            "opt_level": [
              "simd_avx",
              "simd_avx2",
              "simd_avx512"
            ],
            "file": "gelu_kernels.c",
            "notes": "AVX-512/AVX2 with fast tanh. AVX1 uses scalar tanh with vectorized arithmetic."
          },
          "backward": {
            "status": "done",
            "opt_level": [
              "simd_avx",
              "simd_avx2",
              "simd_avx512"
            ],
            "file": "gelu_kernels.c",
            "notes": "Both exact and fast backward variants. AVX1 uses scalar exp/tanh with vectorized arithmetic."
          }
        },
        "bf16": {
          "forward": {
            "status": "done",
            "opt_level": [
              "simd_avx512"
            ],
            "file": "gelu_kernels_bf16.c",
            "notes": "AVX-512 vectorized BF16<->FP32 conversion, delegates to FP32 SIMD kernel."
          },
          "backward": {
            "status": "done",
            "opt_level": [
              "simd_avx512"
            ],
            "file": "gelu_kernels_bf16.c",
            "notes": "AVX-512 vectorized BF16<->FP32 conversion."
          }
        }
      },
      "tests": {
        "reference": "PyTorch F.gelu",
        "test_files": []
      }
    },
    "swiglu": {
      "description": "SwiGLU activation (Swish-Gated Linear Unit)",
      "category": "activation",
      "variants": {
        "fp32": {
          "forward": {
            "status": "done",
            "opt_level": [
              "simd_avx",
              "simd_avx2",
              "simd_avx512"
            ],
            "file": "swiglu_kernels.c",
            "notes": "AVX-512/AVX2 with fast sigmoid. AVX1 uses scalar sigmoid with vectorized arithmetic."
          },
          "backward": {
            "status": "done",
            "opt_level": [
              "simd_avx",
              "simd_avx2",
              "simd_avx512"
            ],
            "file": "swiglu_kernels.c",
            "notes": "AVX-512/AVX2 with FMA. AVX1 uses scalar sigmoid with vectorized arithmetic."
          }
        },
        "bf16": {
          "forward": {
            "status": "done",
            "opt_level": [
              "simd_avx512"
            ],
            "file": "swiglu_kernels_bf16.c",
            "notes": "AVX-512 with fast sigmoid, BF16<->FP32 conversion inline."
          },
          "backward": {
            "status": "done",
            "opt_level": [
              "simd_avx512"
            ],
            "file": "swiglu_kernels_bf16.c",
            "notes": "AVX-512 with fast sigmoid, BF16<->FP32 conversion inline."
          }
        }
      },
      "tests": {
        "reference": "PyTorch custom swiglu",
        "test_files": [
          "unittest/test_swiglu.py"
        ]
      }
    },
    "rope": {
      "description": "Rotary Position Embedding",
      "category": "embedding",
      "critical_path": true,
      "variants": {
        "fp32": {
          "forward": {
            "status": "done",
            "opt_level": [
              "simd_avx",
              "simd_avx512"
            ],
            "file": "rope_kernels.c",
            "notes": "AVX-512 with FMA, AVX fallback. Processes 16/8 rotation pairs at a time."
          },
          "backward": {
            "status": "done",
            "opt_level": [
              "simd_avx",
              "simd_avx512"
            ],
            "file": "rope_kernels.c",
            "notes": "AVX-512 with FMA, AVX fallback. Vectorized inverse rotation."
          }
        },
        "bf16": {
          "forward": {
            "status": "done",
            "opt_level": [
              "simd_avx512"
            ],
            "file": "rope_kernels_bf16.c",
            "notes": "AVX-512 vectorized BF16<->FP32 conversion, delegates to FP32 SIMD kernel."
          },
          "backward": {
            "status": "done",
            "opt_level": [
              "simd_avx512"
            ],
            "file": "rope_kernels_bf16.c",
            "notes": "AVX-512 vectorized BF16<->FP32 conversion."
          }
        }
      },
      "tests": {
        "reference": "HuggingFace transformers RoPE",
        "test_files": []
      },
      "optimizations_pending": [
        {
          "name": "fused_qk",
          "description": "Fuse RoPE into Q/K projection",
          "priority": "low"
        }
      ]
    },
    "embedding": {
      "description": "Token embedding lookup",
      "category": "embedding",
      "variants": {
        "fp32": {
          "forward": {
            "status": "done",
            "opt_level": [
              "scalar"
            ],
            "file": "embedding_kernels.c"
          },
          "backward": {
            "status": "done",
            "opt_level": [
              "scalar"
            ],
            "file": "embedding_kernels.c"
          }
        },
        "bf16": {
          "forward": {
            "status": "done",
            "opt_level": [
              "scalar"
            ],
            "file": "embedding_kernels_bf16.c"
          },
          "backward": {
            "status": "done",
            "opt_level": [
              "scalar"
            ],
            "file": "embedding_kernels_bf16.c"
          }
        }
      },
      "tests": {
        "reference": "PyTorch nn.Embedding",
        "test_files": []
      }
    },
    "mlp": {
      "description": "MLP block (FC1 -> activation -> FC2)",
      "category": "compute",
      "critical_path": true,
      "variants": {
        "fp32": {
          "forward": {
            "status": "done",
            "opt_level": [
              "blocked",
              "parallel",
              "simd_avx512"
            ],
            "file": "mlp_kernels.c"
          },
          "backward": {
            "status": "done",
            "opt_level": [
              "blocked",
              "parallel",
              "simd_avx512"
            ],
            "file": "mlp_kernels.c"
          }
        },
        "bf16": {
          "forward": {
            "status": "done",
            "opt_level": [
              "parallel",
              "simd_avx2",
              "simd_avx512"
            ],
            "file": "mlp_kernels_bf16.c"
          },
          "backward": {
            "status": "pending",
            "opt_level": [],
            "file": "mlp_kernels_bf16.c"
          }
        }
      },
      "tests": {
        "reference": "PyTorch nn.Linear + GELU + nn.Linear",
        "test_files": [
          "unittest/bf16/test_mlp_bf16.py"
        ]
      },
      "optimizations_pending": [
        {
          "name": "fused_gelu",
          "description": "Fuse GELU into GEMM epilogue",
          "priority": "high"
        }
      ]
    },
    "sigmoid": {
      "description": "Sigmoid activation",
      "category": "activation",
      "variants": {
        "fp32": {
          "forward": {
            "status": "done",
            "opt_level": [
              "simd_avx512"
            ],
            "file": "sigmoid_kernels.c"
          },
          "backward": {
            "status": "done",
            "opt_level": [
              "simd_avx512"
            ],
            "file": "sigmoid_kernels.c"
          }
        },
        "bf16": {
          "forward": {
            "status": "done",
            "opt_level": [
              "scalar"
            ],
            "file": "sigmoid_kernels_bf16.c"
          },
          "backward": {
            "status": "done",
            "opt_level": [
              "scalar"
            ],
            "file": "sigmoid_kernels_bf16.c"
          }
        }
      },
      "tests": {
        "reference": "PyTorch torch.sigmoid",
        "test_files": [
          "unittest/bf16/test_sigmoid_bf16.py"
        ]
      }
    },
    "relu": {
      "description": "ReLU activation",
      "category": "activation",
      "variants": {
        "fp32": {
          "forward": {
            "status": "done",
            "opt_level": [
              "simd_avx2",
              "simd_avx512"
            ],
            "file": "relu_kernels.c"
          },
          "backward": {
            "status": "done",
            "opt_level": [
              "simd_avx2",
              "simd_avx512"
            ],
            "file": "relu_kernels.c"
          }
        },
        "bf16": {
          "forward": {
            "status": "done",
            "opt_level": [
              "scalar"
            ],
            "file": "relu_kernels_bf16.c"
          },
          "backward": {
            "status": "done",
            "opt_level": [
              "scalar"
            ],
            "file": "relu_kernels_bf16.c"
          }
        }
      },
      "tests": {
        "reference": "PyTorch F.relu",
        "test_files": []
      }
    },
    "dequant": {
      "description": "Dequantization (INT4/INT8 -> FP32)",
      "category": "quantization",
      "variants": {
        "q4_0": {
          "forward": {
            "status": "done",
            "opt_level": [
              "simd_avx512"
            ],
            "file": "dequant_kernels.c"
          }
        },
        "q4_k": {
          "forward": {
            "status": "done",
            "opt_level": [
              "simd_avx512"
            ],
            "file": "dequant_kernels.c"
          }
        },
        "q8_0": {
          "forward": {
            "status": "done",
            "opt_level": [
              "simd_avx512"
            ],
            "file": "dequant_kernels.c"
          }
        }
      },
      "tests": {
        "reference": "llama.cpp dequant",
        "test_files": [
          "unittest/test_quant_kernels.py",
          "unittest/test_q4_k_quantize.py"
        ]
      }
    },
    "vision": {
      "description": "Vision encoder operations (patch embed, etc.)",
      "category": "vision",
      "variants": {
        "fp32": {
          "forward": {
            "status": "done",
            "opt_level": [
              "scalar"
            ],
            "file": "vision_kernels.c"
          }
        },
        "bf16": {
          "forward": {
            "status": "done",
            "opt_level": [
              "scalar"
            ],
            "file": "vision_kernels_bf16.c"
          }
        }
      },
      "tests": {
        "reference": "HuggingFace ViT",
        "test_files": []
      }
    }
  },
  "inference_optimizations": {
    "prefill": {
      "description": "Process full prompt in parallel (compute-bound)",
      "status": "done",
      "details": "GEMM parallelization with OpenMP, flash attention for memory efficiency",
      "priority": "critical",
      "depends_on": [
        "flash_attention"
      ]
    },
    "decode": {
      "description": "Autoregressive token generation (memory-bound)",
      "status": "done",
      "details": "38 tok/s on SmolLM-135M with parallel GEMV and KV cache (2x speedup from threading)",
      "priority": "critical",
      "depends_on": [
        "kv_cache",
        "gemv_optimized"
      ]
    },
    "kv_cache": {
      "description": "Cache key/value tensors for decode phase",
      "status": "done",
      "details": "Implemented in attention_kernels.c with head-major layout",
      "priority": "critical",
      "depends_on": []
    },
    "flash_attention": {
      "description": "Tiled attention with fused softmax (O(N) memory)",
      "status": "done",
      "details": "Online softmax in attention_forward_decode_head_major_gqa_flash()",
      "priority": "critical",
      "depends_on": [
        "online_softmax"
      ]
    },
    "online_softmax": {
      "description": "Streaming softmax without materializing full attention matrix",
      "status": "done",
      "details": "Integrated into flash attention decode path",
      "priority": "high",
      "depends_on": []
    },
    "multi_threading": {
      "description": "OpenMP + MKL multi-threading with auto-detection",
      "status": "done",
      "details": "ck_set_num_threads() API, auto-detects physical cores, parallel GEMV for decode",
      "priority": "critical",
      "depends_on": []
    },
    "gemv_optimized": {
      "description": "Parallel matrix-vector for decode (M=1)",
      "status": "done",
      "details": "gemm_nt_matvec_parallel() with AVX512, parallelizes over output channels",
      "priority": "critical",
      "depends_on": [
        "multi_threading"
      ]
    },
    "continuous_batching": {
      "description": "Dynamic batching for variable-length sequences",
      "status": "not_started",
      "details": "Requires paged attention and scheduler",
      "priority": "medium",
      "depends_on": [
        "paged_attention"
      ]
    },
    "paged_attention": {
      "description": "Paged KV cache for efficient memory management",
      "status": "not_started",
      "details": "vLLM-style paged attention",
      "priority": "medium",
      "depends_on": [
        "kv_cache"
      ]
    },
    "speculative_decoding": {
      "description": "Use small model to draft, large model to verify",
      "status": "not_started",
      "details": "Requires draft model support",
      "priority": "low",
      "depends_on": []
    },
    "quantized_kv_cache": {
      "description": "Store KV cache in INT8/FP8 to save memory",
      "status": "not_started",
      "details": "Reduces memory bandwidth during decode",
      "priority": "medium",
      "depends_on": [
        "kv_cache"
      ]
    }
  },
  "training_optimizations": {
    "gradient_checkpointing": {
      "description": "Recompute activations during backward to save memory",
      "status": "not_started",
      "priority": "medium"
    },
    "fused_adam": {
      "description": "Fused Adam optimizer update",
      "status": "not_started",
      "priority": "medium"
    },
    "mixed_precision": {
      "description": "FP16/BF16 forward, FP32 master weights",
      "status": "partial",
      "details": "BF16 kernels exist, need loss scaling",
      "priority": "high"
    },
    "gradient_accumulation": {
      "description": "Accumulate gradients over micro-batches",
      "status": "not_started",
      "priority": "low"
    }
  },
  "single_core_optimizations": {
    "description": "Optimizations to maximize single-core performance before going parallel",
    "status_summary": {
      "gemm": "blocked + AVX-512",
      "attention": "needs work",
      "activation": "mostly scalar",
      "normalization": "mostly scalar"
    },
    "priorities": [
      {
        "name": "gemm_microkernel",
        "description": "8x8 register-blocked GEMM microkernel with cache blocking",
        "status": "done",
        "file": "gemm_microkernel.c",
        "speedup": "2.04x vs naive"
      },
      {
        "name": "cache_blocking",
        "description": "Tune block sizes for L1/L2/L3",
        "status": "partial"
      },
      {
        "name": "prefetch",
        "description": "Software prefetch for memory access",
        "status": "not_started"
      },
      {
        "name": "branch_elimination",
        "description": "Remove branches in hot loops",
        "status": "not_started"
      },
      {
        "name": "simd_everywhere",
        "description": "Vectorize all scalar kernels",
        "status": "partial"
      }
    ]
  },
  "performance_targets": {
    "gemm_fp32": {
      "metric": "GFLOPS",
      "target": "80% of peak",
      "current": "~40% estimated",
      "reference": "oneDNN"
    },
    "gemm_bf16": {
      "metric": "GFLOPS",
      "target": "80% of peak (2x FP32 with BF16 instructions)",
      "current": "unknown",
      "reference": "oneDNN"
    },
    "attention": {
      "metric": "tokens/sec",
      "target": "match FlashAttention",
      "current": "naive O(N^2) memory",
      "reference": "FlashAttention-2"
    },
    "inference_latency": {
      "metric": "ms/token",
      "target": "competitive with llama.cpp",
      "current": "not measured",
      "reference": "llama.cpp"
    }
  },
  "test_references": {
    "pytorch": "Primary reference for correctness",
    "onednn": "Reference for GEMM/convolution performance",
    "llama_cpp": "Reference for quantized inference",
    "flash_attention": "Reference for attention performance",
    "huggingface": "Reference for model-level correctness"
  }
}
