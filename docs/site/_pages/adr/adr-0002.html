<!-- TITLE: ADR-0002 Templates are Canonical -->
<!-- NAV: adr -->

<h1>ADR-0002: Templates are Canonical, IR is Compiled Output</h1>

<div class="card">
    <table>
        <tr><td><strong>Status</strong></td><td><span class="badge badge-success">Accepted</span></td></tr>
        <tr><td><strong>Date</strong></td><td>2025-01</td></tr>
        <tr><td><strong>Supersedes</strong></td><td>IR v3 hardcoded architecture in build_ir_v3.py</td></tr>
    </table>
</div>

<h2>Context</h2>

<p>In IR v3, the model architecture knowledge was embedded directly in Python code (<code>build_layer_layout()</code>).
This made it difficult to:</p>

<ul>
    <li>Add support for new architectures without modifying core codegen</li>
    <li>Understand what a model consists of at a glance</li>
    <li>Validate that our implementation matches HuggingFace</li>
    <li>Share architecture definitions with other tools</li>
</ul>

<p>We needed a single source of truth for model architectures that is human-readable and machine-parseable.</p>

<h2>Decision</h2>

<p><strong>Templates are the canonical definition of model architectures.</strong> The IR is a compiled artifact derived from templates + config.json.</p>

<h3>Template Format (YAML)</h3>

<p>Each supported architecture has a YAML template in <code>templates/</code>:</p>

<pre><code class="language-yaml">
# templates/qwen2.yaml
name: qwen2
version: 1
description: "Qwen2 decoder-only transformer"

# Map HuggingFace config.json keys to our internal names
config_mapping:
  hidden_size: embed_dim
  num_attention_heads: num_heads
  num_key_value_heads: num_kv_heads
  intermediate_size: mlp_intermediate
  num_hidden_layers: num_layers
  vocab_size: vocab_size
  rms_norm_eps: rms_eps
  rope_theta: rope_theta

# Architecture graph (executed in order)
layers:
  - name: embed
    op: Embed
    input: token_ids
    output: hidden_states
    params:
      vocab_size: "{{vocab_size}}"
      embed_dim: "{{embed_dim}}"

  - name: decoder_layers
    repeat: "{{num_layers}}"
    ops:
      - name: pre_attn_norm
        op: RMSNorm
        input: "{{prev_output}}"
        output: normed
        params:
          eps: "{{rms_eps}}"

      - name: qkv_proj
        op: GEMM
        input: normed
        outputs: [q, k, v]
        params:
          weights: [wq, wk, wv]
          num_heads: "{{num_heads}}"
          num_kv_heads: "{{num_kv_heads}}"

      - name: rope
        op: RoPE
        inputs: [q, k]
        outputs: [q_rope, k_rope]
        params:
          head_dim: "{{head_dim}}"
          theta: "{{rope_theta}}"

      - name: attention
        op: Attention
        inputs: [q_rope, k_rope, v]
        output: attn_out
        params:
          causal: true
          gqa: "{{num_kv_heads}} < {{num_heads}}"

      - name: o_proj
        op: GEMM
        input: attn_out
        output: proj_out

      - name: residual_1
        op: Add
        inputs: ["{{layer_input}}", proj_out]
        output: residual_1

      - name: post_attn_norm
        op: RMSNorm
        input: residual_1
        output: normed_2

      - name: mlp
        op: SwiGLU
        input: normed_2
        output: mlp_out
        params:
          intermediate: "{{mlp_intermediate}}"

      - name: residual_2
        op: Add
        inputs: [residual_1, mlp_out]
        output: layer_output

  - name: final_norm
    op: RMSNorm
    input: "{{last_layer_output}}"
    output: final_normed

  - name: lm_head
    op: GEMM
    input: final_normed
    output: logits
    params:
      weight_tied: embed  # Tie to embedding weights
</code></pre>

<h3>Weight Name Mapping</h3>

<p>Templates include a mapping from HuggingFace weight names to our internal names:</p>

<pre><code class="language-yaml">
# Weight name mapping (HF â†’ internal)
weight_mapping:
  "model.embed_tokens.weight": "embed.weight"
  "model.layers.{i}.self_attn.q_proj.weight": "layers.{i}.attn.wq"
  "model.layers.{i}.self_attn.k_proj.weight": "layers.{i}.attn.wk"
  "model.layers.{i}.self_attn.v_proj.weight": "layers.{i}.attn.wv"
  "model.layers.{i}.self_attn.o_proj.weight": "layers.{i}.attn.wo"
  "model.layers.{i}.mlp.gate_proj.weight": "layers.{i}.mlp.gate"
  "model.layers.{i}.mlp.up_proj.weight": "layers.{i}.mlp.up"
  "model.layers.{i}.mlp.down_proj.weight": "layers.{i}.mlp.down"
  "model.layers.{i}.input_layernorm.weight": "layers.{i}.ln1.gamma"
  "model.layers.{i}.post_attention_layernorm.weight": "layers.{i}.ln2.gamma"
  "model.norm.weight": "final_norm.gamma"
  "lm_head.weight": "lm_head.weight"
</code></pre>

<h3>Template Discovery</h3>

<p>The codegen tool auto-selects templates based on <code>config.json</code>:</p>

<pre><code class="language-python">
def select_template(config):
    arch = config.get("model_type", "").lower()
    if arch in ["qwen2", "qwen2.5"]:
        return "templates/qwen2.yaml"
    elif arch in ["llama", "llama2", "llama3"]:
        return "templates/llama.yaml"
    elif arch == "mistral":
        return "templates/mistral.yaml"
    else:
        raise ValueError(f"Unknown architecture: {arch}")
</code></pre>

<h2>Consequences</h2>

<h3>Benefits</h3>
<ul>
    <li><strong>Human-readable</strong>: Model architecture visible at a glance</li>
    <li><strong>Single source of truth</strong>: No duplicate architecture logic</li>
    <li><strong>Easy to extend</strong>: New architectures = new YAML file</li>
    <li><strong>Validation</strong>: Can diff templates against HF implementations</li>
    <li><strong>Weight mapping</strong>: Clear connection between HF names and internal names</li>
</ul>

<h3>Costs</h3>
<ul>
    <li>YAML parsing adds a dependency</li>
    <li>Template syntax needs to be documented and learned</li>
    <li>Template validation logic required</li>
</ul>

<h2>Related ADRs</h2>
<ul>
    <li><a href="adr-0001.html">ADR-0001</a>: Templates generate Graph IR</li>
    <li><a href="adr-0006.html">ADR-0006</a>: Weight mapping from HF names</li>
</ul>
