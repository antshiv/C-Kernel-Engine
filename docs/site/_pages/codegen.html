<!-- TITLE: Code Generation -->
<!-- NAV: codegen -->

<h1>Code Generation</h1>

<p>C-Kernel-Engine can generate optimized C runtime code from HuggingFace model configs. This enables running models with zero Python overhead.</p>

<div class="img-container">
    <img src="assets/architecture-overview.svg" alt="Code Generation Pipeline">
</div>

<h2>Pipeline Overview</h2>

<div class="grid grid-3">
    <div class="card card-accent">
        <h3 style="margin-top: 0;">1. Config Parsing</h3>
        <p>Load a HuggingFace-style <code>config.json</code> and extract model dimensions.</p>
        <pre>ck_model_config_from_hf_json(
    "config.json",
    &cfg
);</pre>
    </div>
    <div class="card">
        <h3 style="margin-top: 0;">2. IR Building</h3>
        <p>Construct an intermediate representation graph of all operations.</p>
        <pre>ck_build_decoder_ir(
    &cfg,
    &forward_graph
);
ck_build_decoder_backward_ir(
    &forward_graph,
    &backward_graph
);</pre>
    </div>
    <div class="card card-green">
        <h3 style="margin-top: 0;">3. Code Emission</h3>
        <p>Generate complete C code with forward and backward passes.</p>
        <pre>ck_codegen_emit_runtime(
    &forward_graph,
    "model.c"
);</pre>
    </div>
</div>

<h2>Quick Start</h2>

<div class="card card-accent">
    <h3 style="margin-top: 0;">Generate from Config</h3>
    <pre># Build the codegen tool
make build/ck_ir_demo

# Generate C runtime from a HuggingFace config
./build/ck_ir_demo path/to/config.json --emit build/model.c

# Or use the make target
make ck-emit CONFIG=path/to/config.json OUT=build/model.c</pre>
</div>

<div class="card">
    <h3 style="margin-top: 0;">Library Mode (Prefill + Decode)</h3>
    <p>Library mode emits a generated runtime with an exported ABI suitable for <code>dlopen</code>/<code>ctypes</code>.</p>
    <pre># Emit a model runtime with exported symbols (prefill + KV-cache decode + training entry points)
./build/ck_ir_demo path/to/config.json --emit build/model.c --emit-lib

# The generator also writes build/model.c.kernels (one kernel .c per line)
cc -O3 -fPIC -fopenmp -shared -Iinclude -o build/libmodel.so build/model.c $(cat build/model.c.kernels) -lm</pre>
    <p><strong>Inference sequence:</strong> enable KV cache → prefill once → decode token-by-token. Decode is disabled when training is enabled.</p>
</div>

<h2>Input: Model Config</h2>

<div class="card">
    <h3 style="margin-top: 0;">HuggingFace config.json</h3>
    <p>The codegen accepts standard HuggingFace model configurations:</p>
    <pre>{
  "architectures": ["LlamaForCausalLM"],
  "hidden_size": 768,
  "num_attention_heads": 12,
  "num_key_value_heads": 4,
  "num_hidden_layers": 6,
  "intermediate_size": 2048,
  "vocab_size": 32000,
  "max_position_embeddings": 2048,
  "rms_norm_eps": 1e-5,
  "rope_theta": 10000.0
}</pre>
</div>

<div class="card">
    <h3 style="margin-top: 0;">Parsed CKModelConfig</h3>
    <pre>typedef struct {
    int hidden_size;           // 768
    int num_attention_heads;   // 12
    int num_key_value_heads;   // 4 (for GQA)
    int num_hidden_layers;     // 6
    int intermediate_size;     // 2048
    int vocab_size;            // 32000
    int max_position_embeddings; // 2048
    float rms_norm_eps;        // 1e-5
    float rope_theta;          // 10000.0
} CKModelConfig;</pre>
</div>

<h2>Kernel Map: Single Source of Truth</h2>

<div class="alert alert-info">
    <div class="alert-icon">
        <svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><circle cx="12" cy="12" r="10"/><line x1="12" y1="16" x2="12" y2="12"/><line x1="12" y1="8" x2="12.01" y2="8"/></svg>
    </div>
    <div>
        <strong>Why a Kernel Map?</strong><br>
        Every kernel is tested individually with PyTorch parity. The codegen uses the <em>exact same kernels</em> that passed tests. No drift, no duplication.
    </div>
</div>

<div class="card card-accent">
    <h3 style="margin-top: 0;">CKKernelSpec: Kernel Registry</h3>
    <p>Each kernel maps to its tested C implementation:</p>
    <pre>const CKKernelSpec ck_kernel_specs[] = {
    {"rmsnorm",   "rmsnorm_forward",   "rmsnorm_backward",
     {"src/kernels/rmsnorm_kernels.c", NULL, ...}},

    {"attention", "attention_forward_causal_head_major_gqa",
                  "attention_backward_causal_head_major_gqa",
     {"src/kernels/attention_kernels.c",
      "src/kernels/softmax_kernels.c", NULL, ...}},

    {"swiglu",    "swiglu_forward",    "swiglu_backward",
     {"src/kernels/swiglu_kernels.c",
      "src/kernels/sigmoid_kernels.c", NULL, ...}},
};</pre>
</div>

<div class="card">
    <h3 style="margin-top: 0;">CKBufferSpec: Buffer Definitions</h3>
    <p>All buffers with symbolic dimensions, roles, and conditions:</p>
    <pre>const CKBufferSpec ck_decoder_buffers[] = {
    // Global buffers
    {"token_emb", CK_SCOPE_GLOBAL, CK_ROLE_WEIGHT,
     {{CK_DIM_VOCAB, 1, 1}, {CK_DIM_ALIGNED_EMBED, 1, 1}, ...}},

    {"lm_head_weight", CK_SCOPE_GLOBAL, CK_ROLE_WEIGHT,
     {{CK_DIM_VOCAB, 1, 1}, {CK_DIM_ALIGNED_EMBED, 1, 1}, ...},
     .alias_of = "token_emb"},  // tied embeddings

    {"rope_cos_cache", CK_SCOPE_GLOBAL, CK_ROLE_ACTIVATION,
     {{CK_DIM_TOKENS, 1, 1}, {CK_DIM_HEAD_DIM, 1, 2}, ...},
     .condition = "rope_theta"},  // only if RoPE enabled

    // Per-layer buffers
    {"q", CK_SCOPE_LAYER, CK_ROLE_OUTPUT,
     {{CK_DIM_NUM_HEADS, 1, 1}, {CK_DIM_TOKENS, 1, 1},
      {CK_DIM_ALIGNED_HEAD, 1, 1}, ...}},
};</pre>
</div>

<div class="card card-green">
    <h3 style="margin-top: 0;">CKPlanStep: Execution Plan</h3>
    <p>Forward pass as a sequence of kernel invocations:</p>
    <pre>const CKPlanStep ck_decoder_forward_plan[] = {
    {"rmsnorm",      NULL},           // LN1
    {"qkv_project",  NULL},           // Q, K, V projections
    {"rope",         "rope_theta>0"}, // RoPE (conditional)
    {"attention",    NULL},           // Self-attention
    {"attn_proj",    NULL},           // Output projection
    {"residual_add", NULL},           // Residual connection
    {"rmsnorm",      NULL},           // LN2
    {"mlp_up",       NULL},           // FC1: hidden → 2×intermediate
    {"swiglu",       NULL},           // SwiGLU activation
    {"mlp_down",     NULL},           // FC2: intermediate → hidden
    {"residual_add", NULL},           // Residual connection
};</pre>
</div>

<div class="grid grid-2">
    <div class="card">
        <h3 style="margin-top: 0;">How Codegen Uses This</h3>
        <ol>
            <li>Read execution plan (<code>ck_decoder_forward_plan</code>)</li>
            <li>For each step, lookup kernel in <code>ck_kernel_specs</code></li>
            <li>Emit source files from <code>sources[]</code></li>
            <li>Emit function call using <code>forward</code>/<code>backward</code></li>
            <li>Bind buffers from <code>ck_decoder_buffers</code></li>
        </ol>
    </div>
    <div class="card">
        <h3 style="margin-top: 0;">Why This Matters</h3>
        <ul>
            <li><strong>No duplication</strong>: codegen uses tested kernels</li>
            <li><strong>No drift</strong>: one source of truth</li>
            <li><strong>Composable</strong>: add new kernels to registry</li>
            <li><strong>Conditional</strong>: RoPE, bias, etc. are optional</li>
            <li><strong>Typed</strong>: symbolic dims, roles, conditions</li>
        </ul>
    </div>
</div>

<h2>Registry Flow: How It All Fits Together</h2>

<div class="img-container svg-viewer" data-title="Kernel Registry Flow">
    <img src="assets/kernel-registry-flow.svg" alt="Kernel Registry Flow Diagram">
</div>

<div class="card">
    <h3 style="margin-top: 0;">The Complete Pipeline</h3>
    <table>
        <tr>
            <th>Step</th>
            <th>Component</th>
            <th>What It Does</th>
        </tr>
        <tr>
            <td>1</td>
            <td><code>config.json</code></td>
            <td>HuggingFace model config with dimensions (hidden_size, num_heads, etc.)</td>
        </tr>
        <tr>
            <td>2</td>
            <td><code>CKIRGraph</code></td>
            <td>Parse config into IR with operations and tensor shapes</td>
        </tr>
        <tr>
            <td>3</td>
            <td><code>CKKernelSpec[]</code></td>
            <td>Lookup kernel name → get forward/backward function + source files</td>
        </tr>
        <tr>
            <td>4</td>
            <td><code>CKBufferSpec[]</code></td>
            <td>Get buffer shapes, roles, dtypes → compute memory layout</td>
        </tr>
        <tr>
            <td>5</td>
            <td><code>CKPlanStep[]</code></td>
            <td>Execute kernels in order, respecting conditions</td>
        </tr>
        <tr>
            <td>6</td>
            <td><code>Bump Allocator</code></td>
            <td>Resolve symbolic dims → concrete byte offsets (dtype-aware)</td>
        </tr>
        <tr>
            <td>7</td>
            <td><code>Source Emitter</code></td>
            <td>Emit tested kernel sources + function call wiring</td>
        </tr>
        <tr>
            <td>8</td>
            <td><code>generated_model.c</code></td>
            <td>Standalone C file: <code>gcc -O3 model.c -o model</code></td>
        </tr>
    </table>
</div>

<div class="alert alert-info">
    <div class="alert-icon">
        <svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><circle cx="12" cy="12" r="10"/><line x1="12" y1="16" x2="12" y2="12"/><line x1="12" y1="8" x2="12.01" y2="8"/></svg>
    </div>
    <div>
        <strong>Key Insight</strong><br>
        The generated <code>model.c</code> contains the <em>exact same kernel code</em> that passed PyTorch parity tests. The registry ensures no code is written twice.
    </div>
</div>

<div class="card card-accent">
    <h3 style="margin-top: 0;">Adding a New Kernel</h3>
    <p>To add a new kernel (e.g., GeLU activation):</p>
    <pre>// 1. Write and test the kernel
// src/kernels/gelu_kernels.c
void gelu_forward(const float *in, float *out, int n);
void gelu_backward(const float *in, const float *d_out, float *d_in, int n);

// 2. Add to kernel specs
{"gelu", "gelu_forward", "gelu_backward",
 {"src/kernels/gelu_kernels.c", NULL, ...}},

// 3. Add buffers if needed
{"gelu_out", CK_SCOPE_LAYER, CK_ROLE_OUTPUT,
 {{CK_DIM_TOKENS, 1, 1}, {CK_DIM_ALIGNED_EMBED, 1, 1}, ...}},

// 4. Add to execution plan (for models that use GeLU)
{"gelu", "activation_type==gelu"},</pre>
    <p>Codegen automatically picks up the new kernel. No other changes needed.</p>
</div>

<h2>Intermediate Representation</h2>

<div class="alert alert-info">
    <div class="alert-icon">
        <svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><circle cx="12" cy="12" r="10"/><line x1="12" y1="16" x2="12" y2="12"/><line x1="12" y1="8" x2="12.01" y2="8"/></svg>
    </div>
    <div>
        <strong>Why IR?</strong><br>
        The IR layer decouples config parsing from code generation, enabling optimizations and different backends.
    </div>
</div>

<div class="card">
    <h3 style="margin-top: 0;">IR Graph Structure</h3>
    <pre>typedef struct {
    CKOpType op;        // CK_OP_RMSNORM, CK_OP_ATTENTION, etc.
    int layer_index;
    int input_ids[4];   // References to input nodes
    int output_id;      // Output node ID
    // ... dimension info
} CKIRNode;

typedef struct {
    CKModelConfig config;
    CKIRNode *nodes;
    int num_nodes;
    int num_layers;
} CKIRGraph;</pre>
</div>

<div class="card card-accent">
    <h3 style="margin-top: 0;">Supported Operations</h3>
    <table>
        <tr>
            <th>Op Type</th>
            <th>Forward</th>
            <th>Backward</th>
        </tr>
        <tr>
            <td><code>CK_OP_RMSNORM</code></td>
            <td>rmsnorm_forward</td>
            <td>rmsnorm_backward</td>
        </tr>
        <tr>
            <td><code>CK_OP_ATTENTION</code></td>
            <td>attention_forward_causal_head_major_gqa</td>
            <td>attention_backward_causal_head_major_gqa</td>
        </tr>
        <tr>
            <td><code>CK_OP_ROPE</code></td>
            <td>rope_forward_qk</td>
            <td>rope_backward_qk</td>
        </tr>
        <tr>
            <td><code>CK_OP_SWIGLU</code></td>
            <td>swiglu_forward</td>
            <td>swiglu_backward</td>
        </tr>
        <tr>
            <td><code>CK_OP_LINEAR</code></td>
            <td>gemm_blocked_serial</td>
            <td>fc_backward_kernel</td>
        </tr>
        <tr>
            <td><code>CK_OP_RESIDUAL</code></td>
            <td>ck_add_inplace</td>
            <td>(gradient passthrough)</td>
        </tr>
    </table>
</div>

<h2>Output: Generated Code</h2>

<div class="card">
    <h3 style="margin-top: 0;">Generated Forward Pass</h3>
    <pre>void forward_layer_0(
    const float *input,
    const ModelWeights *weights,
    LayerActivations *acts,
    const float *cos_cache,
    const float *sin_cache,
    int num_tokens
) {
    // 1. Pre-attention RMSNorm
    rmsnorm_forward(input, weights->ln1_gamma, acts->ln1_out,
                    acts->rstd1, num_tokens, 768, 768, 1e-5f);

    // 2. QKV projection
    ck_qkv_project_head_major(acts->ln1_out,
        weights->wq, weights->bq,
        weights->wk, weights->bk,
        weights->wv, weights->bv,
        acts->q, acts->k, acts->v,
        num_tokens, 768, 12, 4, 64);

    // 3. Apply RoPE
    rope_forward_qk(acts->q, acts->k, cos_cache, sin_cache,
                    12, 4, num_tokens, 64, 0);

    // 4. Attention
    attention_forward_causal_head_major_gqa(
        acts->q, acts->k, acts->v,
        acts->scores, acts->attn_out,
        12, 4, num_tokens, 64, 64, 2048);

    // 5. Output projection + residual
    ck_attention_project_head_major(acts->attn_out, weights->wo, weights->bo,
                                     acts->proj_out, acts->scratch,
                                     num_tokens, 768, 12, 64);
    ck_add_inplace(acts->proj_out, input, num_tokens, 768);

    // 6-8. MLP block...
}</pre>
</div>

<div class="card card-accent">
    <h3 style="margin-top: 0;">Generated Backward Pass</h3>
    <pre>void backward_layer_0(
    const float *d_output,
    const ModelWeights *weights,
    const LayerActivations *acts,
    WeightGradients *grads,
    float *d_input
) {
    // Reverse order of forward pass

    // 1. Backward through MLP residual
    // d_mlp_out = d_output (residual gradient passthrough)

    // 2. Backward through FC2
    fc2_backward_kernel(d_output, acts->swiglu_out, weights->w2,
                        d_swiglu, grads->d_w2, grads->d_b2, ...);

    // 3. Backward through SwiGLU
    swiglu_backward(acts->fc1_out, d_swiglu, d_fc1, num_tokens, 2048);

    // ... continue backwards through all ops

    // N. Backward through RMSNorm 1
    rmsnorm_backward(d_rmsnorm, input, weights->ln1_gamma,
                     acts->rstd1, d_input, grads->d_ln1_gamma,
                     num_tokens, 768, 768);
}</pre>
</div>

<h2>IR Serialization</h2>

<div class="grid grid-2">
    <div class="card">
        <h3 style="margin-top: 0;">Export IR to JSON</h3>
        <pre>ck_ir_serialize_json(
    &graph,
    "model_ir.json"
);</pre>
        <p>Enables inspection, debugging, and external tooling.</p>
    </div>
    <div class="card">
        <h3 style="margin-top: 0;">Load IR from JSON</h3>
        <pre>ck_ir_parse_json(
    "model_ir.json",
    &graph
);</pre>
        <p>Two-stage pipeline: generate IR once, emit code multiple times.</p>
    </div>
</div>

<h2>Memory Layout</h2>

<div class="card">
    <h3 style="margin-top: 0;">Buffer Allocation</h3>
    <p>The codegen calculates all buffer sizes based on config dimensions:</p>
    <pre>layout_transformer_from_ir(&model, &ir);

// Computed sizes:
// - Weight memory: embeddings + all layer weights
// - Activation memory: per-layer intermediates
// - KV cache: for autoregressive generation</pre>
</div>

<table>
    <tr>
        <th>Buffer</th>
        <th>Size Formula</th>
    </tr>
    <tr>
        <td>Q</td>
        <td><code>num_heads × max_tokens × head_dim</code></td>
    </tr>
    <tr>
        <td>K, V</td>
        <td><code>num_kv_heads × max_tokens × head_dim</code></td>
    </tr>
    <tr>
        <td>Attention scores</td>
        <td><code>num_heads × max_tokens × context_window</code></td>
    </tr>
    <tr>
        <td>MLP hidden</td>
        <td><code>max_tokens × 2 × intermediate_size</code></td>
    </tr>
    <tr>
        <td>Layer weights</td>
        <td><code>num_layers × (QKV + O + MLP weights)</code></td>
    </tr>
</table>

<h2>Bump Allocator: Dtype-Aware Layout</h2>

<div class="alert alert-info">
    <div class="alert-icon">
        <svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><circle cx="12" cy="12" r="10"/><line x1="12" y1="16" x2="12" y2="12"/><line x1="12" y1="8" x2="12.01" y2="8"/></svg>
    </div>
    <div>
        <strong>Why Bump Allocation?</strong><br>
        All memory is allocated in one contiguous block. Offsets are computed at codegen time based on dtype. No runtime malloc, clean strides, cache-friendly.
    </div>
</div>

<div class="card">
    <h3 style="margin-top: 0;">Mixed Precision Layout</h3>
    <p>Weights can be bf16/int8 while activations stay fp32:</p>
    <pre>// Generated at codegen time - dtype-aware
size_t off = 0;

// Weights: bf16 (2 bytes each)
m->token_emb_offset = off;
off += align64(V * aligned_embed * 2);     // bf16

m->wq_offset = off;
off += align64(num_heads * head_dim * embed * 2);  // bf16

// Activations: fp32 (4 bytes each)
m->embedded_input_offset = off;
off += align64(T * aligned_embed * 4);     // fp32

m->q_offset = off;
off += align64(num_heads * T * head_dim * 4);  // fp32</pre>
</div>

<div class="grid grid-2">
    <div class="card">
        <h3 style="margin-top: 0;">Dtype Enum</h3>
        <pre>typedef enum {
    CK_DTYPE_F32 = 0,  // 4 bytes
    CK_DTYPE_F16,      // 2 bytes
    CK_DTYPE_BF16,     // 2 bytes
    CK_DTYPE_I8,       // 1 byte
    CK_DTYPE_I4,       // 0.5 bytes (packed)
} CKDType;</pre>
    </div>
    <div class="card">
        <h3 style="margin-top: 0;">Per-Buffer Dtype</h3>
        <pre>{"wq", CK_SCOPE_LAYER, CK_ROLE_WEIGHT,
 CK_DTYPE_BF16,  // weights in bf16
 {{CK_DIM_NUM_HEADS, ...}}},

{"q", CK_SCOPE_LAYER, CK_ROLE_OUTPUT,
 CK_DTYPE_F32,   // activations in fp32
 {{CK_DIM_NUM_HEADS, ...}}},</pre>
    </div>
</div>

<div class="card card-accent">
    <h3 style="margin-top: 0;">All Offsets Cacheline-Aligned</h3>
    <p>Every buffer starts at a 64-byte boundary for optimal SIMD access:</p>
    <pre>static size_t align64(size_t n) {
    return (n + 63) & ~63;
}

static size_t bump(size_t *off, size_t bytes) {
    size_t start = align64(*off);
    *off = start + bytes;
    return start;
}</pre>
</div>

<h2>Source Files</h2>

<table>
    <tr>
        <th>File</th>
        <th>Purpose</th>
    </tr>
    <tr>
        <td><code>ckernel_ir.c</code></td>
        <td>IR graph building, serialization</td>
    </tr>
    <tr>
        <td><code>ckernel_codegen.c</code></td>
        <td>C code emission from IR</td>
    </tr>
    <tr>
        <td><code>ckernel_kernel_specs.c</code></td>
        <td>Kernel map: specs, buffers, execution plan</td>
    </tr>
    <tr>
        <td><code>ckernel_model_layout.c</code></td>
        <td>Bump allocator, memory layout</td>
    </tr>
    <tr>
        <td><code>ckernel_model_load.c</code></td>
        <td>Weight loading from files</td>
    </tr>
    <tr>
        <td><code>ckernel_registry.c</code></td>
        <td>Op validation, kernel lookup</td>
    </tr>
</table>

<p>For full source code, see the <a href="doxygen/files.html">Doxygen source browser</a>.</p>
