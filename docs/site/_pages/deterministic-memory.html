<!-- TITLE: Deterministic Memory -->
<!-- NAV: concepts -->

<div class="container">
  <h1>Deterministic Memory Layout</h1>
  <p class="lead">
    Why static memory offsets matter, and what they enable for monitoring, debugging, and mechanistic interpretability.
  </p>

  <div class="card card-accent">
    <h3 style="margin-top: 0;">The Core Insight</h3>
    <p>
      C-Kernel-Engine generates C code with <strong>compile-time known memory offsets</strong>.
      Every tensor, every weight, every activation has a fixed address that never changes between runs.
    </p>
    <p>
      This isn't just an implementation detail&mdash;it's a fundamental architectural choice that unlocks
      capabilities impossible in dynamic frameworks like PyTorch or JAX.
    </p>
  </div>

  <h2>Why Deterministic Memory?</h2>

  <div class="grid-2">
    <div class="card">
      <h3>Dynamic Frameworks (PyTorch)</h3>
      <ul>
        <li>Memory allocated at runtime</li>
        <li>Addresses change every run</li>
        <li>Need hooks to observe tensors</li>
        <li>Hooks add overhead to training</li>
        <li>Can't observe without perturbing</li>
      </ul>
    </div>
    <div class="card">
      <h3>C-Kernel-Engine</h3>
      <ul>
        <li>Memory layout computed at compile time</li>
        <li>Same offsets every run</li>
        <li>Direct memory access via RDMA</li>
        <li>Zero overhead observation</li>
        <li>Observe without disturbing compute</li>
      </ul>
    </div>
  </div>

  <h2>The Layout JSON: Your Memory Address Book</h2>

  <p>
    When you generate code with <code>build_ir_v4.py</code>, you also get a <code>layout.json</code>
    that maps every tensor to its exact memory location:
  </p>

  <pre><code class="language-json">{
  "model": "Qwen2-0.5B",
  "layers": {
    "layer_15": {
      "attn_q": {"offset": "0x0A2400", "size": 1605632, "dtype": "bf16"},
      "attn_k": {"offset": "0x0A4800", "size": 1605632, "dtype": "bf16"},
      "attn_v": {"offset": "0x0A6C00", "size": 1605632, "dtype": "bf16"},
      "mlp_gate": {"offset": "0x0B2000", "size": 3211264, "dtype": "q4_k"}
    }
  }
}</code></pre>

  <p>
    This "address book" is the key to everything that follows. You can point to any tensor by name
    and know exactly where it lives in memory.
  </p>

  <h2>What This Enables</h2>

  <h3>1. Zero-Overhead RDMA Monitoring</h3>

  <div class="card">
    <p>
      RDMA (Remote Direct Memory Access) supports <strong>one-sided operations</strong>. An observer
      can read memory from compute nodes without those nodes even knowing it happened&mdash;no interrupts,
      no CPU involvement, no overhead on training.
    </p>

    <div class="diagram">
      <pre>
┌─────────────────────────────────────────────────────────────────┐
│                      RDMA Network (100-400 Gbps)                │
└───────┬───────────────────┬───────────────────┬─────────────────┘
        │                   │                   │
   ┌────▼────┐         ┌────▼────┐         ┌────▼────┐
   │ Node 0  │         │ Node 1  │         │ Observer│
   │ Layers  │         │ Layers  │         │ Laptop  │
   │  0-11   │         │ 12-23   │         │         │
   │         │         │         │         │ RDMA    │
   │ [memory]│         │ [memory]│         │ READ    │
   │ weights │         │ weights │         │ only    │
   │ grads   │         │ grads   │         │         │
   │ acts    │         │ acts    │         │ UI/Viz  │
   └─────────┘         └─────────┘         └─────────┘
   (no logging)        (no logging)        (reads memory)
      </pre>
    </div>

    <p>
      The observer laptop can sample any tensor at any rate (1Hz, 10Hz, 100Hz) without
      affecting training throughput. Traditional logging requires compute nodes to serialize
      and send data&mdash;that's overhead you can't avoid.
    </p>
  </div>

  <h3>2. Live Tensor Inspector</h3>

  <div class="card">
    <pre>
┌─────────────────────────────────────────────────────────┐
│ Layer 15 Attention                                      │
├─────────────────────────────────────────────────────────┤
│ Q [0x0A2400 - 0x0A4800]  896×896 bf16  ████████░░ 2.1ms │
│ K [0x0A4800 - 0x0A6C00]  896×896 bf16  ██████████ 1.8ms │
│ V [0x0A6C00 - 0x0A9000]  896×896 bf16  ███░░░░░░░ 0.4ms │
│ Scores [0x0A9000]        896×896 f32   computing...     │
└─────────────────────────────────────────────────────────┘
    </pre>
    <p>
      Click any tensor to see values, histograms, NaN detection. Because memory layout is static,
      these offsets work across runs&mdash;you can compare the same tensor position across different
      training experiments.
    </p>
  </div>

  <h3>3. Pipeline Parallel Visualization</h3>

  <div class="card">
    <pre>
Node 0 (Layers 0-7)    Node 1 (Layers 8-15)   Node 2 (Layers 16-23)
┌──────────────────┐   ┌──────────────────┐   ┌──────────────────┐
│ █ Micro-batch 0  │──▶│ █ Micro-batch 0  │──▶│ ░ waiting        │
│ █ Micro-batch 1  │   │ ░ Micro-batch 1  │   │                  │
│ ░ Micro-batch 2  │   │                  │   │                  │
└──────────────────┘   └──────────────────┘   └──────────────────┘
  RDMA: 12.4 GB/s        RDMA: 11.8 GB/s
    </pre>
    <p>
      For distributed training, you know exactly which bytes transfer between nodes. Track
      pipeline bubble efficiency in real-time without any instrumentation overhead.
    </p>
  </div>

  <h3>4. Checkpoint Diff Tool</h3>

  <div class="card">
    <pre><code class="language-bash">ck-diff checkpoint_step1000.bump checkpoint_step2000.bump

Layer 15 MLP Gate [0x0B2000]:
  Step 1000: mean=-0.0012, std=0.0234
  Step 2000: mean=-0.0018, std=0.0241
  Delta:     mean=-0.0006, std=+0.0007  ✓ healthy

Layer 23 Output [0x2A0000]:
  WARNING: std increased 3x - possible gradient explosion</code></pre>
    <p>
      Compare checkpoints byte-by-byte. Because layout is deterministic, offset 0x0B2000 means
      the same thing in both files.
    </p>
  </div>

  <h3>5. Deterministic Replay Debugging</h3>

  <div class="card">
    <p>
      Since memory layout is static:
    </p>
    <ul>
      <li>Record input tokens + random seeds</li>
      <li>Replay exact same forward pass</li>
      <li>Step through layer-by-layer with known offsets</li>
      <li>Compare across runs (impossible in PyTorch where allocations differ)</li>
    </ul>
  </div>

  <h2>Mechanistic Interpretability</h2>

  <p>
    This is where deterministic memory becomes transformative. Mechanistic interpretability
    research aims to understand <em>how</em> neural networks actually work internally.
    The field struggles because frameworks hide everything behind abstractions.
  </p>

  <div class="card card-accent">
    <h3 style="margin-top: 0;">The Interpretability Unlock</h3>
    <table>
      <tr>
        <th>Traditional Approach</th>
        <th>With C-Kernel-Engine</th>
      </tr>
      <tr>
        <td>"What's attention head 7 doing?"<br>→ Add hooks, slow down training</td>
        <td><code>layer_15_head_7 @ 0x0A2400</code><br>→ RDMA read, zero overhead</td>
      </tr>
      <tr>
        <td>"Patch this activation"<br>→ Framework gymnastics</td>
        <td><code>memcpy(0x0A2400, patch, size)</code><br>→ Direct memory write</td>
      </tr>
      <tr>
        <td>"Trace information flow"<br>→ Black box</td>
        <td>Follow the offsets in layout.json<br>→ Explicit dataflow graph</td>
      </tr>
    </table>
  </div>

  <h3>Live Circuit Discovery</h3>

  <pre><code class="language-python"># While training is running, observe attention patterns
for layer in range(24):
    for head in range(14):
        attn = rdma_read(f"layer_{layer}_head_{head}_scores")
        if attn[query_pos, key_pos] > 0.8:
            print(f"Strong connection: L{layer}H{head}")</code></pre>

  <p>No hooks. No slowdown. Just read memory.</p>

  <h3>Activation Patching at Scale</h3>

  <pre><code class="language-python"># Causal intervention: "What happens if we zero this head?"
original = rdma_read(node=0, offset=LAYER_15_HEAD_7)
rdma_write(node=0, offset=LAYER_15_HEAD_7, data=zeros)
# Observe effect on output
rdma_write(node=0, offset=LAYER_15_HEAD_7, data=original)  # Restore</code></pre>

  <h3>Neuron-Level Probing</h3>

  <pre>
Layer 15, Neuron 847:
  Activates strongly on: "capital cities", "country names"
  Offset: 0x0B2400 + 847*4
  Historical max: 12.4 (on token "Paris")
  Current: 0.02
  </pre>

  <h3>Gradient Attribution</h3>

  <pre>
"Which weights most affected this prediction?"

∂Loss/∂W for every weight, at known offsets:
  layer_15_mlp_gate[0x0B2000]: 0.0023  ← high attribution
  layer_15_mlp_up[0x0B4000]:   0.0001  ← low
  layer_22_attn_o[0x1A2000]:   0.0089  ← highest!
  </pre>

  <h2>Comparison with Traditional Approaches</h2>

  <table>
    <tr>
      <th>Capability</th>
      <th>Dynamic Framework</th>
      <th>C-Kernel-Engine</th>
    </tr>
    <tr>
      <td>Memory layout</td>
      <td>Changes every run</td>
      <td>Fixed at compile time</td>
    </tr>
    <tr>
      <td>Observability</td>
      <td>Hooks (add overhead)</td>
      <td>Direct RDMA read (zero overhead)</td>
    </tr>
    <tr>
      <td>Activation patching</td>
      <td>Slow, framework-specific</td>
      <td>Direct memory write</td>
    </tr>
    <tr>
      <td>Probing classifiers</td>
      <td>Offline analysis</td>
      <td>Real-time</td>
    </tr>
    <tr>
      <td>Attention visualization</td>
      <td>Post-hoc</td>
      <td>Live during training</td>
    </tr>
    <tr>
      <td>Circuit tracing</td>
      <td>Manual, tedious</td>
      <td>Automated via layout.json</td>
    </tr>
    <tr>
      <td>Reproducibility</td>
      <td>Difficult (allocation varies)</td>
      <td>Guaranteed (same offsets)</td>
    </tr>
  </table>

  <h2>The Observer Architecture</h2>

  <p>
    A third machine (laptop, monitoring server) can observe training clusters via RDMA
    without affecting their performance:
  </p>

  <div class="card">
    <pre><code class="language-python"># observer.py - runs on separate machine
class TrainingObserver:
    def __init__(self, nodes, layout_json):
        self.layout = load_layout(layout_json)  # Static offsets!
        self.rdma = RDMAConnection(nodes)

    def sample_gradients(self, layer):
        offset = self.layout[f"layer_{layer}"]["grad_offset"]
        size = self.layout[f"layer_{layer}"]["grad_size"]

        # One-sided read - compute node doesn't notice
        data = self.rdma.read(node=layer // 12, offset=offset, size=size)
        return np.frombuffer(data, dtype=np.float32)

    def check_for_nans(self):
        for layer in range(24):
            grad = self.sample_gradients(layer)
            if np.isnan(grad).any():
                alert(f"NaN detected in layer {layer}!")</code></pre>
  </div>

  <h2>The Philosophy</h2>

  <blockquote>
    "We can't interpret what we can't see."
  </blockquote>

  <p>
    Dynamic frameworks hide computation behind layers of abstraction. The memory allocator
    decides where tensors live. Hooks and callbacks add overhead to observe what's happening.
  </p>

  <p>
    C-Kernel-Engine takes the opposite approach: <strong>everything is explicit</strong>.
    The generated C code shows exactly what computation happens. The layout.json shows
    exactly where every tensor lives. RDMA lets you observe without disturbing.
  </p>

  <p>
    This doesn't solve interpretability&mdash;the science is still hard. But it removes
    the <em>infrastructure barriers</em> that make interpretability research difficult.
    When you can see everything, you can start to understand everything.
  </p>

  <div class="card card-accent">
    <h3 style="margin-top: 0;">Summary</h3>
    <ul>
      <li><strong>Deterministic layout</strong> → Reproducible experiments, same offsets every run</li>
      <li><strong>Zero-overhead observation</strong> → Don't perturb what you measure</li>
      <li><strong>Explicit computation</strong> → No hidden framework magic</li>
      <li><strong>Known offsets</strong> → Point to any neuron, any layer, any time</li>
      <li><strong>RDMA observer</strong> → Separate monitoring from compute</li>
    </ul>
  </div>
</div>
