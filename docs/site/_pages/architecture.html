<!-- TITLE: Architecture -->
<!-- NAV: architecture -->

<h1>System Architecture</h1>

<p>C-Kernel-Engine uses a three-stage pipeline to transform model configurations into optimized C runtimes.</p>

<div class="card">
    <h3 style="margin-top: 0;">The "Website" Metaphor</h3>
    <p>The engine treats LLMs like a website generator treats pages. This allows us to unroll the "Block" section efficiently in C without complex control flow.</p>
    <table>
        <tr>
            <th>Section</th>
            <th>Website</th>
            <th>LLM</th>
        </tr>
        <tr>
            <td><strong>Header</strong></td>
            <td>&lt;head&gt;, Nav, CSS</td>
            <td>Embeddings, Positional Encoding</td>
        </tr>
        <tr>
            <td><strong>Block</strong></td>
            <td>Blog Posts, Articles</td>
            <td>Transformer Layers (repeated)</td>
        </tr>
        <tr>
            <td><strong>Footer</strong></td>
            <td>Copyright, Scripts</td>
            <td>Final Norm, Language Head</td>
        </tr>
    </table>
</div>

<div class="img-container">
    <img src="assets/architecture-overview.svg" alt="Architecture Overview">
</div>

<h2>Stage 1: Model Configuration</h2>

<div class="card card-accent">
    <p>The engine accepts HuggingFace-style <code>config.json</code> files as input:</p>
    <pre>{
  "hidden_size": 768,
  "num_attention_heads": 12,
  "num_key_value_heads": 4,
  "num_hidden_layers": 6,
  "intermediate_size": 2048,
  "rms_norm_eps": 1e-5,
  "rope_theta": 10000.0
}</pre>
    <p>This defines all the dimensions needed to generate layer structures.</p>
</div>

<h2>Stage 2: Intermediate Representation</h2>

<p>The IR Builder creates a structured representation of each layer:</p>

<div class="grid grid-2">
    <div class="card">
        <h3 style="margin-top: 0;">CKLayerIR Structure</h3>
        <pre>typedef struct {
    int layer_index;
    int embed_dim;
    int num_heads;
    int num_kv_heads;
    int head_dim;
    int intermediate_dim;
    int context_window;
    float eps;
    float rope_theta;
} CKLayerIR;</pre>
    </div>
    <div class="card">
        <h3 style="margin-top: 0;">Key Decisions</h3>
        <ul>
            <li><strong>Aligned dimensions</strong>: Head dim padded to cache-friendly sizes</li>
            <li><strong>GQA ratio</strong>: Computed from num_heads / num_kv_heads</li>
            <li><strong>Buffer sizing</strong>: Calculated for all intermediate activations</li>
        </ul>
    </div>
</div>

<h2>Stage 3: Code Generation</h2>

<p>The codegen emits complete C functions for forward and backward passes:</p>

<div class="card">
    <h3 style="margin-top: 0;">Generated Forward Pass</h3>
    <pre>void forward_layer_0(
    const float *input,
    const ModelWeights *weights,
    LayerActivations *acts,
    const float *cos_cache,
    const float *sin_cache,
    int num_tokens
) {
    // 1. Pre-attention RMSNorm
    rmsnorm_forward(input, weights->ln1_gamma, acts->ln1_out, ...);

    // 2. QKV projection
    ck_qkv_project_head_major(acts->ln1_out, weights->wq, ...);

    // 3. Apply RoPE
    rope_forward_qk(acts->q, acts->k, cos_cache, sin_cache, ...);

    // 4. Attention
    attention_forward_causal_head_major_gqa(acts->q, acts->k, acts->v, ...);

    // 5. Output projection + residual
    // 6. Post-attention RMSNorm
    // 7. MLP (SwiGLU)
    // 8. Final residual
}</pre>
</div>

<div class="card card-accent">
    <h3 style="margin-top: 0;">Generated Backward Pass</h3>
    <pre>void backward_layer_0(
    const float *d_output,
    const ModelWeights *weights,
    const LayerActivations *acts,
    WeightGradients *grads,
    float *d_input
) {
    // Reverse order of forward pass
    // Each kernel uses saved activations from forward

    // 1. Backward through final residual
    // 2. Backward through MLP (SwiGLU)
    // 3. Backward through RMSNorm 2
    // 4. Backward through attention output projection
    // 5. Backward through attention
    attention_backward_causal_head_major_gqa(d_attn_out, acts->q, ...);

    // 6. Backward through RoPE (inverse rotation)
    rope_backward_qk(d_q, d_k, ...);

    // 7. Backward through QKV projection
    // 8. Backward through RMSNorm 1
}</pre>
</div>

<h2>Memory Layout</h2>

<div class="alert alert-info">
    <div class="alert-icon">
        <svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><circle cx="12" cy="12" r="10"/><line x1="12" y1="16" x2="12" y2="12"/><line x1="12" y1="8" x2="12.01" y2="8"/></svg>
    </div>
    <div>
        <strong>Head-Major Layout</strong><br>
        Q/K/V use <code>[num_heads, num_tokens, head_dim]</code> layout for cache-efficient attention computation.
    </div>
</div>

<table>
    <tr>
        <th>Buffer</th>
        <th>Layout</th>
        <th>Size</th>
    </tr>
    <tr>
        <td><code>input</code></td>
        <td>[B, T, D]</td>
        <td>batch * tokens * embed_dim</td>
    </tr>
    <tr>
        <td><code>Q</code></td>
        <td>[H, T, d_k]</td>
        <td>num_heads * tokens * head_dim</td>
    </tr>
    <tr>
        <td><code>K, V</code></td>
        <td>[H_kv, T, d_k]</td>
        <td>num_kv_heads * tokens * head_dim</td>
    </tr>
    <tr>
        <td><code>scores</code></td>
        <td>[H, T, T]</td>
        <td>num_heads * tokens * context_window</td>
    </tr>
    <tr>
        <td><code>mlp_hidden</code></td>
        <td>[T, 2*I]</td>
        <td>tokens * 2 * intermediate_dim</td>
    </tr>
</table>

<h2>Kernel Composition</h2>

<p>Kernels are composed following transformer layer structure:</p>

<div class="img-container">
    <img src="assets/forward-backward-flow.svg" alt="Forward and Backward Data Flow">
</div>

<h2>Build System</h2>

<div class="grid grid-2">
    <div class="card">
        <h3 style="margin-top: 0;">Full Library</h3>
        <pre>make</pre>
        <p>Builds <code>libckernel_engine.so</code> with all kernels linked together.</p>
    </div>
    <div class="card">
        <h3 style="margin-top: 0;">Per-Kernel Libraries</h3>
        <pre>make libckernel_attention.so
make libckernel_rope.so
make libckernel_rmsnorm.so</pre>
        <p>Builds individual kernel libraries for testing.</p>
    </div>
</div>

<h2>Codegen Pipeline</h2>

<div class="card card-accent">
    <h3 style="margin-top: 0;">Generate Runtime from Config</h3>
    <pre># Build the IR demo tool
make build/ck_ir_demo

# Generate C runtime
./build/ck_ir_demo config.json --emit build/model.c

# Or use the make target
make ck-emit CONFIG=config.json OUT=build/model.c</pre>
</div>

<p>The generated file contains:</p>
<ul>
    <li>Buffer allocation functions</li>
    <li>Forward pass for all layers</li>
    <li>Backward pass for all layers</li>
    <li>Parameter gradient accumulation</li>
</ul>

<h2>Project Structure</h2>

<p>The codebase is organized for easy navigation:</p>

{{FOLDER_STRUCTURE}}
