<?xml version='1.0' encoding='UTF-8' standalone='no'?>
<doxygen xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:noNamespaceSchemaLocation="compound.xsd" version="1.9.1" xml:lang="en-US">
  <compounddef id="optimizer__kernels_8c" kind="file" language="C++">
    <compoundname>optimizer_kernels.c</compoundname>
    <includes local="no">math.h</includes>
    <includes local="no">stddef.h</includes>
    <includes local="no">stdint.h</includes>
    <includes local="no">string.h</includes>
    <incdepgraph>
      <node id="4">
        <label>stdint.h</label>
      </node>
      <node id="3">
        <label>stddef.h</label>
      </node>
      <node id="5">
        <label>string.h</label>
      </node>
      <node id="2">
        <label>math.h</label>
      </node>
      <node id="1">
        <label>optimizer_kernels.c</label>
        <link refid="optimizer__kernels_8c"/>
        <childnode refid="2" relation="include">
        </childnode>
        <childnode refid="3" relation="include">
        </childnode>
        <childnode refid="4" relation="include">
        </childnode>
        <childnode refid="5" relation="include">
        </childnode>
      </node>
    </incdepgraph>
      <sectiondef kind="func">
      <memberdef kind="function" id="optimizer__kernels_8c_1ac1a9ed01c15286fc6e3be73f96371d76" prot="public" static="no" const="no" explicit="no" inline="no" virt="non-virtual">
        <type>void</type>
        <definition>void adamw_update_f32</definition>
        <argsstring>(const float *grad, float *weight, float *m, float *v, size_t numel, float lr, float beta1, float beta2, float eps, float weight_decay, int step)</argsstring>
        <name>adamw_update_f32</name>
        <param>
          <type>const float *</type>
          <declname>grad</declname>
        </param>
        <param>
          <type>float *</type>
          <declname>weight</declname>
        </param>
        <param>
          <type>float *</type>
          <declname>m</declname>
        </param>
        <param>
          <type>float *</type>
          <declname>v</declname>
        </param>
        <param>
          <type>size_t</type>
          <declname>numel</declname>
        </param>
        <param>
          <type>float</type>
          <declname>lr</declname>
        </param>
        <param>
          <type>float</type>
          <declname>beta1</declname>
        </param>
        <param>
          <type>float</type>
          <declname>beta2</declname>
        </param>
        <param>
          <type>float</type>
          <declname>eps</declname>
        </param>
        <param>
          <type>float</type>
          <declname>weight_decay</declname>
        </param>
        <param>
          <type>int</type>
          <declname>step</declname>
        </param>
        <briefdescription>
<para>AdamW optimizer update (fp32 version) </para>
        </briefdescription>
        <detaileddescription>
<para>Updates weights in-place using the AdamW algorithm. Momentum (m) and variance (v) are stored in fp32 for numerical stability.</para>
<para><parameterlist kind="param"><parameteritem>
<parameternamelist>
<parametername>grad</parametername>
</parameternamelist>
<parameterdescription>
<para>Gradient tensor (fp32) [numel] </para>
</parameterdescription>
</parameteritem>
<parameteritem>
<parameternamelist>
<parametername>weight</parametername>
</parameternamelist>
<parameterdescription>
<para>Weight tensor to update (fp32, in-place) [numel] </para>
</parameterdescription>
</parameteritem>
<parameteritem>
<parameternamelist>
<parametername>m</parametername>
</parameternamelist>
<parameterdescription>
<para>First moment (momentum) buffer (fp32, in-place) [numel] </para>
</parameterdescription>
</parameteritem>
<parameteritem>
<parameternamelist>
<parametername>v</parametername>
</parameternamelist>
<parameterdescription>
<para>Second moment (variance) buffer (fp32, in-place) [numel] </para>
</parameterdescription>
</parameteritem>
<parameteritem>
<parameternamelist>
<parametername>numel</parametername>
</parameternamelist>
<parameterdescription>
<para>Number of elements </para>
</parameterdescription>
</parameteritem>
<parameteritem>
<parameternamelist>
<parametername>lr</parametername>
</parameternamelist>
<parameterdescription>
<para>Learning rate </para>
</parameterdescription>
</parameteritem>
<parameteritem>
<parameternamelist>
<parametername>beta1</parametername>
</parameternamelist>
<parameterdescription>
<para>Exponential decay rate for first moment (typically 0.9) </para>
</parameterdescription>
</parameteritem>
<parameteritem>
<parameternamelist>
<parametername>beta2</parametername>
</parameternamelist>
<parameterdescription>
<para>Exponential decay rate for second moment (typically 0.999) </para>
</parameterdescription>
</parameteritem>
<parameteritem>
<parameternamelist>
<parametername>eps</parametername>
</parameternamelist>
<parameterdescription>
<para>Small constant for numerical stability (typically 1e-8) </para>
</parameterdescription>
</parameteritem>
<parameteritem>
<parameternamelist>
<parametername>weight_decay</parametername>
</parameternamelist>
<parameterdescription>
<para>Weight decay coefficient (typically 0.01) </para>
</parameterdescription>
</parameteritem>
<parameteritem>
<parameternamelist>
<parametername>step</parametername>
</parameternamelist>
<parameterdescription>
<para>Current step number (1-indexed for bias correction) </para>
</parameterdescription>
</parameteritem>
</parameterlist>
</para>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="/home/antshiv/Workspace/C-Kernel-Engine/src/kernels/optimizer_kernels.c" line="43" column="6" bodyfile="/home/antshiv/Workspace/C-Kernel-Engine/src/kernels/optimizer_kernels.c" bodystart="43" bodyend="145"/>
      </memberdef>
      <memberdef kind="function" id="optimizer__kernels_8c_1ac43e9094fbc4956d450b5f6deb5ae899" prot="public" static="no" const="no" explicit="no" inline="no" virt="non-virtual">
        <type>void</type>
        <definition>void gradient_accumulate_f32</definition>
        <argsstring>(float *dst, const float *src, size_t numel)</argsstring>
        <name>gradient_accumulate_f32</name>
        <param>
          <type>float *</type>
          <declname>dst</declname>
        </param>
        <param>
          <type>const float *</type>
          <declname>src</declname>
        </param>
        <param>
          <type>size_t</type>
          <declname>numel</declname>
        </param>
        <briefdescription>
<para>Accumulate gradients: dst += src (fp32) </para>
        </briefdescription>
        <detaileddescription>
<para>Used for gradient accumulation across micro-batches.</para>
<para><parameterlist kind="param"><parameteritem>
<parameternamelist>
<parametername>dst</parametername>
</parameternamelist>
<parameterdescription>
<para>Destination gradient buffer (in-place) [numel] </para>
</parameterdescription>
</parameteritem>
<parameteritem>
<parameternamelist>
<parametername>src</parametername>
</parameternamelist>
<parameterdescription>
<para>Source gradient buffer [numel] </para>
</parameterdescription>
</parameteritem>
<parameteritem>
<parameternamelist>
<parametername>numel</parametername>
</parameternamelist>
<parameterdescription>
<para>Number of elements </para>
</parameterdescription>
</parameteritem>
</parameterlist>
</para>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="/home/antshiv/Workspace/C-Kernel-Engine/src/kernels/optimizer_kernels.c" line="234" column="6" bodyfile="/home/antshiv/Workspace/C-Kernel-Engine/src/kernels/optimizer_kernels.c" bodystart="234" bodyend="255"/>
      </memberdef>
      <memberdef kind="function" id="optimizer__kernels_8c_1a4e854ffc919637e40ea0d320ecb5ed0b" prot="public" static="no" const="no" explicit="no" inline="no" virt="non-virtual">
        <type>float</type>
        <definition>float gradient_clip_norm_f32</definition>
        <argsstring>(float *grad, size_t numel, float max_norm)</argsstring>
        <name>gradient_clip_norm_f32</name>
        <param>
          <type>float *</type>
          <declname>grad</declname>
        </param>
        <param>
          <type>size_t</type>
          <declname>numel</declname>
        </param>
        <param>
          <type>float</type>
          <declname>max_norm</declname>
        </param>
        <briefdescription>
<para>Clip gradient norm (fp32) </para>
        </briefdescription>
        <detaileddescription>
<para>If ||grad||_2 &gt; max_norm, scale grad so that ||grad||_2 = max_norm</para>
<para><parameterlist kind="param"><parameteritem>
<parameternamelist>
<parametername>grad</parametername>
</parameternamelist>
<parameterdescription>
<para>Gradient tensor to clip (in-place) [numel] </para>
</parameterdescription>
</parameteritem>
<parameteritem>
<parameternamelist>
<parametername>numel</parametername>
</parameternamelist>
<parameterdescription>
<para>Number of elements </para>
</parameterdescription>
</parameteritem>
<parameteritem>
<parameternamelist>
<parametername>max_norm</parametername>
</parameternamelist>
<parameterdescription>
<para>Maximum allowed L2 norm </para>
</parameterdescription>
</parameteritem>
</parameterlist>
<simplesect kind="return"><para>The original L2 norm before clipping </para>
</simplesect>
</para>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="/home/antshiv/Workspace/C-Kernel-Engine/src/kernels/optimizer_kernels.c" line="301" column="7" bodyfile="/home/antshiv/Workspace/C-Kernel-Engine/src/kernels/optimizer_kernels.c" bodystart="301" bodyend="335"/>
        <references refid="optimizer__kernels_8c_1ab615ca5560d68ed2f098960cd81acfc2" compoundref="optimizer__kernels_8c" startline="267" endline="288">gradient_scale_f32</references>
      </memberdef>
      <memberdef kind="function" id="optimizer__kernels_8c_1ab615ca5560d68ed2f098960cd81acfc2" prot="public" static="no" const="no" explicit="no" inline="no" virt="non-virtual">
        <type>void</type>
        <definition>void gradient_scale_f32</definition>
        <argsstring>(float *grad, size_t numel, float scale)</argsstring>
        <name>gradient_scale_f32</name>
        <param>
          <type>float *</type>
          <declname>grad</declname>
        </param>
        <param>
          <type>size_t</type>
          <declname>numel</declname>
        </param>
        <param>
          <type>float</type>
          <declname>scale</declname>
        </param>
        <briefdescription>
<para>Scale gradients by a constant: grad *= scale (fp32) </para>
        </briefdescription>
        <detaileddescription>
<para>Used for averaging gradients after accumulation: grad /= batch_size</para>
<para><parameterlist kind="param"><parameteritem>
<parameternamelist>
<parametername>grad</parametername>
</parameternamelist>
<parameterdescription>
<para>Gradient tensor to scale (in-place) [numel] </para>
</parameterdescription>
</parameteritem>
<parameteritem>
<parameternamelist>
<parametername>numel</parametername>
</parameternamelist>
<parameterdescription>
<para>Number of elements </para>
</parameterdescription>
</parameteritem>
<parameteritem>
<parameternamelist>
<parametername>scale</parametername>
</parameternamelist>
<parameterdescription>
<para>Scale factor (typically 1.0 / batch_size) </para>
</parameterdescription>
</parameteritem>
</parameterlist>
</para>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="/home/antshiv/Workspace/C-Kernel-Engine/src/kernels/optimizer_kernels.c" line="267" column="6" bodyfile="/home/antshiv/Workspace/C-Kernel-Engine/src/kernels/optimizer_kernels.c" bodystart="267" bodyend="288"/>
        <referencedby refid="optimizer__kernels_8c_1a4e854ffc919637e40ea0d320ecb5ed0b" compoundref="optimizer__kernels_8c" startline="301" endline="335">gradient_clip_norm_f32</referencedby>
      </memberdef>
      <memberdef kind="function" id="optimizer__kernels_8c_1a1c1d97635474696da41a9f318ac7600b" prot="public" static="no" const="no" explicit="no" inline="no" virt="non-virtual">
        <type>void</type>
        <definition>void sgd_momentum_update_f32</definition>
        <argsstring>(const float *grad, float *weight, float *velocity, size_t numel, float lr, float momentum, float weight_decay)</argsstring>
        <name>sgd_momentum_update_f32</name>
        <param>
          <type>const float *</type>
          <declname>grad</declname>
        </param>
        <param>
          <type>float *</type>
          <declname>weight</declname>
        </param>
        <param>
          <type>float *</type>
          <declname>velocity</declname>
        </param>
        <param>
          <type>size_t</type>
          <declname>numel</declname>
        </param>
        <param>
          <type>float</type>
          <declname>lr</declname>
        </param>
        <param>
          <type>float</type>
          <declname>momentum</declname>
        </param>
        <param>
          <type>float</type>
          <declname>weight_decay</declname>
        </param>
        <briefdescription>
<para>SGD with momentum optimizer update (fp32 version) </para>
        </briefdescription>
        <detaileddescription>
<para>v_t = momentum * v_{t-1} + g_t w_t = w_{t-1} - lr * (v_t + weight_decay * w_{t-1})</para>
<para><parameterlist kind="param"><parameteritem>
<parameternamelist>
<parametername>grad</parametername>
</parameternamelist>
<parameterdescription>
<para>Gradient tensor (fp32) [numel] </para>
</parameterdescription>
</parameteritem>
<parameteritem>
<parameternamelist>
<parametername>weight</parametername>
</parameternamelist>
<parameterdescription>
<para>Weight tensor to update (fp32, in-place) [numel] </para>
</parameterdescription>
</parameteritem>
<parameteritem>
<parameternamelist>
<parametername>velocity</parametername>
</parameternamelist>
<parameterdescription>
<para>Velocity buffer (fp32, in-place) [numel] </para>
</parameterdescription>
</parameteritem>
<parameteritem>
<parameternamelist>
<parametername>numel</parametername>
</parameternamelist>
<parameterdescription>
<para>Number of elements </para>
</parameterdescription>
</parameteritem>
<parameteritem>
<parameternamelist>
<parametername>lr</parametername>
</parameternamelist>
<parameterdescription>
<para>Learning rate </para>
</parameterdescription>
</parameteritem>
<parameteritem>
<parameternamelist>
<parametername>momentum</parametername>
</parameternamelist>
<parameterdescription>
<para>Momentum coefficient (typically 0.9) </para>
</parameterdescription>
</parameteritem>
<parameteritem>
<parameternamelist>
<parametername>weight_decay</parametername>
</parameternamelist>
<parameterdescription>
<para>Weight decay coefficient </para>
</parameterdescription>
</parameteritem>
</parameterlist>
</para>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="/home/antshiv/Workspace/C-Kernel-Engine/src/kernels/optimizer_kernels.c" line="162" column="6" bodyfile="/home/antshiv/Workspace/C-Kernel-Engine/src/kernels/optimizer_kernels.c" bodystart="162" bodyend="207"/>
      </memberdef>
      <memberdef kind="function" id="optimizer__kernels_8c_1adecdd5f34f2c7634e7e665ab3d96da37" prot="public" static="no" const="no" explicit="no" inline="no" virt="non-virtual">
        <type>void</type>
        <definition>void zero_gradients_f32</definition>
        <argsstring>(float *grad, size_t numel)</argsstring>
        <name>zero_gradients_f32</name>
        <param>
          <type>float *</type>
          <declname>grad</declname>
        </param>
        <param>
          <type>size_t</type>
          <declname>numel</declname>
        </param>
        <briefdescription>
<para>Zero out gradient buffer (fp32) </para>
        </briefdescription>
        <detaileddescription>
<para><parameterlist kind="param"><parameteritem>
<parameternamelist>
<parametername>grad</parametername>
</parameternamelist>
<parameterdescription>
<para>Gradient tensor to zero [numel] </para>
</parameterdescription>
</parameteritem>
<parameteritem>
<parameternamelist>
<parametername>numel</parametername>
</parameternamelist>
<parameterdescription>
<para>Number of elements </para>
</parameterdescription>
</parameteritem>
</parameterlist>
</para>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="/home/antshiv/Workspace/C-Kernel-Engine/src/kernels/optimizer_kernels.c" line="216" column="6" bodyfile="/home/antshiv/Workspace/C-Kernel-Engine/src/kernels/optimizer_kernels.c" bodystart="216" bodyend="222"/>
      </memberdef>
      </sectiondef>
    <briefdescription>
<para>Optimizer kernels for training (AdamW, SGD) </para>
    </briefdescription>
    <detaileddescription>
<para>AdamW Algorithm: m_t = beta1 * m_{t-1} + (1 - beta1) * g_t v_t = beta2 * v_{t-1} + (1 - beta2) * g_t^2 m_hat = m_t / (1 - beta1^t) v_hat = v_t / (1 - beta2^t) w_t = w_{t-1} - lr * (m_hat / (sqrt(v_hat) + eps) + weight_decay * w_{t-1})</para>
<para>Note: AdamW applies weight decay directly to weights, not to gradients. This is different from L2 regularization (Adam with L2 adds decay to gradient). </para>
    </detaileddescription>
    <programlisting>
<codeline lineno="1"><highlight class="comment">/**</highlight></codeline>
<codeline lineno="2"><highlight class="comment"><sp/>*<sp/>@file<sp/>optimizer_kernels.c</highlight></codeline>
<codeline lineno="3"><highlight class="comment"><sp/>*<sp/>@brief<sp/>Optimizer<sp/>kernels<sp/>for<sp/>training<sp/>(AdamW,<sp/>SGD)</highlight></codeline>
<codeline lineno="4"><highlight class="comment"><sp/>*</highlight></codeline>
<codeline lineno="5"><highlight class="comment"><sp/>*<sp/>AdamW<sp/>Algorithm:</highlight></codeline>
<codeline lineno="6"><highlight class="comment"><sp/>*<sp/><sp/><sp/>m_t<sp/>=<sp/>beta1<sp/>*<sp/>m_{t-1}<sp/>+<sp/>(1<sp/>-<sp/>beta1)<sp/>*<sp/>g_t</highlight></codeline>
<codeline lineno="7"><highlight class="comment"><sp/>*<sp/><sp/><sp/>v_t<sp/>=<sp/>beta2<sp/>*<sp/>v_{t-1}<sp/>+<sp/>(1<sp/>-<sp/>beta2)<sp/>*<sp/>g_t^2</highlight></codeline>
<codeline lineno="8"><highlight class="comment"><sp/>*<sp/><sp/><sp/>m_hat<sp/>=<sp/>m_t<sp/>/<sp/>(1<sp/>-<sp/>beta1^t)</highlight></codeline>
<codeline lineno="9"><highlight class="comment"><sp/>*<sp/><sp/><sp/>v_hat<sp/>=<sp/>v_t<sp/>/<sp/>(1<sp/>-<sp/>beta2^t)</highlight></codeline>
<codeline lineno="10"><highlight class="comment"><sp/>*<sp/><sp/><sp/>w_t<sp/>=<sp/>w_{t-1}<sp/>-<sp/>lr<sp/>*<sp/>(m_hat<sp/>/<sp/>(sqrt(v_hat)<sp/>+<sp/>eps)<sp/>+<sp/>weight_decay<sp/>*<sp/>w_{t-1})</highlight></codeline>
<codeline lineno="11"><highlight class="comment"><sp/>*</highlight></codeline>
<codeline lineno="12"><highlight class="comment"><sp/>*<sp/>Note:<sp/>AdamW<sp/>applies<sp/>weight<sp/>decay<sp/>directly<sp/>to<sp/>weights,<sp/>not<sp/>to<sp/>gradients.</highlight></codeline>
<codeline lineno="13"><highlight class="comment"><sp/>*<sp/>This<sp/>is<sp/>different<sp/>from<sp/>L2<sp/>regularization<sp/>(Adam<sp/>with<sp/>L2<sp/>adds<sp/>decay<sp/>to<sp/>gradient).</highlight></codeline>
<codeline lineno="14"><highlight class="comment"><sp/>*/</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="15"><highlight class="normal"></highlight></codeline>
<codeline lineno="16"><highlight class="normal"></highlight><highlight class="preprocessor">#include<sp/>&lt;math.h&gt;</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="17"><highlight class="normal"></highlight><highlight class="preprocessor">#include<sp/>&lt;stddef.h&gt;</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="18"><highlight class="normal"></highlight><highlight class="preprocessor">#include<sp/>&lt;stdint.h&gt;</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="19"><highlight class="normal"></highlight><highlight class="preprocessor">#include<sp/>&lt;string.h&gt;</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="20"><highlight class="normal"></highlight></codeline>
<codeline lineno="21"><highlight class="normal"></highlight><highlight class="preprocessor">#if<sp/>defined(__AVX512F__)</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="22"><highlight class="normal"></highlight><highlight class="preprocessor">#include<sp/>&lt;immintrin.h&gt;</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="23"><highlight class="normal"></highlight><highlight class="preprocessor">#endif</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="24"><highlight class="normal"></highlight><highlight class="comment"></highlight></codeline>
<codeline lineno="25"><highlight class="comment">/**</highlight></codeline>
<codeline lineno="26"><highlight class="comment"><sp/>*<sp/>@brief<sp/>AdamW<sp/>optimizer<sp/>update<sp/>(fp32<sp/>version)</highlight></codeline>
<codeline lineno="27"><highlight class="comment"><sp/>*</highlight></codeline>
<codeline lineno="28"><highlight class="comment"><sp/>*<sp/>Updates<sp/>weights<sp/>in-place<sp/>using<sp/>the<sp/>AdamW<sp/>algorithm.</highlight></codeline>
<codeline lineno="29"><highlight class="comment"><sp/>*<sp/>Momentum<sp/>(m)<sp/>and<sp/>variance<sp/>(v)<sp/>are<sp/>stored<sp/>in<sp/>fp32<sp/>for<sp/>numerical<sp/>stability.</highlight></codeline>
<codeline lineno="30"><highlight class="comment"><sp/>*</highlight></codeline>
<codeline lineno="31"><highlight class="comment"><sp/>*<sp/>@param<sp/>grad<sp/><sp/><sp/><sp/><sp/><sp/><sp/>Gradient<sp/>tensor<sp/>(fp32)<sp/>[numel]</highlight></codeline>
<codeline lineno="32"><highlight class="comment"><sp/>*<sp/>@param<sp/>weight<sp/><sp/><sp/><sp/><sp/>Weight<sp/>tensor<sp/>to<sp/>update<sp/>(fp32,<sp/>in-place)<sp/>[numel]</highlight></codeline>
<codeline lineno="33"><highlight class="comment"><sp/>*<sp/>@param<sp/>m<sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>First<sp/>moment<sp/>(momentum)<sp/>buffer<sp/>(fp32,<sp/>in-place)<sp/>[numel]</highlight></codeline>
<codeline lineno="34"><highlight class="comment"><sp/>*<sp/>@param<sp/>v<sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>Second<sp/>moment<sp/>(variance)<sp/>buffer<sp/>(fp32,<sp/>in-place)<sp/>[numel]</highlight></codeline>
<codeline lineno="35"><highlight class="comment"><sp/>*<sp/>@param<sp/>numel<sp/><sp/><sp/><sp/><sp/><sp/>Number<sp/>of<sp/>elements</highlight></codeline>
<codeline lineno="36"><highlight class="comment"><sp/>*<sp/>@param<sp/>lr<sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>Learning<sp/>rate</highlight></codeline>
<codeline lineno="37"><highlight class="comment"><sp/>*<sp/>@param<sp/>beta1<sp/><sp/><sp/><sp/><sp/><sp/>Exponential<sp/>decay<sp/>rate<sp/>for<sp/>first<sp/>moment<sp/>(typically<sp/>0.9)</highlight></codeline>
<codeline lineno="38"><highlight class="comment"><sp/>*<sp/>@param<sp/>beta2<sp/><sp/><sp/><sp/><sp/><sp/>Exponential<sp/>decay<sp/>rate<sp/>for<sp/>second<sp/>moment<sp/>(typically<sp/>0.999)</highlight></codeline>
<codeline lineno="39"><highlight class="comment"><sp/>*<sp/>@param<sp/>eps<sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>Small<sp/>constant<sp/>for<sp/>numerical<sp/>stability<sp/>(typically<sp/>1e-8)</highlight></codeline>
<codeline lineno="40"><highlight class="comment"><sp/>*<sp/>@param<sp/>weight_decay<sp/>Weight<sp/>decay<sp/>coefficient<sp/>(typically<sp/>0.01)</highlight></codeline>
<codeline lineno="41"><highlight class="comment"><sp/>*<sp/>@param<sp/>step<sp/><sp/><sp/><sp/><sp/><sp/><sp/>Current<sp/>step<sp/>number<sp/>(1-indexed<sp/>for<sp/>bias<sp/>correction)</highlight></codeline>
<codeline lineno="42"><highlight class="comment"><sp/>*/</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="43" refid="optimizer__kernels_8c_1ac1a9ed01c15286fc6e3be73f96371d76" refkind="member"><highlight class="normal"></highlight><highlight class="keywordtype">void</highlight><highlight class="normal"><sp/><ref refid="optimizer__kernels_8c_1ac1a9ed01c15286fc6e3be73f96371d76" kindref="member">adamw_update_f32</ref>(</highlight></codeline>
<codeline lineno="44"><highlight class="normal"><sp/><sp/><sp/><sp/></highlight><highlight class="keyword">const</highlight><highlight class="normal"><sp/></highlight><highlight class="keywordtype">float</highlight><highlight class="normal"><sp/>*grad,</highlight></codeline>
<codeline lineno="45"><highlight class="normal"><sp/><sp/><sp/><sp/></highlight><highlight class="keywordtype">float</highlight><highlight class="normal"><sp/>*weight,</highlight></codeline>
<codeline lineno="46"><highlight class="normal"><sp/><sp/><sp/><sp/></highlight><highlight class="keywordtype">float</highlight><highlight class="normal"><sp/>*m,</highlight></codeline>
<codeline lineno="47"><highlight class="normal"><sp/><sp/><sp/><sp/></highlight><highlight class="keywordtype">float</highlight><highlight class="normal"><sp/>*v,</highlight></codeline>
<codeline lineno="48"><highlight class="normal"><sp/><sp/><sp/><sp/></highlight><highlight class="keywordtype">size_t</highlight><highlight class="normal"><sp/>numel,</highlight></codeline>
<codeline lineno="49"><highlight class="normal"><sp/><sp/><sp/><sp/></highlight><highlight class="keywordtype">float</highlight><highlight class="normal"><sp/>lr,</highlight></codeline>
<codeline lineno="50"><highlight class="normal"><sp/><sp/><sp/><sp/></highlight><highlight class="keywordtype">float</highlight><highlight class="normal"><sp/>beta1,</highlight></codeline>
<codeline lineno="51"><highlight class="normal"><sp/><sp/><sp/><sp/></highlight><highlight class="keywordtype">float</highlight><highlight class="normal"><sp/>beta2,</highlight></codeline>
<codeline lineno="52"><highlight class="normal"><sp/><sp/><sp/><sp/></highlight><highlight class="keywordtype">float</highlight><highlight class="normal"><sp/>eps,</highlight></codeline>
<codeline lineno="53"><highlight class="normal"><sp/><sp/><sp/><sp/></highlight><highlight class="keywordtype">float</highlight><highlight class="normal"><sp/>weight_decay,</highlight></codeline>
<codeline lineno="54"><highlight class="normal"><sp/><sp/><sp/><sp/></highlight><highlight class="keywordtype">int</highlight><highlight class="normal"><sp/>step)</highlight></codeline>
<codeline lineno="55"><highlight class="normal">{</highlight></codeline>
<codeline lineno="56"><highlight class="normal"><sp/><sp/><sp/><sp/></highlight><highlight class="keywordflow">if</highlight><highlight class="normal"><sp/>(!grad<sp/>||<sp/>!weight<sp/>||<sp/>!m<sp/>||<sp/>!v<sp/>||<sp/>numel<sp/>==<sp/>0)<sp/>{</highlight></codeline>
<codeline lineno="57"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/></highlight><highlight class="keywordflow">return</highlight><highlight class="normal">;</highlight></codeline>
<codeline lineno="58"><highlight class="normal"><sp/><sp/><sp/><sp/>}</highlight></codeline>
<codeline lineno="59"><highlight class="normal"></highlight></codeline>
<codeline lineno="60"><highlight class="normal"><sp/><sp/><sp/><sp/></highlight><highlight class="comment">//<sp/>Bias<sp/>correction<sp/>terms</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="61"><highlight class="normal"><sp/><sp/><sp/><sp/></highlight><highlight class="keywordtype">float</highlight><highlight class="normal"><sp/>bias_correction1<sp/>=<sp/>1.0f<sp/>-<sp/>powf(beta1,<sp/>(</highlight><highlight class="keywordtype">float</highlight><highlight class="normal">)step);</highlight></codeline>
<codeline lineno="62"><highlight class="normal"><sp/><sp/><sp/><sp/></highlight><highlight class="keywordtype">float</highlight><highlight class="normal"><sp/>bias_correction2<sp/>=<sp/>1.0f<sp/>-<sp/>powf(beta2,<sp/>(</highlight><highlight class="keywordtype">float</highlight><highlight class="normal">)step);</highlight></codeline>
<codeline lineno="63"><highlight class="normal"></highlight></codeline>
<codeline lineno="64"><highlight class="normal"><sp/><sp/><sp/><sp/></highlight><highlight class="comment">//<sp/>Precompute<sp/>constants</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="65"><highlight class="normal"><sp/><sp/><sp/><sp/></highlight><highlight class="keywordtype">float</highlight><highlight class="normal"><sp/>one_minus_beta1<sp/>=<sp/>1.0f<sp/>-<sp/>beta1;</highlight></codeline>
<codeline lineno="66"><highlight class="normal"><sp/><sp/><sp/><sp/></highlight><highlight class="keywordtype">float</highlight><highlight class="normal"><sp/>one_minus_beta2<sp/>=<sp/>1.0f<sp/>-<sp/>beta2;</highlight></codeline>
<codeline lineno="67"><highlight class="normal"></highlight></codeline>
<codeline lineno="68"><highlight class="normal"></highlight><highlight class="preprocessor">#if<sp/>defined(__AVX512F__)</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="69"><highlight class="normal"><sp/><sp/><sp/><sp/></highlight><highlight class="comment">//<sp/>Vectorized<sp/>path:<sp/>process<sp/>16<sp/>floats<sp/>at<sp/>a<sp/>time</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="70"><highlight class="normal"><sp/><sp/><sp/><sp/>__m512<sp/>v_beta1<sp/>=<sp/>_mm512_set1_ps(beta1);</highlight></codeline>
<codeline lineno="71"><highlight class="normal"><sp/><sp/><sp/><sp/>__m512<sp/>v_beta2<sp/>=<sp/>_mm512_set1_ps(beta2);</highlight></codeline>
<codeline lineno="72"><highlight class="normal"><sp/><sp/><sp/><sp/>__m512<sp/>v_one_minus_beta1<sp/>=<sp/>_mm512_set1_ps(one_minus_beta1);</highlight></codeline>
<codeline lineno="73"><highlight class="normal"><sp/><sp/><sp/><sp/>__m512<sp/>v_one_minus_beta2<sp/>=<sp/>_mm512_set1_ps(one_minus_beta2);</highlight></codeline>
<codeline lineno="74"><highlight class="normal"><sp/><sp/><sp/><sp/>__m512<sp/>v_lr<sp/>=<sp/>_mm512_set1_ps(lr);</highlight></codeline>
<codeline lineno="75"><highlight class="normal"><sp/><sp/><sp/><sp/>__m512<sp/>v_eps<sp/>=<sp/>_mm512_set1_ps(eps);</highlight></codeline>
<codeline lineno="76"><highlight class="normal"><sp/><sp/><sp/><sp/>__m512<sp/>v_weight_decay<sp/>=<sp/>_mm512_set1_ps(weight_decay);</highlight></codeline>
<codeline lineno="77"><highlight class="normal"><sp/><sp/><sp/><sp/>__m512<sp/>v_bc1_inv<sp/>=<sp/>_mm512_set1_ps(1.0f<sp/>/<sp/>bias_correction1);</highlight></codeline>
<codeline lineno="78"><highlight class="normal"><sp/><sp/><sp/><sp/>__m512<sp/>v_bc2_inv<sp/>=<sp/>_mm512_set1_ps(1.0f<sp/>/<sp/>bias_correction2);</highlight></codeline>
<codeline lineno="79"><highlight class="normal"></highlight></codeline>
<codeline lineno="80"><highlight class="normal"><sp/><sp/><sp/><sp/></highlight><highlight class="keywordtype">size_t</highlight><highlight class="normal"><sp/>i<sp/>=<sp/>0;</highlight></codeline>
<codeline lineno="81"><highlight class="normal"><sp/><sp/><sp/><sp/></highlight><highlight class="keywordflow">for</highlight><highlight class="normal"><sp/>(;<sp/>i<sp/>+<sp/>16<sp/>&lt;=<sp/>numel;<sp/>i<sp/>+=<sp/>16)<sp/>{</highlight></codeline>
<codeline lineno="82"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/></highlight><highlight class="comment">//<sp/>Load<sp/>gradient,<sp/>weight,<sp/>m,<sp/>v</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="83"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>__m512<sp/>g<sp/>=<sp/>_mm512_loadu_ps(&amp;grad[i]);</highlight></codeline>
<codeline lineno="84"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>__m512<sp/>w<sp/>=<sp/>_mm512_loadu_ps(&amp;weight[i]);</highlight></codeline>
<codeline lineno="85"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>__m512<sp/>m_val<sp/>=<sp/>_mm512_loadu_ps(&amp;m[i]);</highlight></codeline>
<codeline lineno="86"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>__m512<sp/>v_val<sp/>=<sp/>_mm512_loadu_ps(&amp;v[i]);</highlight></codeline>
<codeline lineno="87"><highlight class="normal"></highlight></codeline>
<codeline lineno="88"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/></highlight><highlight class="comment">//<sp/>Update<sp/>m:<sp/>m<sp/>=<sp/>beta1<sp/>*<sp/>m<sp/>+<sp/>(1<sp/>-<sp/>beta1)<sp/>*<sp/>g</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="89"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>m_val<sp/>=<sp/>_mm512_fmadd_ps(v_beta1,<sp/>m_val,<sp/>_mm512_mul_ps(v_one_minus_beta1,<sp/>g));</highlight></codeline>
<codeline lineno="90"><highlight class="normal"></highlight></codeline>
<codeline lineno="91"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/></highlight><highlight class="comment">//<sp/>Update<sp/>v:<sp/>v<sp/>=<sp/>beta2<sp/>*<sp/>v<sp/>+<sp/>(1<sp/>-<sp/>beta2)<sp/>*<sp/>g^2</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="92"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>__m512<sp/>g_sq<sp/>=<sp/>_mm512_mul_ps(g,<sp/>g);</highlight></codeline>
<codeline lineno="93"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>v_val<sp/>=<sp/>_mm512_fmadd_ps(v_beta2,<sp/>v_val,<sp/>_mm512_mul_ps(v_one_minus_beta2,<sp/>g_sq));</highlight></codeline>
<codeline lineno="94"><highlight class="normal"></highlight></codeline>
<codeline lineno="95"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/></highlight><highlight class="comment">//<sp/>Bias-corrected<sp/>estimates</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="96"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>__m512<sp/>m_hat<sp/>=<sp/>_mm512_mul_ps(m_val,<sp/>v_bc1_inv);</highlight></codeline>
<codeline lineno="97"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>__m512<sp/>v_hat<sp/>=<sp/>_mm512_mul_ps(v_val,<sp/>v_bc2_inv);</highlight></codeline>
<codeline lineno="98"><highlight class="normal"></highlight></codeline>
<codeline lineno="99"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/></highlight><highlight class="comment">//<sp/>Update<sp/>weight:<sp/>w<sp/>=<sp/>w<sp/>-<sp/>lr<sp/>*<sp/>(m_hat<sp/>/<sp/>(sqrt(v_hat)<sp/>+<sp/>eps)<sp/>+<sp/>weight_decay<sp/>*<sp/>w)</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="100"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>__m512<sp/>denom<sp/>=<sp/>_mm512_add_ps(_mm512_sqrt_ps(v_hat),<sp/>v_eps);</highlight></codeline>
<codeline lineno="101"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>__m512<sp/>update<sp/>=<sp/>_mm512_div_ps(m_hat,<sp/>denom);</highlight></codeline>
<codeline lineno="102"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>update<sp/>=<sp/>_mm512_fmadd_ps(v_weight_decay,<sp/>w,<sp/>update);<sp/><sp/></highlight><highlight class="comment">//<sp/>+<sp/>weight_decay<sp/>*<sp/>w</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="103"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>w<sp/>=<sp/>_mm512_fnmadd_ps(v_lr,<sp/>update,<sp/>w);<sp/><sp/></highlight><highlight class="comment">//<sp/>w<sp/>-<sp/>lr<sp/>*<sp/>update</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="104"><highlight class="normal"></highlight></codeline>
<codeline lineno="105"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/></highlight><highlight class="comment">//<sp/>Store<sp/>updated<sp/>values</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="106"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>_mm512_storeu_ps(&amp;weight[i],<sp/>w);</highlight></codeline>
<codeline lineno="107"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>_mm512_storeu_ps(&amp;m[i],<sp/>m_val);</highlight></codeline>
<codeline lineno="108"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>_mm512_storeu_ps(&amp;v[i],<sp/>v_val);</highlight></codeline>
<codeline lineno="109"><highlight class="normal"><sp/><sp/><sp/><sp/>}</highlight></codeline>
<codeline lineno="110"><highlight class="normal"></highlight></codeline>
<codeline lineno="111"><highlight class="normal"><sp/><sp/><sp/><sp/></highlight><highlight class="comment">//<sp/>Scalar<sp/>tail</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="112"><highlight class="normal"><sp/><sp/><sp/><sp/></highlight><highlight class="keywordflow">for</highlight><highlight class="normal"><sp/>(;<sp/>i<sp/>&lt;<sp/>numel;<sp/>++i)<sp/>{</highlight></codeline>
<codeline lineno="113"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/></highlight><highlight class="keywordtype">float</highlight><highlight class="normal"><sp/>g<sp/>=<sp/>grad[i];</highlight></codeline>
<codeline lineno="114"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/></highlight><highlight class="keywordtype">float</highlight><highlight class="normal"><sp/>w<sp/>=<sp/>weight[i];</highlight></codeline>
<codeline lineno="115"><highlight class="normal"></highlight></codeline>
<codeline lineno="116"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/></highlight><highlight class="comment">//<sp/>Update<sp/>m<sp/>and<sp/>v</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="117"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>m[i]<sp/>=<sp/>beta1<sp/>*<sp/>m[i]<sp/>+<sp/>one_minus_beta1<sp/>*<sp/>g;</highlight></codeline>
<codeline lineno="118"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>v[i]<sp/>=<sp/>beta2<sp/>*<sp/>v[i]<sp/>+<sp/>one_minus_beta2<sp/>*<sp/>g<sp/>*<sp/>g;</highlight></codeline>
<codeline lineno="119"><highlight class="normal"></highlight></codeline>
<codeline lineno="120"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/></highlight><highlight class="comment">//<sp/>Bias-corrected<sp/>estimates</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="121"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/></highlight><highlight class="keywordtype">float</highlight><highlight class="normal"><sp/>m_hat<sp/>=<sp/>m[i]<sp/>/<sp/>bias_correction1;</highlight></codeline>
<codeline lineno="122"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/></highlight><highlight class="keywordtype">float</highlight><highlight class="normal"><sp/>v_hat<sp/>=<sp/>v[i]<sp/>/<sp/>bias_correction2;</highlight></codeline>
<codeline lineno="123"><highlight class="normal"></highlight></codeline>
<codeline lineno="124"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/></highlight><highlight class="comment">//<sp/>Update<sp/>weight</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="125"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>weight[i]<sp/>=<sp/>w<sp/>-<sp/>lr<sp/>*<sp/>(m_hat<sp/>/<sp/>(sqrtf(v_hat)<sp/>+<sp/>eps)<sp/>+<sp/>weight_decay<sp/>*<sp/>w);</highlight></codeline>
<codeline lineno="126"><highlight class="normal"><sp/><sp/><sp/><sp/>}</highlight></codeline>
<codeline lineno="127"><highlight class="normal"></highlight><highlight class="preprocessor">#else</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="128"><highlight class="normal"><sp/><sp/><sp/><sp/></highlight><highlight class="comment">//<sp/>Scalar<sp/>path</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="129"><highlight class="normal"><sp/><sp/><sp/><sp/></highlight><highlight class="keywordflow">for</highlight><highlight class="normal"><sp/>(</highlight><highlight class="keywordtype">size_t</highlight><highlight class="normal"><sp/>i<sp/>=<sp/>0;<sp/>i<sp/>&lt;<sp/>numel;<sp/>++i)<sp/>{</highlight></codeline>
<codeline lineno="130"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/></highlight><highlight class="keywordtype">float</highlight><highlight class="normal"><sp/>g<sp/>=<sp/>grad[i];</highlight></codeline>
<codeline lineno="131"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/></highlight><highlight class="keywordtype">float</highlight><highlight class="normal"><sp/>w<sp/>=<sp/>weight[i];</highlight></codeline>
<codeline lineno="132"><highlight class="normal"></highlight></codeline>
<codeline lineno="133"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/></highlight><highlight class="comment">//<sp/>Update<sp/>m<sp/>and<sp/>v</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="134"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>m[i]<sp/>=<sp/>beta1<sp/>*<sp/>m[i]<sp/>+<sp/>one_minus_beta1<sp/>*<sp/>g;</highlight></codeline>
<codeline lineno="135"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>v[i]<sp/>=<sp/>beta2<sp/>*<sp/>v[i]<sp/>+<sp/>one_minus_beta2<sp/>*<sp/>g<sp/>*<sp/>g;</highlight></codeline>
<codeline lineno="136"><highlight class="normal"></highlight></codeline>
<codeline lineno="137"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/></highlight><highlight class="comment">//<sp/>Bias-corrected<sp/>estimates</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="138"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/></highlight><highlight class="keywordtype">float</highlight><highlight class="normal"><sp/>m_hat<sp/>=<sp/>m[i]<sp/>/<sp/>bias_correction1;</highlight></codeline>
<codeline lineno="139"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/></highlight><highlight class="keywordtype">float</highlight><highlight class="normal"><sp/>v_hat<sp/>=<sp/>v[i]<sp/>/<sp/>bias_correction2;</highlight></codeline>
<codeline lineno="140"><highlight class="normal"></highlight></codeline>
<codeline lineno="141"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/></highlight><highlight class="comment">//<sp/>Update<sp/>weight</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="142"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>weight[i]<sp/>=<sp/>w<sp/>-<sp/>lr<sp/>*<sp/>(m_hat<sp/>/<sp/>(sqrtf(v_hat)<sp/>+<sp/>eps)<sp/>+<sp/>weight_decay<sp/>*<sp/>w);</highlight></codeline>
<codeline lineno="143"><highlight class="normal"><sp/><sp/><sp/><sp/>}</highlight></codeline>
<codeline lineno="144"><highlight class="normal"></highlight><highlight class="preprocessor">#endif</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="145"><highlight class="normal">}</highlight></codeline>
<codeline lineno="146"><highlight class="normal"></highlight></codeline>
<codeline lineno="147"><highlight class="normal"></highlight><highlight class="comment"></highlight></codeline>
<codeline lineno="148"><highlight class="comment">/**</highlight></codeline>
<codeline lineno="149"><highlight class="comment"><sp/>*<sp/>@brief<sp/>SGD<sp/>with<sp/>momentum<sp/>optimizer<sp/>update<sp/>(fp32<sp/>version)</highlight></codeline>
<codeline lineno="150"><highlight class="comment"><sp/>*</highlight></codeline>
<codeline lineno="151"><highlight class="comment"><sp/>*<sp/>v_t<sp/>=<sp/>momentum<sp/>*<sp/>v_{t-1}<sp/>+<sp/>g_t</highlight></codeline>
<codeline lineno="152"><highlight class="comment"><sp/>*<sp/>w_t<sp/>=<sp/>w_{t-1}<sp/>-<sp/>lr<sp/>*<sp/>(v_t<sp/>+<sp/>weight_decay<sp/>*<sp/>w_{t-1})</highlight></codeline>
<codeline lineno="153"><highlight class="comment"><sp/>*</highlight></codeline>
<codeline lineno="154"><highlight class="comment"><sp/>*<sp/>@param<sp/>grad<sp/><sp/><sp/><sp/><sp/><sp/><sp/>Gradient<sp/>tensor<sp/>(fp32)<sp/>[numel]</highlight></codeline>
<codeline lineno="155"><highlight class="comment"><sp/>*<sp/>@param<sp/>weight<sp/><sp/><sp/><sp/><sp/>Weight<sp/>tensor<sp/>to<sp/>update<sp/>(fp32,<sp/>in-place)<sp/>[numel]</highlight></codeline>
<codeline lineno="156"><highlight class="comment"><sp/>*<sp/>@param<sp/>velocity<sp/><sp/><sp/>Velocity<sp/>buffer<sp/>(fp32,<sp/>in-place)<sp/>[numel]</highlight></codeline>
<codeline lineno="157"><highlight class="comment"><sp/>*<sp/>@param<sp/>numel<sp/><sp/><sp/><sp/><sp/><sp/>Number<sp/>of<sp/>elements</highlight></codeline>
<codeline lineno="158"><highlight class="comment"><sp/>*<sp/>@param<sp/>lr<sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>Learning<sp/>rate</highlight></codeline>
<codeline lineno="159"><highlight class="comment"><sp/>*<sp/>@param<sp/>momentum<sp/><sp/><sp/>Momentum<sp/>coefficient<sp/>(typically<sp/>0.9)</highlight></codeline>
<codeline lineno="160"><highlight class="comment"><sp/>*<sp/>@param<sp/>weight_decay<sp/>Weight<sp/>decay<sp/>coefficient</highlight></codeline>
<codeline lineno="161"><highlight class="comment"><sp/>*/</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="162" refid="optimizer__kernels_8c_1a1c1d97635474696da41a9f318ac7600b" refkind="member"><highlight class="normal"></highlight><highlight class="keywordtype">void</highlight><highlight class="normal"><sp/><ref refid="optimizer__kernels_8c_1a1c1d97635474696da41a9f318ac7600b" kindref="member">sgd_momentum_update_f32</ref>(</highlight></codeline>
<codeline lineno="163"><highlight class="normal"><sp/><sp/><sp/><sp/></highlight><highlight class="keyword">const</highlight><highlight class="normal"><sp/></highlight><highlight class="keywordtype">float</highlight><highlight class="normal"><sp/>*grad,</highlight></codeline>
<codeline lineno="164"><highlight class="normal"><sp/><sp/><sp/><sp/></highlight><highlight class="keywordtype">float</highlight><highlight class="normal"><sp/>*weight,</highlight></codeline>
<codeline lineno="165"><highlight class="normal"><sp/><sp/><sp/><sp/></highlight><highlight class="keywordtype">float</highlight><highlight class="normal"><sp/>*velocity,</highlight></codeline>
<codeline lineno="166"><highlight class="normal"><sp/><sp/><sp/><sp/></highlight><highlight class="keywordtype">size_t</highlight><highlight class="normal"><sp/>numel,</highlight></codeline>
<codeline lineno="167"><highlight class="normal"><sp/><sp/><sp/><sp/></highlight><highlight class="keywordtype">float</highlight><highlight class="normal"><sp/>lr,</highlight></codeline>
<codeline lineno="168"><highlight class="normal"><sp/><sp/><sp/><sp/></highlight><highlight class="keywordtype">float</highlight><highlight class="normal"><sp/>momentum,</highlight></codeline>
<codeline lineno="169"><highlight class="normal"><sp/><sp/><sp/><sp/></highlight><highlight class="keywordtype">float</highlight><highlight class="normal"><sp/>weight_decay)</highlight></codeline>
<codeline lineno="170"><highlight class="normal">{</highlight></codeline>
<codeline lineno="171"><highlight class="normal"><sp/><sp/><sp/><sp/></highlight><highlight class="keywordflow">if</highlight><highlight class="normal"><sp/>(!grad<sp/>||<sp/>!weight<sp/>||<sp/>!velocity<sp/>||<sp/>numel<sp/>==<sp/>0)<sp/>{</highlight></codeline>
<codeline lineno="172"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/></highlight><highlight class="keywordflow">return</highlight><highlight class="normal">;</highlight></codeline>
<codeline lineno="173"><highlight class="normal"><sp/><sp/><sp/><sp/>}</highlight></codeline>
<codeline lineno="174"><highlight class="normal"></highlight></codeline>
<codeline lineno="175"><highlight class="normal"></highlight><highlight class="preprocessor">#if<sp/>defined(__AVX512F__)</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="176"><highlight class="normal"><sp/><sp/><sp/><sp/>__m512<sp/>v_lr<sp/>=<sp/>_mm512_set1_ps(lr);</highlight></codeline>
<codeline lineno="177"><highlight class="normal"><sp/><sp/><sp/><sp/>__m512<sp/>v_momentum<sp/>=<sp/>_mm512_set1_ps(momentum);</highlight></codeline>
<codeline lineno="178"><highlight class="normal"><sp/><sp/><sp/><sp/>__m512<sp/>v_weight_decay<sp/>=<sp/>_mm512_set1_ps(weight_decay);</highlight></codeline>
<codeline lineno="179"><highlight class="normal"></highlight></codeline>
<codeline lineno="180"><highlight class="normal"><sp/><sp/><sp/><sp/></highlight><highlight class="keywordtype">size_t</highlight><highlight class="normal"><sp/>i<sp/>=<sp/>0;</highlight></codeline>
<codeline lineno="181"><highlight class="normal"><sp/><sp/><sp/><sp/></highlight><highlight class="keywordflow">for</highlight><highlight class="normal"><sp/>(;<sp/>i<sp/>+<sp/>16<sp/>&lt;=<sp/>numel;<sp/>i<sp/>+=<sp/>16)<sp/>{</highlight></codeline>
<codeline lineno="182"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>__m512<sp/>g<sp/>=<sp/>_mm512_loadu_ps(&amp;grad[i]);</highlight></codeline>
<codeline lineno="183"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>__m512<sp/>w<sp/>=<sp/>_mm512_loadu_ps(&amp;weight[i]);</highlight></codeline>
<codeline lineno="184"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>__m512<sp/>vel<sp/>=<sp/>_mm512_loadu_ps(&amp;velocity[i]);</highlight></codeline>
<codeline lineno="185"><highlight class="normal"></highlight></codeline>
<codeline lineno="186"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/></highlight><highlight class="comment">//<sp/>v<sp/>=<sp/>momentum<sp/>*<sp/>v<sp/>+<sp/>g</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="187"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>vel<sp/>=<sp/>_mm512_fmadd_ps(v_momentum,<sp/>vel,<sp/>g);</highlight></codeline>
<codeline lineno="188"><highlight class="normal"></highlight></codeline>
<codeline lineno="189"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/></highlight><highlight class="comment">//<sp/>w<sp/>=<sp/>w<sp/>-<sp/>lr<sp/>*<sp/>(v<sp/>+<sp/>weight_decay<sp/>*<sp/>w)</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="190"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>__m512<sp/>update<sp/>=<sp/>_mm512_fmadd_ps(v_weight_decay,<sp/>w,<sp/>vel);</highlight></codeline>
<codeline lineno="191"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>w<sp/>=<sp/>_mm512_fnmadd_ps(v_lr,<sp/>update,<sp/>w);</highlight></codeline>
<codeline lineno="192"><highlight class="normal"></highlight></codeline>
<codeline lineno="193"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>_mm512_storeu_ps(&amp;weight[i],<sp/>w);</highlight></codeline>
<codeline lineno="194"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>_mm512_storeu_ps(&amp;velocity[i],<sp/>vel);</highlight></codeline>
<codeline lineno="195"><highlight class="normal"><sp/><sp/><sp/><sp/>}</highlight></codeline>
<codeline lineno="196"><highlight class="normal"></highlight></codeline>
<codeline lineno="197"><highlight class="normal"><sp/><sp/><sp/><sp/></highlight><highlight class="keywordflow">for</highlight><highlight class="normal"><sp/>(;<sp/>i<sp/>&lt;<sp/>numel;<sp/>++i)<sp/>{</highlight></codeline>
<codeline lineno="198"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>velocity[i]<sp/>=<sp/>momentum<sp/>*<sp/>velocity[i]<sp/>+<sp/>grad[i];</highlight></codeline>
<codeline lineno="199"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>weight[i]<sp/>=<sp/>weight[i]<sp/>-<sp/>lr<sp/>*<sp/>(velocity[i]<sp/>+<sp/>weight_decay<sp/>*<sp/>weight[i]);</highlight></codeline>
<codeline lineno="200"><highlight class="normal"><sp/><sp/><sp/><sp/>}</highlight></codeline>
<codeline lineno="201"><highlight class="normal"></highlight><highlight class="preprocessor">#else</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="202"><highlight class="normal"><sp/><sp/><sp/><sp/></highlight><highlight class="keywordflow">for</highlight><highlight class="normal"><sp/>(</highlight><highlight class="keywordtype">size_t</highlight><highlight class="normal"><sp/>i<sp/>=<sp/>0;<sp/>i<sp/>&lt;<sp/>numel;<sp/>++i)<sp/>{</highlight></codeline>
<codeline lineno="203"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>velocity[i]<sp/>=<sp/>momentum<sp/>*<sp/>velocity[i]<sp/>+<sp/>grad[i];</highlight></codeline>
<codeline lineno="204"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>weight[i]<sp/>=<sp/>weight[i]<sp/>-<sp/>lr<sp/>*<sp/>(velocity[i]<sp/>+<sp/>weight_decay<sp/>*<sp/>weight[i]);</highlight></codeline>
<codeline lineno="205"><highlight class="normal"><sp/><sp/><sp/><sp/>}</highlight></codeline>
<codeline lineno="206"><highlight class="normal"></highlight><highlight class="preprocessor">#endif</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="207"><highlight class="normal">}</highlight></codeline>
<codeline lineno="208"><highlight class="normal"></highlight></codeline>
<codeline lineno="209"><highlight class="normal"></highlight><highlight class="comment"></highlight></codeline>
<codeline lineno="210"><highlight class="comment">/**</highlight></codeline>
<codeline lineno="211"><highlight class="comment"><sp/>*<sp/>@brief<sp/>Zero<sp/>out<sp/>gradient<sp/>buffer<sp/>(fp32)</highlight></codeline>
<codeline lineno="212"><highlight class="comment"><sp/>*</highlight></codeline>
<codeline lineno="213"><highlight class="comment"><sp/>*<sp/>@param<sp/>grad<sp/><sp/>Gradient<sp/>tensor<sp/>to<sp/>zero<sp/>[numel]</highlight></codeline>
<codeline lineno="214"><highlight class="comment"><sp/>*<sp/>@param<sp/>numel<sp/>Number<sp/>of<sp/>elements</highlight></codeline>
<codeline lineno="215"><highlight class="comment"><sp/>*/</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="216" refid="optimizer__kernels_8c_1adecdd5f34f2c7634e7e665ab3d96da37" refkind="member"><highlight class="normal"></highlight><highlight class="keywordtype">void</highlight><highlight class="normal"><sp/><ref refid="optimizer__kernels_8c_1adecdd5f34f2c7634e7e665ab3d96da37" kindref="member">zero_gradients_f32</ref>(</highlight><highlight class="keywordtype">float</highlight><highlight class="normal"><sp/>*grad,<sp/></highlight><highlight class="keywordtype">size_t</highlight><highlight class="normal"><sp/>numel)</highlight></codeline>
<codeline lineno="217"><highlight class="normal">{</highlight></codeline>
<codeline lineno="218"><highlight class="normal"><sp/><sp/><sp/><sp/></highlight><highlight class="keywordflow">if</highlight><highlight class="normal"><sp/>(!grad<sp/>||<sp/>numel<sp/>==<sp/>0)<sp/>{</highlight></codeline>
<codeline lineno="219"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/></highlight><highlight class="keywordflow">return</highlight><highlight class="normal">;</highlight></codeline>
<codeline lineno="220"><highlight class="normal"><sp/><sp/><sp/><sp/>}</highlight></codeline>
<codeline lineno="221"><highlight class="normal"><sp/><sp/><sp/><sp/>memset(grad,<sp/>0,<sp/>numel<sp/>*<sp/></highlight><highlight class="keyword">sizeof</highlight><highlight class="normal">(</highlight><highlight class="keywordtype">float</highlight><highlight class="normal">));</highlight></codeline>
<codeline lineno="222"><highlight class="normal">}</highlight></codeline>
<codeline lineno="223"><highlight class="normal"></highlight></codeline>
<codeline lineno="224"><highlight class="normal"></highlight><highlight class="comment"></highlight></codeline>
<codeline lineno="225"><highlight class="comment">/**</highlight></codeline>
<codeline lineno="226"><highlight class="comment"><sp/>*<sp/>@brief<sp/>Accumulate<sp/>gradients:<sp/>dst<sp/>+=<sp/>src<sp/>(fp32)</highlight></codeline>
<codeline lineno="227"><highlight class="comment"><sp/>*</highlight></codeline>
<codeline lineno="228"><highlight class="comment"><sp/>*<sp/>Used<sp/>for<sp/>gradient<sp/>accumulation<sp/>across<sp/>micro-batches.</highlight></codeline>
<codeline lineno="229"><highlight class="comment"><sp/>*</highlight></codeline>
<codeline lineno="230"><highlight class="comment"><sp/>*<sp/>@param<sp/>dst<sp/><sp/><sp/>Destination<sp/>gradient<sp/>buffer<sp/>(in-place)<sp/>[numel]</highlight></codeline>
<codeline lineno="231"><highlight class="comment"><sp/>*<sp/>@param<sp/>src<sp/><sp/><sp/>Source<sp/>gradient<sp/>buffer<sp/>[numel]</highlight></codeline>
<codeline lineno="232"><highlight class="comment"><sp/>*<sp/>@param<sp/>numel<sp/>Number<sp/>of<sp/>elements</highlight></codeline>
<codeline lineno="233"><highlight class="comment"><sp/>*/</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="234" refid="optimizer__kernels_8c_1ac43e9094fbc4956d450b5f6deb5ae899" refkind="member"><highlight class="normal"></highlight><highlight class="keywordtype">void</highlight><highlight class="normal"><sp/><ref refid="optimizer__kernels_8c_1ac43e9094fbc4956d450b5f6deb5ae899" kindref="member">gradient_accumulate_f32</ref>(</highlight><highlight class="keywordtype">float</highlight><highlight class="normal"><sp/>*dst,<sp/></highlight><highlight class="keyword">const</highlight><highlight class="normal"><sp/></highlight><highlight class="keywordtype">float</highlight><highlight class="normal"><sp/>*src,<sp/></highlight><highlight class="keywordtype">size_t</highlight><highlight class="normal"><sp/>numel)</highlight></codeline>
<codeline lineno="235"><highlight class="normal">{</highlight></codeline>
<codeline lineno="236"><highlight class="normal"><sp/><sp/><sp/><sp/></highlight><highlight class="keywordflow">if</highlight><highlight class="normal"><sp/>(!dst<sp/>||<sp/>!src<sp/>||<sp/>numel<sp/>==<sp/>0)<sp/>{</highlight></codeline>
<codeline lineno="237"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/></highlight><highlight class="keywordflow">return</highlight><highlight class="normal">;</highlight></codeline>
<codeline lineno="238"><highlight class="normal"><sp/><sp/><sp/><sp/>}</highlight></codeline>
<codeline lineno="239"><highlight class="normal"></highlight></codeline>
<codeline lineno="240"><highlight class="normal"></highlight><highlight class="preprocessor">#if<sp/>defined(__AVX512F__)</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="241"><highlight class="normal"><sp/><sp/><sp/><sp/></highlight><highlight class="keywordtype">size_t</highlight><highlight class="normal"><sp/>i<sp/>=<sp/>0;</highlight></codeline>
<codeline lineno="242"><highlight class="normal"><sp/><sp/><sp/><sp/></highlight><highlight class="keywordflow">for</highlight><highlight class="normal"><sp/>(;<sp/>i<sp/>+<sp/>16<sp/>&lt;=<sp/>numel;<sp/>i<sp/>+=<sp/>16)<sp/>{</highlight></codeline>
<codeline lineno="243"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>__m512<sp/>d<sp/>=<sp/>_mm512_loadu_ps(&amp;dst[i]);</highlight></codeline>
<codeline lineno="244"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>__m512<sp/>s<sp/>=<sp/>_mm512_loadu_ps(&amp;src[i]);</highlight></codeline>
<codeline lineno="245"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>_mm512_storeu_ps(&amp;dst[i],<sp/>_mm512_add_ps(d,<sp/>s));</highlight></codeline>
<codeline lineno="246"><highlight class="normal"><sp/><sp/><sp/><sp/>}</highlight></codeline>
<codeline lineno="247"><highlight class="normal"><sp/><sp/><sp/><sp/></highlight><highlight class="keywordflow">for</highlight><highlight class="normal"><sp/>(;<sp/>i<sp/>&lt;<sp/>numel;<sp/>++i)<sp/>{</highlight></codeline>
<codeline lineno="248"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>dst[i]<sp/>+=<sp/>src[i];</highlight></codeline>
<codeline lineno="249"><highlight class="normal"><sp/><sp/><sp/><sp/>}</highlight></codeline>
<codeline lineno="250"><highlight class="normal"></highlight><highlight class="preprocessor">#else</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="251"><highlight class="normal"><sp/><sp/><sp/><sp/></highlight><highlight class="keywordflow">for</highlight><highlight class="normal"><sp/>(</highlight><highlight class="keywordtype">size_t</highlight><highlight class="normal"><sp/>i<sp/>=<sp/>0;<sp/>i<sp/>&lt;<sp/>numel;<sp/>++i)<sp/>{</highlight></codeline>
<codeline lineno="252"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>dst[i]<sp/>+=<sp/>src[i];</highlight></codeline>
<codeline lineno="253"><highlight class="normal"><sp/><sp/><sp/><sp/>}</highlight></codeline>
<codeline lineno="254"><highlight class="normal"></highlight><highlight class="preprocessor">#endif</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="255"><highlight class="normal">}</highlight></codeline>
<codeline lineno="256"><highlight class="normal"></highlight></codeline>
<codeline lineno="257"><highlight class="normal"></highlight><highlight class="comment"></highlight></codeline>
<codeline lineno="258"><highlight class="comment">/**</highlight></codeline>
<codeline lineno="259"><highlight class="comment"><sp/>*<sp/>@brief<sp/>Scale<sp/>gradients<sp/>by<sp/>a<sp/>constant:<sp/>grad<sp/>*=<sp/>scale<sp/>(fp32)</highlight></codeline>
<codeline lineno="260"><highlight class="comment"><sp/>*</highlight></codeline>
<codeline lineno="261"><highlight class="comment"><sp/>*<sp/>Used<sp/>for<sp/>averaging<sp/>gradients<sp/>after<sp/>accumulation:<sp/>grad<sp/>/=<sp/>batch_size</highlight></codeline>
<codeline lineno="262"><highlight class="comment"><sp/>*</highlight></codeline>
<codeline lineno="263"><highlight class="comment"><sp/>*<sp/>@param<sp/>grad<sp/><sp/>Gradient<sp/>tensor<sp/>to<sp/>scale<sp/>(in-place)<sp/>[numel]</highlight></codeline>
<codeline lineno="264"><highlight class="comment"><sp/>*<sp/>@param<sp/>numel<sp/>Number<sp/>of<sp/>elements</highlight></codeline>
<codeline lineno="265"><highlight class="comment"><sp/>*<sp/>@param<sp/>scale<sp/>Scale<sp/>factor<sp/>(typically<sp/>1.0<sp/>/<sp/>batch_size)</highlight></codeline>
<codeline lineno="266"><highlight class="comment"><sp/>*/</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="267" refid="optimizer__kernels_8c_1ab615ca5560d68ed2f098960cd81acfc2" refkind="member"><highlight class="normal"></highlight><highlight class="keywordtype">void</highlight><highlight class="normal"><sp/><ref refid="optimizer__kernels_8c_1ab615ca5560d68ed2f098960cd81acfc2" kindref="member">gradient_scale_f32</ref>(</highlight><highlight class="keywordtype">float</highlight><highlight class="normal"><sp/>*grad,<sp/></highlight><highlight class="keywordtype">size_t</highlight><highlight class="normal"><sp/>numel,<sp/></highlight><highlight class="keywordtype">float</highlight><highlight class="normal"><sp/>scale)</highlight></codeline>
<codeline lineno="268"><highlight class="normal">{</highlight></codeline>
<codeline lineno="269"><highlight class="normal"><sp/><sp/><sp/><sp/></highlight><highlight class="keywordflow">if</highlight><highlight class="normal"><sp/>(!grad<sp/>||<sp/>numel<sp/>==<sp/>0)<sp/>{</highlight></codeline>
<codeline lineno="270"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/></highlight><highlight class="keywordflow">return</highlight><highlight class="normal">;</highlight></codeline>
<codeline lineno="271"><highlight class="normal"><sp/><sp/><sp/><sp/>}</highlight></codeline>
<codeline lineno="272"><highlight class="normal"></highlight></codeline>
<codeline lineno="273"><highlight class="normal"></highlight><highlight class="preprocessor">#if<sp/>defined(__AVX512F__)</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="274"><highlight class="normal"><sp/><sp/><sp/><sp/>__m512<sp/>v_scale<sp/>=<sp/>_mm512_set1_ps(scale);</highlight></codeline>
<codeline lineno="275"><highlight class="normal"><sp/><sp/><sp/><sp/></highlight><highlight class="keywordtype">size_t</highlight><highlight class="normal"><sp/>i<sp/>=<sp/>0;</highlight></codeline>
<codeline lineno="276"><highlight class="normal"><sp/><sp/><sp/><sp/></highlight><highlight class="keywordflow">for</highlight><highlight class="normal"><sp/>(;<sp/>i<sp/>+<sp/>16<sp/>&lt;=<sp/>numel;<sp/>i<sp/>+=<sp/>16)<sp/>{</highlight></codeline>
<codeline lineno="277"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>__m512<sp/>g<sp/>=<sp/>_mm512_loadu_ps(&amp;grad[i]);</highlight></codeline>
<codeline lineno="278"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>_mm512_storeu_ps(&amp;grad[i],<sp/>_mm512_mul_ps(g,<sp/>v_scale));</highlight></codeline>
<codeline lineno="279"><highlight class="normal"><sp/><sp/><sp/><sp/>}</highlight></codeline>
<codeline lineno="280"><highlight class="normal"><sp/><sp/><sp/><sp/></highlight><highlight class="keywordflow">for</highlight><highlight class="normal"><sp/>(;<sp/>i<sp/>&lt;<sp/>numel;<sp/>++i)<sp/>{</highlight></codeline>
<codeline lineno="281"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>grad[i]<sp/>*=<sp/>scale;</highlight></codeline>
<codeline lineno="282"><highlight class="normal"><sp/><sp/><sp/><sp/>}</highlight></codeline>
<codeline lineno="283"><highlight class="normal"></highlight><highlight class="preprocessor">#else</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="284"><highlight class="normal"><sp/><sp/><sp/><sp/></highlight><highlight class="keywordflow">for</highlight><highlight class="normal"><sp/>(</highlight><highlight class="keywordtype">size_t</highlight><highlight class="normal"><sp/>i<sp/>=<sp/>0;<sp/>i<sp/>&lt;<sp/>numel;<sp/>++i)<sp/>{</highlight></codeline>
<codeline lineno="285"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>grad[i]<sp/>*=<sp/>scale;</highlight></codeline>
<codeline lineno="286"><highlight class="normal"><sp/><sp/><sp/><sp/>}</highlight></codeline>
<codeline lineno="287"><highlight class="normal"></highlight><highlight class="preprocessor">#endif</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="288"><highlight class="normal">}</highlight></codeline>
<codeline lineno="289"><highlight class="normal"></highlight></codeline>
<codeline lineno="290"><highlight class="normal"></highlight><highlight class="comment"></highlight></codeline>
<codeline lineno="291"><highlight class="comment">/**</highlight></codeline>
<codeline lineno="292"><highlight class="comment"><sp/>*<sp/>@brief<sp/>Clip<sp/>gradient<sp/>norm<sp/>(fp32)</highlight></codeline>
<codeline lineno="293"><highlight class="comment"><sp/>*</highlight></codeline>
<codeline lineno="294"><highlight class="comment"><sp/>*<sp/>If<sp/>||grad||_2<sp/>&gt;<sp/>max_norm,<sp/>scale<sp/>grad<sp/>so<sp/>that<sp/>||grad||_2<sp/>=<sp/>max_norm</highlight></codeline>
<codeline lineno="295"><highlight class="comment"><sp/>*</highlight></codeline>
<codeline lineno="296"><highlight class="comment"><sp/>*<sp/>@param<sp/>grad<sp/><sp/><sp/><sp/><sp/>Gradient<sp/>tensor<sp/>to<sp/>clip<sp/>(in-place)<sp/>[numel]</highlight></codeline>
<codeline lineno="297"><highlight class="comment"><sp/>*<sp/>@param<sp/>numel<sp/><sp/><sp/><sp/>Number<sp/>of<sp/>elements</highlight></codeline>
<codeline lineno="298"><highlight class="comment"><sp/>*<sp/>@param<sp/>max_norm<sp/>Maximum<sp/>allowed<sp/>L2<sp/>norm</highlight></codeline>
<codeline lineno="299"><highlight class="comment"><sp/>*<sp/>@return<sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>The<sp/>original<sp/>L2<sp/>norm<sp/>before<sp/>clipping</highlight></codeline>
<codeline lineno="300"><highlight class="comment"><sp/>*/</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="301" refid="optimizer__kernels_8c_1a4e854ffc919637e40ea0d320ecb5ed0b" refkind="member"><highlight class="normal"></highlight><highlight class="keywordtype">float</highlight><highlight class="normal"><sp/><ref refid="optimizer__kernels_8c_1a4e854ffc919637e40ea0d320ecb5ed0b" kindref="member">gradient_clip_norm_f32</ref>(</highlight><highlight class="keywordtype">float</highlight><highlight class="normal"><sp/>*grad,<sp/></highlight><highlight class="keywordtype">size_t</highlight><highlight class="normal"><sp/>numel,<sp/></highlight><highlight class="keywordtype">float</highlight><highlight class="normal"><sp/>max_norm)</highlight></codeline>
<codeline lineno="302"><highlight class="normal">{</highlight></codeline>
<codeline lineno="303"><highlight class="normal"><sp/><sp/><sp/><sp/></highlight><highlight class="keywordflow">if</highlight><highlight class="normal"><sp/>(!grad<sp/>||<sp/>numel<sp/>==<sp/>0<sp/>||<sp/>max_norm<sp/>&lt;=<sp/>0.0f)<sp/>{</highlight></codeline>
<codeline lineno="304"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/></highlight><highlight class="keywordflow">return</highlight><highlight class="normal"><sp/>0.0f;</highlight></codeline>
<codeline lineno="305"><highlight class="normal"><sp/><sp/><sp/><sp/>}</highlight></codeline>
<codeline lineno="306"><highlight class="normal"></highlight></codeline>
<codeline lineno="307"><highlight class="normal"><sp/><sp/><sp/><sp/></highlight><highlight class="comment">//<sp/>Compute<sp/>L2<sp/>norm</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="308"><highlight class="normal"><sp/><sp/><sp/><sp/></highlight><highlight class="keywordtype">double</highlight><highlight class="normal"><sp/>sum_sq<sp/>=<sp/>0.0;</highlight></codeline>
<codeline lineno="309"><highlight class="normal"></highlight><highlight class="preprocessor">#if<sp/>defined(__AVX512F__)</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="310"><highlight class="normal"><sp/><sp/><sp/><sp/>__m512<sp/>acc<sp/>=<sp/>_mm512_setzero_ps();</highlight></codeline>
<codeline lineno="311"><highlight class="normal"><sp/><sp/><sp/><sp/></highlight><highlight class="keywordtype">size_t</highlight><highlight class="normal"><sp/>i<sp/>=<sp/>0;</highlight></codeline>
<codeline lineno="312"><highlight class="normal"><sp/><sp/><sp/><sp/></highlight><highlight class="keywordflow">for</highlight><highlight class="normal"><sp/>(;<sp/>i<sp/>+<sp/>16<sp/>&lt;=<sp/>numel;<sp/>i<sp/>+=<sp/>16)<sp/>{</highlight></codeline>
<codeline lineno="313"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>__m512<sp/>g<sp/>=<sp/>_mm512_loadu_ps(&amp;grad[i]);</highlight></codeline>
<codeline lineno="314"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>acc<sp/>=<sp/>_mm512_fmadd_ps(g,<sp/>g,<sp/>acc);</highlight></codeline>
<codeline lineno="315"><highlight class="normal"><sp/><sp/><sp/><sp/>}</highlight></codeline>
<codeline lineno="316"><highlight class="normal"><sp/><sp/><sp/><sp/>sum_sq<sp/>=<sp/>_mm512_reduce_add_ps(acc);</highlight></codeline>
<codeline lineno="317"><highlight class="normal"><sp/><sp/><sp/><sp/></highlight><highlight class="keywordflow">for</highlight><highlight class="normal"><sp/>(;<sp/>i<sp/>&lt;<sp/>numel;<sp/>++i)<sp/>{</highlight></codeline>
<codeline lineno="318"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>sum_sq<sp/>+=<sp/>(double)grad[i]<sp/>*<sp/>(</highlight><highlight class="keywordtype">double</highlight><highlight class="normal">)grad[i];</highlight></codeline>
<codeline lineno="319"><highlight class="normal"><sp/><sp/><sp/><sp/>}</highlight></codeline>
<codeline lineno="320"><highlight class="normal"></highlight><highlight class="preprocessor">#else</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="321"><highlight class="normal"><sp/><sp/><sp/><sp/></highlight><highlight class="keywordflow">for</highlight><highlight class="normal"><sp/>(</highlight><highlight class="keywordtype">size_t</highlight><highlight class="normal"><sp/>i<sp/>=<sp/>0;<sp/>i<sp/>&lt;<sp/>numel;<sp/>++i)<sp/>{</highlight></codeline>
<codeline lineno="322"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>sum_sq<sp/>+=<sp/>(double)grad[i]<sp/>*<sp/>(</highlight><highlight class="keywordtype">double</highlight><highlight class="normal">)grad[i];</highlight></codeline>
<codeline lineno="323"><highlight class="normal"><sp/><sp/><sp/><sp/>}</highlight></codeline>
<codeline lineno="324"><highlight class="normal"></highlight><highlight class="preprocessor">#endif</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="325"><highlight class="normal"></highlight></codeline>
<codeline lineno="326"><highlight class="normal"><sp/><sp/><sp/><sp/></highlight><highlight class="keywordtype">float</highlight><highlight class="normal"><sp/>norm<sp/>=<sp/>sqrtf((</highlight><highlight class="keywordtype">float</highlight><highlight class="normal">)sum_sq);</highlight></codeline>
<codeline lineno="327"><highlight class="normal"></highlight></codeline>
<codeline lineno="328"><highlight class="normal"><sp/><sp/><sp/><sp/></highlight><highlight class="comment">//<sp/>Clip<sp/>if<sp/>necessary</highlight><highlight class="normal"></highlight></codeline>
<codeline lineno="329"><highlight class="normal"><sp/><sp/><sp/><sp/></highlight><highlight class="keywordflow">if</highlight><highlight class="normal"><sp/>(norm<sp/>&gt;<sp/>max_norm)<sp/>{</highlight></codeline>
<codeline lineno="330"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/></highlight><highlight class="keywordtype">float</highlight><highlight class="normal"><sp/>scale<sp/>=<sp/>max_norm<sp/>/<sp/>norm;</highlight></codeline>
<codeline lineno="331"><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><ref refid="optimizer__kernels_8c_1ab615ca5560d68ed2f098960cd81acfc2" kindref="member">gradient_scale_f32</ref>(grad,<sp/>numel,<sp/>scale);</highlight></codeline>
<codeline lineno="332"><highlight class="normal"><sp/><sp/><sp/><sp/>}</highlight></codeline>
<codeline lineno="333"><highlight class="normal"></highlight></codeline>
<codeline lineno="334"><highlight class="normal"><sp/><sp/><sp/><sp/></highlight><highlight class="keywordflow">return</highlight><highlight class="normal"><sp/>norm;</highlight></codeline>
<codeline lineno="335"><highlight class="normal">}</highlight></codeline>
    </programlisting>
    <location file="/home/antshiv/Workspace/C-Kernel-Engine/src/kernels/optimizer_kernels.c"/>
  </compounddef>
</doxygen>
