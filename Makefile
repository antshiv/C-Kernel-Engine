# Default to gcc for portability.
# (icx-built binaries typically depend on the Intel runtime, e.g. `libimf.so`.)
ifeq ($(origin CC),default)
CC := gcc
endif
# Some environments export CC=cc; treat that like "unset".
ifeq ($(CC),cc)
CC := gcc
endif
# Opt-in to icx selection if desired:
#   make CK_USE_ICX=1
ifeq ($(CK_USE_ICX),1)
CC := $(if $(shell command -v icx 2>/dev/null),icx,$(CC))
endif
# OpenMP flag varies by compiler (icx/icc prefer -qopenmp; gcc/clang use -fopenmp).
OPENMP_FLAG ?= -fopenmp
ifneq (,$(findstring icc,$(CC)))
OPENMP_FLAG := -qopenmp
endif
ifneq (,$(findstring icx,$(CC)))
OPENMP_FLAG := -qopenmp
endif
# You can override AVX/arch flags from the environment if needed, e.g.:
#   make AVX_FLAGS="-mavx2"
#   make AVX_FLAGS=""            # scalar build
CPU_FLAGS := $(shell grep -m1 '^flags' /proc/cpuinfo 2>/dev/null)
# Detect FMA support
ifneq (,$(findstring fma,$(CPU_FLAGS)))
FMA_FLAGS := -mfma
else
FMA_FLAGS :=
endif
# Detect AVX level
ifneq (,$(findstring avx512f,$(CPU_FLAGS)))
AVX_FLAGS ?= -mavx512f $(FMA_FLAGS)
else ifneq (,$(findstring avx2,$(CPU_FLAGS)))
AVX_FLAGS ?= -mavx2 $(FMA_FLAGS)
else ifneq (,$(findstring avx,$(CPU_FLAGS)))
AVX_FLAGS ?= -mavx $(FMA_FLAGS)
else
AVX_FLAGS ?=
endif
INCLUDES := -Iinclude
CFLAGS  := -O3 -fPIC $(OPENMP_FLAG) -Wall $(AVX_FLAGS) $(INCLUDES)
CXX ?= g++
BENCH_CC ?= gcc
BENCH_CXX ?= $(CXX)

BUILD_DIR := build
BUILD_STAMP := $(BUILD_DIR)/.ck_build_flags

# =============================================================================
# Intel oneAPI Integration (MKL / oneDNN)
# =============================================================================
# Auto-detection: MKL is used automatically if found
# Disable with: make USE_NATIVE=1
# Force MKL:    make USE_MKL=1
# Force oneDNN: make USE_ONEDNN=1

ONEAPI_ROOT ?= /opt/intel/oneapi

# MKL paths
MKL_ROOT ?= $(ONEAPI_ROOT)/mkl/latest
MKL_INC := $(MKL_ROOT)/include
MKL_LIB := $(MKL_ROOT)/lib/intel64

# Auto-detect MKL availability
MKL_AVAILABLE := $(wildcard $(MKL_INC)/mkl.h)

# Auto-enable MKL if available and not explicitly using native/oneDNN
ifndef USE_NATIVE
ifndef USE_ONEDNN
ifndef USE_MKL
ifneq ($(MKL_AVAILABLE),)
USE_MKL := 1
endif
endif
endif
endif

# oneDNN paths
# Prefer oneAPI if installed; otherwise default to /usr/local (typical from-source install).
DNNL_ROOT ?= $(if $(wildcard $(ONEAPI_ROOT)/dnnl/latest/include/dnnl.h),$(ONEAPI_ROOT)/dnnl/latest,/usr/local)
DNNL_INC := $(DNNL_ROOT)/include
DNNL_LIB := $(DNNL_ROOT)/lib

DNNL_HPP := $(wildcard $(DNNL_INC)/dnnl.hpp)

# Add MKL support
ifdef USE_MKL
    # Intel compiler runtime library (libimf.so, etc.) needed by MKL
    INTEL_COMPILER_LIB := $(ONEAPI_ROOT)/compiler/latest/lib
    CFLAGS += -DUSE_MKL -I$(MKL_INC)
    LDFLAGS += -L$(MKL_LIB) -lmkl_rt -Wl,-rpath,$(MKL_LIB)
    LDFLAGS += -L$(INTEL_COMPILER_LIB) -Wl,-rpath,$(INTEL_COMPILER_LIB)
    $(info Building with Intel MKL backend for GEMM)
endif

# Add oneDNN support
ifdef USE_ONEDNN
    # Prefer /usr/local OpenMP-based oneDNN over Intel oneAPI SYCL version
    DNNL_LOCAL := $(wildcard /usr/local/lib/libdnnl.so)
    ifdef DNNL_LOCAL
        DNNL_INC := /usr/local/include
        DNNL_LIB := /usr/local/lib
    else
        # Intel compiler runtime library (libimf.so, etc.) needed by oneAPI oneDNN
        INTEL_COMPILER_LIB := $(ONEAPI_ROOT)/compiler/latest/lib
        LDFLAGS += -L$(INTEL_COMPILER_LIB) -Wl,-rpath,$(INTEL_COMPILER_LIB)
    endif
    CFLAGS += -DUSE_ONEDNN -I$(DNNL_INC)
    LDFLAGS += -L$(DNNL_LIB) -ldnnl -Wl,-rpath,$(DNNL_LIB)
    $(info Building with Intel oneDNN backend for GEMM)
endif

# Default message
ifndef USE_MKL
ifndef USE_ONEDNN
    $(info Building with native AVX kernels for GEMM)
endif
endif

SRCS    := src/backend_native.c \
           src/ckernel_ir.c \
           src/ckernel_codegen.c \
           src/ckernel_kernel_specs.c \
           src/ckernel_alloc.c \
           src/ckernel_strict.c \
           src/ckernel_registry.c \
           src/ckernel_orchestration.c \
           src/ckernel_model_layout.c \
           src/ckernel_model_load.c \
           src/cpu_features.c \
            src/kernels/gemm_kernels.c \
            src/kernels/gemm_fused_kernels.c \
            src/kernels/mlp_fused_decode.c \
            src/kernels/gemm_microkernel.c \
	           src/kernels/layernorm_kernels.c \
	           src/kernels/layernorm_kernels_bf16.c \
	           src/kernels/gelu_kernels.c \
	           src/kernels/gelu_kernels_bf16.c \
	           src/kernels/softmax_kernels.c \
	           src/kernels/softmax_kernels_bf16.c \
	           src/kernels/attention_kernels.c \
	           src/kernels/attention_decode_fused.c \
	           src/kernels/embedding_kernels.c \
	           src/kernels/embedding_kernels_bf16.c \
	           src/kernels/loss_kernels.c \
	           src/kernels/loss_kernels_bf16.c \
	           src/kernels/mlp_kernels.c \
	           src/kernels/mlp_kernels_bf16.c \
	           src/kernels/rmsnorm_kernels.c \
	            src/kernels/rmsnorm_kernels_bf16.c \
	            src/kernels/rmsnorm_kernels_int8.c \
	            src/kernels/rmsnorm_kernels_int4.c \
	            src/kernels/swiglu_kernels.c \
	           src/kernels/swiglu_kernels_bf16.c \
	           src/kernels/sigmoid_kernels.c \
	           src/kernels/sigmoid_kernels_bf16.c \
	           src/kernels/relu_kernels.c \
	           src/kernels/relu_kernels_bf16.c \
	           src/kernels/vision_kernels.c \
	           src/kernels/vision_kernels_bf16.c \
	           src/kernels/rope_kernels.c \
	           src/kernels/rope_kernels_bf16.c \
	           src/kernels/kv_cache_kernels.c \
	           src/kernels/dequant_kernels.c \
	           src/kernels/gemm_kernels_bf16.c \
	           src/kernels/gemm_kernels_q4_0.c \
	           src/kernels/gemm_kernels_q4k.c \
	           src/kernels/gemm_kernels_q8_0.c \
	           src/kernels/gemm_kernels_f16.c
LIB          := $(BUILD_DIR)/libckernel_engine.so
LIB_QUANT    := $(BUILD_DIR)/libckernel_quant.so
LIB_GELU     := $(BUILD_DIR)/libckernel_gelu.so
LIB_RMSNORM  := $(BUILD_DIR)/libckernel_rmsnorm.so
LIB_LN       := $(BUILD_DIR)/libckernel_layernorm.so
LIB_SOFT     := $(BUILD_DIR)/libckernel_softmax.so
LIB_SWIGLU   := $(BUILD_DIR)/libckernel_swiglu.so
LIB_SIGMOID  := $(BUILD_DIR)/libckernel_sigmoid.so
LIB_RELU     := $(BUILD_DIR)/libckernel_relu.so
LIB_VISION   := $(BUILD_DIR)/libckernel_vision.so
LIB_ATTENTION := $(BUILD_DIR)/libckernel_attention.so
LIB_ROPE     := $(BUILD_DIR)/libckernel_rope.so
BENCH_GEMM_ONEDNN := $(BUILD_DIR)/bench_gemm_onednn

IR_DEMO := $(BUILD_DIR)/ck_ir_demo
DEFAULT_CONFIG := default.config.json
CONFIG ?= $(DEFAULT_CONFIG)
OUT ?= $(BUILD_DIR)/generated_model.c
GGUF ?=
GGUF_OUT ?= $(BUILD_DIR)/gguf_weights.bump
GGUF_CONFIG_OUT ?= $(BUILD_DIR)/gguf_config.json
GGUF_CONTEXT ?=
TINY_CONFIG ?= tiny.config.json
SMALL_CONFIG ?= small10mb.config.json
TINY_TRAIN_LR ?= 1e-3
TINY_TRAIN_ARGS ?= --dump
TINY_PARITY_ARGS ?=
ALL_TEST_LAYER_ARGS ?= --tokens 256 --embed 64 --heads 4 --kv-heads 2 --intermediate 128 --rope --strict-ref
ALL_TEST_LAYER_TOL ?= 2e-3
SMOLLM_CONFIG ?= smolLM-135.json
SMOLLM_MODEL_DIR ?= $(HOME)/.cache/huggingface/hub/SmolLM-135M
SMOLLM_REPO ?= HuggingFaceTB/SmolLM-135M
SMOLLM_DOWNLOAD ?=
SMOLLM_CONTEXT ?= 2
SMOLLM_DATASET ?= roneneldan/TinyStories
SMOLLM_DATASET_CONFIG ?=
SMOLLM_SPLIT ?= train
SMOLLM_MAX_SAMPLES ?= 4
SMOLLM_TEXT ?= Once upon a time
SMOLLM_TOPK ?= 5
SMOLLM_LAYER ?= 0
SMOLLM_STAGE_TOL ?= 1e-3
SMOLLM_STAGE_DUMP ?=
SMOLLM_BUMP ?= $(BUILD_DIR)/smollm_weights.bin
SMOLLM_OUT_WEIGHTS ?= $(BUILD_DIR)/smollm_weights_after.bin
SMOLLM_MAX_LAYERS ?=

PYTHON  ?= python3
PYTHONFLAGS ?= -B
PY_TESTS := unittest/test_layernorm.py \
            unittest/test_gelu.py \
            unittest/test_softmax.py \
            unittest/test_softmax_backward.py \
            unittest/test_gemm.py \
            unittest/test_gemm_fused.py \
            unittest/test_gemm_microkernel.py \
            unittest/test_mlp.py \
            unittest/test_rmsnorm.py \
            unittest/test_swiglu.py \
            unittest/test_fused_swiglu_decode.py \
            unittest/test_fused_attention_decode.py \
            unittest/test_sigmoid.py \
            unittest/test_relu.py \
            unittest/test_attention.py \
            unittest/test_attention_backward.py \
            unittest/test_kv_cache_attention.py \
            unittest/test_kv_cache_layer_decode.py \
            unittest/test_rope.py \
            unittest/test_embedding.py \
            unittest/test_cross_entropy.py \
            unittest/test_orchestration_layer.py \
            unittest/test_lm_head_litmus.py

PY_TESTS_BF16 := unittest/bf16/test_sigmoid_bf16.py \
                unittest/bf16/test_rmsnorm_bf16.py \
                unittest/bf16/test_mlp_bf16.py \
                unittest/bf16/test_attention_bf16.py \
                unittest/bf16/test_gelu_bf16.py \
                unittest/bf16/test_layernorm_bf16.py \
                unittest/bf16/test_rope_bf16.py \
                unittest/bf16/test_relu_bf16.py \
                unittest/bf16/test_swiglu_bf16.py \
                unittest/bf16/test_embedding_bf16.py \
                unittest/bf16/test_cross_entropy_bf16.py

LITMUS_DEMO_ARGS ?= --vocab 100 --ctx 100 --embed 64 --intermediate 128 --heads 4 --kv-heads 2
LITMUS_DEMO_SVG ?= $(BUILD_DIR)/litmus_report.svg
LITMUS_DEMO_LOG ?= $(BUILD_DIR)/litmus_demo.log
CK_GELU_TOL ?= 1e-7
TEST_ENV :=
ifneq (,$(findstring icx,$(CC)))
CK_GELU_TOL := 1e-6
TEST_ENV += CK_GELU_TOL=$(CK_GELU_TOL)
endif
ifneq (,$(findstring icc,$(CC)))
CK_GELU_TOL := 1e-6
TEST_ENV += CK_GELU_TOL=$(CK_GELU_TOL)
endif
export CK_GELU_TOL

all: $(BUILD_DIR) $(LIB)

$(BUILD_DIR):
	mkdir -p $(BUILD_DIR)

$(BUILD_STAMP): | $(BUILD_DIR)
	@printf 'CC=%s\nCFLAGS=%s\n' "$(CC)" "$(CFLAGS)" > $@.tmp
	@if [ ! -f $@ ] || ! cmp -s $@.tmp $@; then mv $@.tmp $@; else rm $@.tmp; fi

$(LIB): $(BUILD_STAMP) $(SRCS)
	$(CC) $(CFLAGS) -shared -o $@ $(SRCS) $(LDFLAGS) -lm

$(IR_DEMO): $(BUILD_DIR) src/ckernel_ir.c src/ckernel_ir_demo.c src/ckernel_codegen.c src/ckernel_kernel_specs.c src/ckernel_registry.c include/ckernel_ir.h include/ckernel_codegen.h include/ckernel_registry.h include/ckernel_kernel_specs.h
	$(CC) -O2 -Wall -Iinclude -o $@ src/ckernel_ir.c src/ckernel_codegen.c src/ckernel_kernel_specs.c src/ckernel_registry.c src/ckernel_ir_demo.c

ck: $(IR_DEMO)
	@echo "Running $(IR_DEMO) with $(DEFAULT_CONFIG)..."
	./$(IR_DEMO) $(DEFAULT_CONFIG)

emit: $(IR_DEMO)
	@echo "Generating runtime from $(CONFIG) -> $(OUT)..."
	./$(IR_DEMO) $(CONFIG) --emit $(OUT)

ck-emit: emit
	@true

$(LIB_GELU): $(BUILD_STAMP) src/kernels/gelu_kernels.c src/kernels/gelu_kernels_bf16.c include/ckernel_engine.h
	$(CC) $(CFLAGS) -shared -o $@ src/kernels/gelu_kernels.c src/kernels/gelu_kernels_bf16.c -lm

$(LIB_RMSNORM): $(BUILD_STAMP) src/kernels/rmsnorm_kernels.c src/kernels/rmsnorm_kernels_bf16.c src/kernels/rmsnorm_kernels_int8.c src/kernels/rmsnorm_kernels_int4.c include/ckernel_engine.h
	$(CC) $(CFLAGS) -shared -o $@ src/kernels/rmsnorm_kernels.c src/kernels/rmsnorm_kernels_bf16.c src/kernels/rmsnorm_kernels_int8.c src/kernels/rmsnorm_kernels_int4.c -lm

$(LIB_LN): $(BUILD_STAMP) src/kernels/layernorm_kernels.c src/kernels/layernorm_kernels_bf16.c include/ckernel_engine.h
	$(CC) $(CFLAGS) -shared -o $@ src/kernels/layernorm_kernels.c src/kernels/layernorm_kernels_bf16.c -lm

$(LIB_SOFT): $(BUILD_STAMP) src/kernels/softmax_kernels.c src/kernels/softmax_kernels_bf16.c include/ckernel_engine.h
	$(CC) $(CFLAGS) -shared -o $@ src/kernels/softmax_kernels.c src/kernels/softmax_kernels_bf16.c -lm

$(LIB_SWIGLU): $(BUILD_STAMP) src/kernels/swiglu_kernels.c src/kernels/swiglu_kernels_bf16.c src/kernels/sigmoid_kernels.c include/ckernel_engine.h
	$(CC) $(CFLAGS) -shared -o $@ src/kernels/swiglu_kernels.c src/kernels/swiglu_kernels_bf16.c src/kernels/sigmoid_kernels.c -lm

$(LIB_SIGMOID): $(BUILD_STAMP) src/kernels/sigmoid_kernels.c src/kernels/sigmoid_kernels_bf16.c include/ckernel_engine.h
	$(CC) $(CFLAGS) -shared -o $@ src/kernels/sigmoid_kernels.c src/kernels/sigmoid_kernels_bf16.c -lm

$(LIB_RELU): $(BUILD_STAMP) src/kernels/relu_kernels.c src/kernels/relu_kernels_bf16.c include/ckernel_engine.h
	$(CC) $(CFLAGS) -shared -o $@ src/kernels/relu_kernels.c src/kernels/relu_kernels_bf16.c -lm

$(LIB_VISION): $(BUILD_STAMP) src/kernels/vision_kernels.c src/kernels/vision_kernels_bf16.c include/ckernel_engine.h
	$(CC) $(CFLAGS) -shared -o $@ src/kernels/vision_kernels.c src/kernels/vision_kernels_bf16.c -lm

$(LIB_ATTENTION): $(BUILD_STAMP) src/kernels/attention_kernels.c src/kernels/softmax_kernels.c include/ckernel_engine.h
	$(CC) $(CFLAGS) -shared -o $@ src/kernels/attention_kernels.c src/kernels/softmax_kernels.c -lm

$(LIB_ROPE): $(BUILD_STAMP) src/kernels/rope_kernels.c src/kernels/rope_kernels_bf16.c include/ckernel_engine.h
	$(CC) $(CFLAGS) -shared -o $@ src/kernels/rope_kernels.c src/kernels/rope_kernels_bf16.c -lm

$(LIB_QUANT): $(BUILD_STAMP) src/kernels/dequant_kernels.c src/kernels/gemm_kernels_q4_0.c src/kernels/gemm_kernels_q4k.c src/kernels/gemm_kernels_q8_0.c src/kernels/gemm_kernels_f16.c include/ckernel_quant.h include/ckernel_dtype.h
	$(CC) $(CFLAGS) -shared -o $@ src/kernels/dequant_kernels.c src/kernels/gemm_kernels_q4_0.c src/kernels/gemm_kernels_q4k.c src/kernels/gemm_kernels_q8_0.c src/kernels/gemm_kernels_f16.c -lm

# Convenience alias targets so existing commands still work.
libckernel_gelu.so: $(LIB_GELU)
	@true

libckernel_rmsnorm.so: $(LIB_RMSNORM)
	@true

libckernel_layernorm.so: $(LIB_LN)
	@true

libckernel_softmax.so: $(LIB_SOFT)
	@true

libckernel_swiglu.so: $(LIB_SWIGLU)
	@true

libckernel_sigmoid.so: $(LIB_SIGMOID)
	@true

libckernel_attention.so: $(LIB_ATTENTION)
	@true

libckernel_relu.so: $(LIB_RELU)
	@true

libckernel_vision.so: $(LIB_VISION)
	@true

test-relu: $(LIB_RELU)
	LD_LIBRARY_PATH=$(BUILD_DIR):$$LD_LIBRARY_PATH $(PYTHON) $(PYTHONFLAGS) unittest/test_relu.py

test-vision: $(LIB_VISION)
	LD_LIBRARY_PATH=$(BUILD_DIR):$$LD_LIBRARY_PATH $(PYTHON) $(PYTHONFLAGS) unittest/test_vision.py

libckernel_rope.so: $(LIB_ROPE)
	@true

libckernel_quant.so: $(LIB_QUANT)
	@true

test-libs: $(LIB_GELU) $(LIB_RMSNORM) $(LIB_LN) $(LIB_SOFT) $(LIB_SWIGLU) $(LIB_SIGMOID) $(LIB_ATTENTION) $(LIB_ROPE) $(LIB_RELU) $(LIB_VISION) $(LIB_QUANT)

test-quant: $(LIB_QUANT)
	@set -e; \
	LD_LIBRARY_PATH=$(BUILD_DIR):$$LD_LIBRARY_PATH $(PYTHON) $(PYTHONFLAGS) unittest/test_quant_kernels.py; \
	LD_LIBRARY_PATH=$(BUILD_DIR):$$LD_LIBRARY_PATH $(PYTHON) $(PYTHONFLAGS) unittest/test_q4_k_quantize.py

gguf-inspect:
	@if [ -z "$(GGUF)" ]; then \
	  echo "Usage: make gguf-inspect GGUF=/path/to/model.gguf"; \
	  exit 2; \
	fi
	@$(PYTHON) $(PYTHONFLAGS) scripts/convert_gguf_to_bump.py --gguf "$(GGUF)" --inspect

gguf-list:
	@if [ -z "$(GGUF)" ]; then \
	  echo "Usage: make gguf-list GGUF=/path/to/model.gguf"; \
	  exit 2; \
	fi
	@$(PYTHON) $(PYTHONFLAGS) scripts/convert_gguf_to_bump.py --gguf "$(GGUF)" --list

gguf-to-bump:
	@if [ -z "$(GGUF)" ]; then \
	  echo "Usage: make gguf-to-bump GGUF=/path/to/model.gguf [GGUF_OUT=$(GGUF_OUT)] [GGUF_CONFIG_OUT=$(GGUF_CONFIG_OUT)] [GGUF_CONTEXT=<n>]"; \
	  exit 2; \
	fi
	@$(PYTHON) $(PYTHONFLAGS) scripts/convert_gguf_to_bump.py \
	  --gguf "$(GGUF)" \
	  --output "$(GGUF_OUT)" \
	  $(if $(GGUF_CONFIG_OUT),--config-out "$(GGUF_CONFIG_OUT)") \
	  $(if $(GGUF_CONTEXT),--context "$(GGUF_CONTEXT)")

test: $(LIB) test-libs
	@set -e; \
	for t in $(PY_TESTS); do \
	  echo "Running $$t"; \
	  LD_LIBRARY_PATH=$(BUILD_DIR):$$LD_LIBRARY_PATH $(TEST_ENV) $(PYTHON) $(PYTHONFLAGS) $$t; \
	done; \
	echo "All Python kernel tests completed."

test-bf16: $(LIB) test-libs
	@failed=0; \
	for t in $(PY_TESTS_BF16); do \
	  echo "Running $$t"; \
	  if ! LD_LIBRARY_PATH=$(BUILD_DIR):$$LD_LIBRARY_PATH $(TEST_ENV) $(PYTHON) $(PYTHONFLAGS) $$t; then \
	    failed=1; \
	  fi; \
	done; \
	if [ $$failed -ne 0 ]; then \
	  echo "BF16 Python kernel tests failed."; \
	  exit 1; \
	fi; \
	echo "BF16 Python kernel tests completed."

# GEMM benchmark comparing CKernel (Native + MKL if available) vs PyTorch
bench_gemm:
	@echo "Building native kernels..."
	@rm -f $(BUILD_DIR)/libckernel_engine.so $(BUILD_DIR)/libckernel_native.so $(BUILD_DIR)/libckernel_mkl.so
	@# Force true native build even if MKL is auto-detected (or exported via env).
	@$(MAKE) --no-print-directory USE_NATIVE=1 USE_MKL= USE_ONEDNN=
	@cp $(BUILD_DIR)/libckernel_engine.so $(BUILD_DIR)/libckernel_native.so
ifneq ($(MKL_AVAILABLE),)
	@echo "Building MKL kernels..."
	@rm -f $(BUILD_DIR)/libckernel_engine.so
	@$(MAKE) --no-print-directory USE_MKL=1 USE_NATIVE= USE_ONEDNN=
	@cp $(BUILD_DIR)/libckernel_engine.so $(BUILD_DIR)/libckernel_mkl.so
	@LD_LIBRARY_PATH=$(BUILD_DIR):$$LD_LIBRARY_PATH \
		CK_NATIVE_LIB=$(BUILD_DIR)/libckernel_native.so \
		CK_MKL_LIB=$(BUILD_DIR)/libckernel_mkl.so \
		$(PYTHON) $(PYTHONFLAGS) benchmarks/bench_gemm_vs_pytorch.py
else
	@LD_LIBRARY_PATH=$(BUILD_DIR):$$LD_LIBRARY_PATH \
		CK_NATIVE_LIB=$(BUILD_DIR)/libckernel_native.so \
		CK_MKL_MISSING=1 \
		$(PYTHON) $(PYTHONFLAGS) benchmarks/bench_gemm_vs_pytorch.py
endif

# Benchmark with MKL only
bench_gemm_mkl:
ifneq ($(MKL_AVAILABLE),)
	@$(MAKE) --no-print-directory clean
	@$(MAKE) --no-print-directory USE_MKL=1 USE_NATIVE= USE_ONEDNN=
	@LD_LIBRARY_PATH=$(BUILD_DIR):$$LD_LIBRARY_PATH CK_LIB_PATH=$(BUILD_DIR)/libckernel_engine.so \
		$(PYTHON) $(PYTHONFLAGS) benchmarks/bench_gemm_vs_pytorch.py
else
	@echo "Error: MKL not found at $(MKL_INC)"
	@echo "Install Intel oneAPI: https://www.intel.com/content/www/us/en/developer/tools/oneapi/base-toolkit.html"
	@exit 1
endif

# Benchmark with native kernels only
bench_gemm_native:
	@$(MAKE) --no-print-directory clean
	@$(MAKE) --no-print-directory USE_NATIVE=1 USE_MKL= USE_ONEDNN=
	@LD_LIBRARY_PATH=$(BUILD_DIR):$$LD_LIBRARY_PATH CK_LIB_PATH=$(BUILD_DIR)/libckernel_engine.so \
		$(PYTHON) $(PYTHONFLAGS) benchmarks/bench_gemm_vs_pytorch.py

tests-list:
	@echo ""
	@echo "╔══════════════════════════════════════════════════════════════════════════════╗"
	@echo "║                      C-KERNEL-ENGINE UNIT TESTS                              ║"
	@echo "╚══════════════════════════════════════════════════════════════════════════════╝"
	@echo ""
	@echo "Run all tests:     make test"
	@echo "Run BF16 tests:    make test-bf16"
	@echo "Run single test:   python3 <script>"
	@echo ""
	@echo "┌──────────────────────────────────────────────────────────────────────────────┐"
	@echo "│  FP32 Unit Tests                                                             │"
	@echo "└──────────────────────────────────────────────────────────────────────────────┘"
	@echo "  unittest/test_gelu.py              - GELU forward/backward vs PyTorch"
	@echo "  unittest/test_rmsnorm.py           - RMSNorm forward/backward vs PyTorch"
	@echo "  unittest/test_sigmoid.py           - Sigmoid forward/backward vs PyTorch"
	@echo "  unittest/test_relu.py              - ReLU forward/backward vs PyTorch"
	@echo "  unittest/test_layernorm.py         - LayerNorm forward/backward vs PyTorch"
	@echo "  unittest/test_softmax.py           - Causal softmax forward vs PyTorch"
	@echo "  unittest/test_softmax_backward.py  - Causal softmax backward vs PyTorch"
	@echo "  unittest/test_gemm.py              - GEMM variants vs PyTorch matmul"
	@echo "  unittest/test_gemm_fused.py        - Fused GEMM+activation (ReLU/GELU/SiLU/SwiGLU)"
	@echo "  unittest/test_gemm_microkernel.py  - GEMM 8x8 microkernel with register blocking"
	@echo "  unittest/test_mlp.py               - MLP block forward/backward vs PyTorch"
	@echo "  unittest/test_swiglu.py            - SwiGLU activation forward/backward"
	@echo "  unittest/test_attention.py         - Multi-head attention forward vs PyTorch"
	@echo "  unittest/test_attention_backward.py - Attention backward (MHA/GQA)"
	@echo "  unittest/test_kv_cache_attention.py - Flash prefill + KV-cache decode attention"
	@echo "  unittest/test_kv_cache_layer_decode.py - Layer prefill+decode parity (KV cache)"
	@echo "  unittest/test_rope.py              - RoPE forward/backward vs PyTorch"
	@echo "  unittest/test_embedding.py         - Embedding forward/backward vs PyTorch"
	@echo "  unittest/test_cross_entropy.py     - Cross-entropy loss vs PyTorch"
	@echo "  unittest/test_orchestration_layer.py - Full layer stitch (GQA/MHA)"
	@echo "  unittest/test_lm_head_litmus.py    - LM head + CE end-to-end test"
	@echo "  unittest/test_fused_swiglu_decode.py - Fused SwiGLU decode MLP parity"
	@echo "  unittest/test_fused_attention_decode.py - Fused attention decode parity"
	@echo ""
	@echo "┌──────────────────────────────────────────────────────────────────────────────┐"
	@echo "│  BF16 Unit Tests                                                             │"
	@echo "└──────────────────────────────────────────────────────────────────────────────┘"
	@echo "  unittest/bf16/test_sigmoid_bf16.py   - BF16 sigmoid forward/backward"
	@echo "  unittest/bf16/test_rmsnorm_bf16.py   - BF16 RMSNorm forward/backward"
	@echo "  unittest/bf16/test_gelu_bf16.py      - BF16 GELU forward/backward"
	@echo "  unittest/bf16/test_relu_bf16.py      - BF16 ReLU forward/backward"
	@echo "  unittest/bf16/test_layernorm_bf16.py - BF16 LayerNorm forward/backward"
	@echo "  unittest/bf16/test_mlp_bf16.py       - BF16 MLP forward"
	@echo "  unittest/bf16/test_attention_bf16.py - BF16 attention forward/backward"
	@echo "  unittest/bf16/test_rope_bf16.py      - BF16 RoPE forward/backward"
	@echo "  unittest/bf16/test_swiglu_bf16.py    - BF16 SwiGLU forward/backward"
	@echo "  unittest/bf16/test_embedding_bf16.py - BF16 embedding forward/backward"
	@echo "  unittest/bf16/test_cross_entropy_bf16.py - BF16 cross-entropy loss"
	@echo ""

$(BUILD_DIR)/bench_gemm_onednn.o: tools/bench_gemm_onednn.cpp include/ckernel_engine.h
	$(BENCH_CXX) -O3 -std=c++17 -Iinclude -I$(DNNL_INC) -c -o $@ tools/bench_gemm_onednn.cpp

$(BUILD_DIR)/bench_gemm_gemm_kernels.o: src/kernels/gemm_kernels.c include/ckernel_engine.h
	$(BENCH_CC) -O3 -Wall $(AVX_FLAGS) $(OPENMP_FLAG) -Iinclude -c -o $@ src/kernels/gemm_kernels.c

$(BUILD_DIR)/bench_gemm_strict.o: src/ckernel_strict.c include/ckernel_engine.h
	$(BENCH_CC) -O3 -Wall -Iinclude -c -o $@ src/ckernel_strict.c

$(BENCH_GEMM_ONEDNN): $(BUILD_DIR) $(BUILD_DIR)/bench_gemm_onednn.o $(BUILD_DIR)/bench_gemm_gemm_kernels.o $(BUILD_DIR)/bench_gemm_strict.o
	$(BENCH_CXX) -O3 -o $@ $(BUILD_DIR)/bench_gemm_onednn.o $(BUILD_DIR)/bench_gemm_gemm_kernels.o $(BUILD_DIR)/bench_gemm_strict.o \
	    -L$(DNNL_LIB) -ldnnl -lm $(OPENMP_FLAG) -Wl,-rpath,$(DNNL_LIB)

bench-gemm-onednn:
ifeq ($(DNNL_HPP),)
	@echo "oneDNN headers not found at $(DNNL_INC) (set DNNL_ROOT=/path/to/install)"; \
	exit 1
else
	$(MAKE) $(BENCH_GEMM_ONEDNN)
	LD_LIBRARY_PATH=$(DNNL_LIB):$$LD_LIBRARY_PATH ./$(BENCH_GEMM_ONEDNN) $(ARGS)
endif

rope-test: $(LIB) test-libs
	$(PYTHON) $(PYTHONFLAGS) unittest/test_rope.py

litmus:
	$(PYTHON) $(PYTHONFLAGS) unittest/test_lm_head_litmus.py $(ARGS)

litmus-demo: $(BUILD_DIR)
	@echo "Running litmus demo: $(LITMUS_DEMO_ARGS)"
	@$(PYTHON) $(PYTHONFLAGS) unittest/test_lm_head_litmus.py $(LITMUS_DEMO_ARGS) --svg $(LITMUS_DEMO_SVG) | tee $(LITMUS_DEMO_LOG)

layer-parity: $(LIB)
	$(PYTHON) $(PYTHONFLAGS) unittest/test_orchestration_layer.py $(ARGS) $(if $(TOL),--tol $(TOL),)

layer-parity-scalar:
	$(MAKE) -B $(LIB) AVX_FLAGS=
	$(PYTHON) $(PYTHONFLAGS) unittest/test_orchestration_layer.py $(ARGS) $(if $(TOL),--tol $(TOL),)

gen-specs:
	$(PYTHON) $(PYTHONFLAGS) scripts/gen_kernel_specs.py

tiny-e2e: $(IR_DEMO) $(LIB)
	$(PYTHON) $(PYTHONFLAGS) scripts/gen_random_bump.py --config $(TINY_CONFIG) --output $(BUILD_DIR)/tiny_weights.bin
	$(PYTHON) $(PYTHONFLAGS) scripts/gen_random_tokens.py --config $(TINY_CONFIG) --output $(BUILD_DIR)/tiny_tokens.bin
	./$(IR_DEMO) $(TINY_CONFIG) --emit $(BUILD_DIR)/tiny_generated.c
	$(CC) $(CFLAGS) -Iinclude $(BUILD_DIR)/tiny_generated.c $$(cat $(BUILD_DIR)/tiny_generated.c.kernels) -o $(BUILD_DIR)/tiny_model -lm
	LD_LIBRARY_PATH=$(BUILD_DIR):$$LD_LIBRARY_PATH $(BUILD_DIR)/tiny_model \
	  --model-weights $(BUILD_DIR)/tiny_weights.bin \
	  --tokens $(BUILD_DIR)/tiny_tokens.bin \
	  --out-logits $(BUILD_DIR)/tiny_logits.bin

tiny-train: $(IR_DEMO) $(LIB)
	$(PYTHON) $(PYTHONFLAGS) scripts/gen_random_bump.py --config $(TINY_CONFIG) --output $(BUILD_DIR)/tiny_weights.bin
	$(PYTHON) $(PYTHONFLAGS) scripts/gen_random_tokens.py --config $(TINY_CONFIG) --output $(BUILD_DIR)/tiny_tokens.bin --targets $(BUILD_DIR)/tiny_targets.bin
	./$(IR_DEMO) $(TINY_CONFIG) --emit $(BUILD_DIR)/tiny_generated.c
	$(CC) $(CFLAGS) -Iinclude $(BUILD_DIR)/tiny_generated.c $$(cat $(BUILD_DIR)/tiny_generated.c.kernels) -o $(BUILD_DIR)/tiny_model -lm
	LD_LIBRARY_PATH=$(BUILD_DIR):$$LD_LIBRARY_PATH $(BUILD_DIR)/tiny_model \
	  --model-weights $(BUILD_DIR)/tiny_weights.bin \
	  --tokens $(BUILD_DIR)/tiny_tokens.bin \
	  --targets $(BUILD_DIR)/tiny_targets.bin \
	  --backward --lr $(TINY_TRAIN_LR) $(TINY_TRAIN_ARGS)

tiny-parity: $(IR_DEMO)
	$(PYTHON) $(PYTHONFLAGS) scripts/tiny_train_parity.py --config $(TINY_CONFIG) $(TINY_PARITY_ARGS)

smollm-demo:
	$(PYTHON) $(PYTHONFLAGS) scripts/smollm_train_demo.py \
	  --model-dir $(SMOLLM_MODEL_DIR) \
	  $(if $(SMOLLM_DOWNLOAD),--download-model --repo $(SMOLLM_REPO),) \
	  --context $(SMOLLM_CONTEXT) \
	  --dataset $(SMOLLM_DATASET) \
	  $(if $(SMOLLM_DATASET_CONFIG),--dataset-config $(SMOLLM_DATASET_CONFIG),) \
	  --split $(SMOLLM_SPLIT) \
	  --max-samples $(SMOLLM_MAX_SAMPLES)

smollm-forward:
	$(PYTHON) $(PYTHONFLAGS) scripts/smollm_forward_parity.py \
	  --model-dir $(SMOLLM_MODEL_DIR) \
	  $(if $(SMOLLM_DOWNLOAD),--download-model --repo $(SMOLLM_REPO),) \
	  --context $(SMOLLM_CONTEXT) \
	  --text "$(SMOLLM_TEXT)" \
	  --topk $(SMOLLM_TOPK)

smollm-layer-diff: $(LIB)
	$(PYTHON) $(PYTHONFLAGS) scripts/smollm_layer_stage_diff.py \
	  --model-dir $(SMOLLM_MODEL_DIR) \
	  $(if $(SMOLLM_DOWNLOAD),--download-model --repo $(SMOLLM_REPO),) \
	  --context $(SMOLLM_CONTEXT) \
	  --text "$(SMOLLM_TEXT)" \
	  --layer $(SMOLLM_LAYER) \
	  --tol $(SMOLLM_STAGE_TOL) \
	  $(if $(SMOLLM_STAGE_DUMP),--dump-stages,)

smollm-bump-compare:
	$(PYTHON) $(PYTHONFLAGS) scripts/compare_bump_to_hf.py \
	  --checkpoint $(SMOLLM_MODEL_DIR) \
	  --bump $(SMOLLM_BUMP) \
	  --context $(SMOLLM_CONTEXT) \
	  --layer $(SMOLLM_LAYER)

smollm-weight-check:
	$(PYTHON) $(PYTHONFLAGS) scripts/compare_bump_payload.py \
	  --bump $(SMOLLM_BUMP) \
	  --raw $(SMOLLM_OUT_WEIGHTS)

smollm-layer-stack:
	$(PYTHON) $(PYTHONFLAGS) scripts/smollm_layer_stack_diff.py \
	  --model-dir $(SMOLLM_MODEL_DIR) \
	  --context $(SMOLLM_CONTEXT) \
	  --text "$(SMOLLM_TEXT)" \
	  $(if $(SMOLLM_MAX_LAYERS),--max-layers $(SMOLLM_MAX_LAYERS),) \
	  --tol $(SMOLLM_STAGE_TOL)

smollm-train-parity: $(LIB)
	$(PYTHON) $(PYTHONFLAGS) scripts/tiny_train_parity.py \
	  --checkpoint $(SMOLLM_MODEL_DIR) \
	  --context $(SMOLLM_CONTEXT) \
	  --steps 1 \
	  --lr 1e-4

all-tests: $(LIB)
	$(MAKE) test
	$(MAKE) layer-parity-scalar TOL=$(ALL_TEST_LAYER_TOL) ARGS="$(ALL_TEST_LAYER_ARGS)"
	$(MAKE) tiny-parity

# Comprehensive test suite (scripts/run_all_tests.sh)
test-quick: $(LIB)
	@./scripts/run_all_tests.sh quick

test-full: $(LIB)
	@./scripts/run_all_tests.sh full

test-stress: $(LIB)
	@./scripts/run_all_tests.sh stress

# Profiling targets
PROFILE_CFLAGS := -O0 -g
PROFILE_PERF_CFLAGS := -O3 -fno-omit-frame-pointer -g

profile-memory: $(BUILD_DIR)
	@echo "Building with debug symbols..."
	$(MAKE) -B $(LIB) CFLAGS="$(PROFILE_CFLAGS) -fPIC -fopenmp -Wall $(AVX_FLAGS) $(INCLUDES)"
	$(MAKE) tiny-e2e
	@echo "Running Valgrind memcheck..."
	valgrind --leak-check=full --show-leak-kinds=all --track-origins=yes \
		--suppressions=valgrind.supp \
		./build/tiny_model --model-weights build/tiny_weights.bin \
		--tokens build/tiny_tokens.bin --out-logits build/tiny_logits.bin

profile-heap: $(BUILD_DIR)
	@echo "Building with debug symbols..."
	$(MAKE) -B $(LIB) CFLAGS="$(PROFILE_CFLAGS) -fPIC -fopenmp -Wall $(AVX_FLAGS) $(INCLUDES)"
	$(MAKE) tiny-e2e
	@echo "Running Valgrind massif..."
	valgrind --tool=massif --pages-as-heap=yes --massif-out-file=$(BUILD_DIR)/massif.out \
		./build/tiny_model --model-weights build/tiny_weights.bin \
		--tokens build/tiny_tokens.bin --out-logits build/tiny_logits.bin
	@echo "Heap profile saved to $(BUILD_DIR)/massif.out"
	@echo "View with: ms_print $(BUILD_DIR)/massif.out"

profile-cpu: $(BUILD_DIR)
	@echo "Building with frame pointers..."
	$(MAKE) -B $(LIB) CFLAGS="$(PROFILE_PERF_CFLAGS) -fPIC -fopenmp -Wall $(AVX_FLAGS) $(INCLUDES)"
	$(MAKE) tiny-e2e
	@echo "Recording with perf..."
	perf record -g -F 99 -o $(BUILD_DIR)/perf.data \
		./build/tiny_model --model-weights build/tiny_weights.bin \
		--tokens build/tiny_tokens.bin --out-logits build/tiny_logits.bin
	@echo "Profile saved to $(BUILD_DIR)/perf.data"
	perf report -i $(BUILD_DIR)/perf.data --stdio --sort=overhead | head -30

FLAMEGRAPH_DIR ?= $(HOME)/Programs/FlameGraph

flamegraph: $(BUILD_DIR)/perf.data
	@echo "Generating flamegraph..."
	perf script -i $(BUILD_DIR)/perf.data | $(FLAMEGRAPH_DIR)/stackcollapse-perf.pl | $(FLAMEGRAPH_DIR)/flamegraph.pl > $(BUILD_DIR)/flamegraph.svg
	@echo "Flamegraph saved to $(BUILD_DIR)/flamegraph.svg"

profile-cache: $(BUILD_DIR)
	@echo "Building with debug symbols..."
	$(MAKE) -B $(LIB) CFLAGS="$(PROFILE_CFLAGS) -fPIC -fopenmp -Wall $(AVX_FLAGS) $(INCLUDES)"
	$(MAKE) tiny-e2e
	@echo "Running Valgrind cachegrind..."
	valgrind --tool=cachegrind --cachegrind-out-file=$(BUILD_DIR)/cachegrind.out \
		./build/tiny_model --model-weights build/tiny_weights.bin \
		--tokens build/tiny_tokens.bin --out-logits build/tiny_logits.bin
	@echo "Cache profile saved to $(BUILD_DIR)/cachegrind.out"
	@echo "View with: cg_annotate $(BUILD_DIR)/cachegrind.out"

small-e2e: $(IR_DEMO) $(LIB)
	$(PYTHON) $(PYTHONFLAGS) scripts/gen_random_bump.py --config $(SMALL_CONFIG) --output $(BUILD_DIR)/small_weights.bin
	$(PYTHON) $(PYTHONFLAGS) scripts/gen_random_tokens.py --config $(SMALL_CONFIG) --output $(BUILD_DIR)/small_tokens.bin
	./$(IR_DEMO) $(SMALL_CONFIG) --emit $(BUILD_DIR)/small_generated.c
	$(CC) $(CFLAGS) -Iinclude $(BUILD_DIR)/small_generated.c $$(cat $(BUILD_DIR)/small_generated.c.kernels) -o $(BUILD_DIR)/small_model -lm
	LD_LIBRARY_PATH=$(BUILD_DIR):$$LD_LIBRARY_PATH $(BUILD_DIR)/small_model \
	  --model-weights $(BUILD_DIR)/small_weights.bin \
	  --tokens $(BUILD_DIR)/small_tokens.bin \
	  --out-logits $(BUILD_DIR)/small_logits.bin

help:
	@echo "C-Kernel-Engine Make targets:"
	@echo ""
	@echo "  CLI (main entry point):"
	@echo "  make ck-cli          Build CLI tool with all dependencies"
	@echo "  make generate-model MODEL=<name>  Generate C code for model (inspect before compile)"
	@echo "  ./build/ck run <model> [--generate-only] [--force-convert] [--verbose]"
	@echo ""
	@echo "  Build:"
	@echo "  make                 Build full engine library ($(LIB))"
	@echo "  make test            Build engine + per-kernel libs, run all Python kernel tests"
	@echo "  make $(IR_DEMO)      Build IR + codegen tool (HF config.json -> IR + C skeleton) into $(BUILD_DIR)"
	@echo "  make ck              Build IR tool and run it with $(DEFAULT_CONFIG) (prints forward/backward IR and C skeleton)"
	@echo "  make emit CONFIG=path OUT=path  Emit stitched runtime C file from config/IR (writes OUT.kernels manifest)"
	@echo "  make ck-emit CONFIG=path OUT=path  Alias for emit"
	@echo "  ./$(IR_DEMO) <config> --emit out.c  Emit a stitched runtime C file from config/IR (writes out.c.kernels)"
	@echo ""
	@echo "  Quantization:"
	@echo "  make test-quant       Run quantization kernel tests (dequant + q4/q8 gemm)"
	@echo "  make gguf-inspect GGUF=path   Inspect GGUF tensor dtypes (what is quantized?)"
	@echo "  make gguf-list GGUF=path      List all GGUF tensors (name/type/shape)"
	@echo "  make gguf-to-bump GGUF=path   Convert GGUF -> bump weights (GGUF_OUT/ GGUF_CONFIG_OUT / GGUF_CONTEXT)"
	@echo ""
	@echo "  Tests:"
	@echo "  make test-libs       Build per-kernel shared libs in $(BUILD_DIR) ($(LIB_GELU), $(LIB_RMSNORM), $(LIB_LN), $(LIB_SOFT), $(LIB_SWIGLU), $(LIB_SIGMOID))"
	@echo "  make litmus ARGS=\"--layers 1 --embed 64 --svg build/litmus.svg\"  Run LM head + CE backward parity litmus (PyTorch)"
	@echo "  make litmus-demo     Run the 100x100 litmus demo and capture output + SVG"
	@echo "  make layer-parity ARGS=\"--tokens 1024 --embed 64 --heads 4 --kv-heads 2 --rope\" TOL=5e-2  Run layer forward parity vs PyTorch"
	@echo "  make layer-parity-scalar ARGS=\"--tokens 1024 --embed 64 --heads 4 --kv-heads 2 --rope --strict-ref\" TOL=1e-3  Run layer parity with scalar build (C ref)"
	@echo "  make tiny-e2e        Generate random weights/tokens and run tiny end-to-end forward"
	@echo "  make tiny-train      Generate random weights/tokens/targets and run tiny forward+backward+SGD"
	@echo "  make tiny-parity     Run tiny end-to-end training parity vs PyTorch (1 step)"
	@echo "  make smollm-demo     Run a tiny SmolLM training demo (C only, uses HF weights + TinyStories)"
	@echo "  make smollm-forward  Run SmolLM forward parity vs PyTorch on a short prompt (C vs Torch logits)"
	@echo "  make smollm-layer-diff  Run per-stage diffs for one SmolLM layer vs PyTorch (C kernel vs ref)"
	@echo "  make smollm-bump-compare  Compare bump weights vs HF for one SmolLM layer"
	@echo "  make smollm-weight-check  Compare bump weights against a raw --out-weights dump"
	@echo "  make smollm-layer-stack  Compare C vs PyTorch per-layer outputs across the full stack"
	@echo "  make smollm-train-parity  Full forward+backward parity vs PyTorch with SmolLM weights"
	@echo "  make all-tests       Run kernel tests + layer parity + tiny parity (safe defaults)"
	@echo "  make test-quick      Comprehensive quick tests (<1 min) - tiny models, basic configs"
	@echo "  make test-full       Comprehensive full tests (5-10 min) - GQA, medium, deep, wide models"
	@echo "  make test-stress     Comprehensive stress tests (10+ min) - convergence, overfit tests"
	@echo "  make bench_gemm      Compare GEMM: Native vs MKL vs PyTorch"
	@echo "  make bench_gemm_native  Benchmark GEMM: Native vs PyTorch"
	@echo "  make bench_gemm_mkl     Benchmark GEMM: MKL vs PyTorch (requires oneAPI MKL)"
	@echo "  make bench-gemm-onednn  Compare GEMM: oneDNN vs CK (requires oneDNN)"
	@echo "  make profile-memory  Run Valgrind memcheck on tiny model"
	@echo "  make profile-heap    Run Valgrind massif heap profiler"
	@echo "  make profile-cpu     Run perf CPU profiler"
	@echo "  make profile-cache   Run Valgrind cachegrind"
	@echo "  make flamegraph      Generate SVG flamegraph (requires FlameGraph tools)"
	@echo "  make show_config     Display system hardware topology and recommendations"
	@echo "  make small-e2e       Same as tiny-e2e but ~10MB weights"
	@echo "  make libckernel_gelu.so      Build GELU-only shared library (outputs to $(LIB_GELU))"
	@echo "  make libckernel_rmsnorm.so   Build RMSNorm-only shared library (outputs to $(LIB_RMSNORM))"
	@echo "  make libckernel_layernorm.so Build LayerNorm-only shared library (AVX-512 if available, outputs to $(LIB_LN))"
	@echo "  make libckernel_softmax.so    Build Softmax-only shared library (outputs to $(LIB_SOFT))"
	@echo "  make libckernel_swiglu.so     Build SwiGLU-only shared library (outputs to $(LIB_SWIGLU))"
	@echo "  make libckernel_sigmoid.so    Build Sigmoid-only shared library (outputs to $(LIB_SIGMOID))"
	@echo "  make libckernel_relu.so       Build ReLU-only shared library (outputs to $(LIB_RELU))"
	@echo "  make libckernel_attention.so  Build attention-only shared library (scalar math + softmax kernel, outputs to $(LIB_ATTENTION))"
	@echo "  make libckernel_rope.so       Build RoPE-only shared library (outputs to $(LIB_ROPE))"
	@echo "  make gen-specs        Regenerate C kernel spec registry from kernel_maps/*.json"
	@echo ""
	@echo "  make test-relu        Run isolated ReLU C kernel tests (with PyTorch parity)"
	@echo ""
	@echo "Main CLI (ollama-style orchestrator):"
	@echo "  make ck-cli           Build the 'ck' orchestrator CLI"
	@echo "    ./build/ck run HuggingFaceTB/SmolLM-135M"
	@echo "    ./build/ck run https://huggingface.co/Qwen/Qwen2-0.5B --server"
	@echo "    ./build/ck list     List cached models"
	@echo "    ./build/ck remove <model>  Remove cached model"
	@echo ""
	@echo "Interactive Tools (llama.cpp style):"
	@echo "  make ck-chat          Build interactive CLI (C-based)"
	@echo "  make ck-server        Build streaming HTTP server (C-based)"
	@echo "  make ck-chat-py       Run interactive chat (Python + C inference)"
	@echo "  make ck-server-py     Run streaming server (Python + C inference)"
	@echo ""
	@echo "  make clean            Remove all built libraries"
	@echo ""
	@echo "Python unittest scripts (run with: python3 <script>):"
	@echo "  unittest/test_gelu.py        - GELU forward/backward vs PyTorch"
	@echo "  unittest/test_rmsnorm.py     - RMSNorm forward/backward vs PyTorch"
	@echo "  unittest/test_sigmoid.py     - Sigmoid forward/backward vs PyTorch"
	@echo "  unittest/test_layernorm.py   - LayerNorm forward/backward vs PyTorch"
	@echo "  unittest/test_softmax.py     - Causal softmax forward vs PyTorch"
	@echo "  unittest/test_softmax_backward.py - Causal softmax backward vs PyTorch"
	@echo "  unittest/test_gemm.py        - GEMM variants vs PyTorch matmul"
	@echo "  unittest/test_mlp.py         - MLP block forward/backward vs PyTorch"
	@echo "  unittest/test_swiglu.py      - SwiGLU activation forward/backward vs PyTorch"
	@echo "  unittest/test_relu.py        - ReLU activation forward/backward vs PyTorch"
	@echo "  unittest/test_orchestration_layer.py - Full layer forward stitch vs PyTorch (GQA/MHA)"
	@echo "  unittest/test_fused_swiglu_decode.py - Fused SwiGLU decode MLP parity"
	@echo "  unittest/test_fused_attention_decode.py - Fused attention decode parity"
	@echo "  unittest/bf16/test_sigmoid_bf16.py     - BF16 sigmoid forward/backward vs PyTorch"
	@echo "  unittest/bf16/test_rmsnorm_bf16.py    - BF16 RMSNorm forward/backward vs PyTorch"
	@echo "  unittest/bf16/test_mlp_bf16.py        - BF16 MLP forward vs PyTorch"
	@echo "  unittest/bf16/test_attention_bf16.py  - BF16 attention forward/backward vs PyTorch"
	@echo "  make test-bf16        - Run the BF16 kernel test suite"

clean:
	rm -rf $(BUILD_DIR)

# Litmus test for full forward pass parity with PyTorch
# ==============================================================================
TEST_HARNESS_SRCS := src/backend_native.c \
	src/ckernel_alloc.c \
	src/ckernel_ir.c \
	src/ckernel_orchestration.c \
	src/ckernel_registry.c \
	src/cpu_features.c \
	src/kernels/attention_kernels.c \
	src/kernels/attention_decode_fused.c \
	src/kernels/gelu_kernels.c \
	src/kernels/gemm_kernels.c \
	src/kernels/gemm_fused_kernels.c \
	src/kernels/mlp_fused_decode.c \
	src/kernels/gemm_microkernel.c \
	src/kernels/layernorm_kernels.c \
	src/kernels/mlp_kernels.c \
	src/kernels/rmsnorm_kernels.c \
	src/kernels/sigmoid_kernels.c \
	src/kernels/relu_kernels.c \
	src/kernels/softmax_kernels.c \
	src/kernels/swiglu_kernels.c \
	src/kernels/rope_kernels.c

.PHONY: litmus-test

litmus-test:
	@echo "--- [Step 1] Generating C Runtime Code ---"
	@make emit OUT=build/generated_model.c
	@echo "\n--- [Step 2] Generating PyTorch Reference Data ---"
	$(PYTHON) $(PYTHONFLAGS) unittest/generate_reference_data.py
	@echo "\n--- [Step 3] Compiling C Test Harness ---"
	$(CC) $(CFLAGS) -Iinclude test_forward_pass.c build/generated_model.c $(TEST_HARNESS_SRCS) -o build/test_forward_pass -lm -lpthread -lrt
	@echo "Compilation complete: build/test_forward_pass"
	@echo "\n--- [Step 4] Running C Test Harness ---"
	./build/test_forward_pass
	@echo "\n--- [Step 5] Comparing C output with PyTorch reference ---"
	$(PYTHON) $(PYTHONFLAGS) unittest/compare_outputs.py

# ============================================================================
# Interactive CLI and Server Tools
# ============================================================================

CK_TOKENIZER := src/ck_tokenizer.c
CK_MAIN := tools/ck_main.c
CK_SERVER := tools/ck_server.c
CK_CLI := tools/ck.c

# Main orchestrator (ck run, ck list, etc.)
# Suppress format-truncation warnings - paths are validated at runtime
$(BUILD_DIR)/ck: $(BUILD_DIR) $(CK_CLI)
	$(CC) -O2 -Wall -Wno-format-truncation -Wno-stringop-truncation -o $@ $(CK_CLI) -ldl -lm

# Build CLI with all dependencies (library + IR tool)
ck-cli: $(LIB) $(IR_DEMO) $(BUILD_DIR)/ck
	@echo ""
	@echo "  C-Kernel-Engine CLI built: $(BUILD_DIR)/ck"
	@echo "  Dependencies:"
	@echo "    - $(LIB)"
	@echo "    - $(IR_DEMO)"
	@echo ""
	@echo "  Usage:"
	@echo "    ./$(BUILD_DIR)/ck run HuggingFaceTB/SmolLM-135M"
	@echo "    ./$(BUILD_DIR)/ck run https://huggingface.co/Qwen/Qwen2-0.5B --server"
	@echo "    ./$(BUILD_DIR)/ck list"
	@echo "    ./$(BUILD_DIR)/ck help"
	@echo ""
	@echo "  To install system-wide:"
	@echo "    sudo cp $(BUILD_DIR)/ck /usr/local/bin/"
	@echo ""

# Generate C code for a model without compiling (for inspection)
# Usage: make generate-model MODEL=HuggingFaceTB/SmolLM-135M
generate-model: $(LIB) $(IR_DEMO) $(BUILD_DIR)/ck
ifndef MODEL
	@echo "Usage: make generate-model MODEL=<model-name>"
	@echo "Example: make generate-model MODEL=HuggingFaceTB/SmolLM-135M"
	@exit 1
endif
	@$(BUILD_DIR)/ck run $(MODEL) --generate-only --verbose

$(BUILD_DIR)/ck_main: $(BUILD_DIR) $(CK_MAIN) $(CK_TOKENIZER) include/ck_tokenizer.h
	$(CC) $(CFLAGS) -o $@ $(CK_MAIN) $(CK_TOKENIZER) -lm

$(BUILD_DIR)/ck_server: $(BUILD_DIR) $(CK_SERVER)
	$(CC) $(CFLAGS) -o $@ $(CK_SERVER) -lpthread

ck-chat: $(BUILD_DIR)/ck_main
	@echo "Interactive CLI built: $(BUILD_DIR)/ck_main"
	@echo "Usage: ./$(BUILD_DIR)/ck_main --help"

ck-server: $(BUILD_DIR)/ck_server
	@echo "Server built: $(BUILD_DIR)/ck_server"
	@echo "Usage: ./$(BUILD_DIR)/ck_server --port 8080"

ck-chat-py:
	$(PYTHON) $(PYTHONFLAGS) tools/ck_chat.py --model-dir $(SMOLLM_MODEL_DIR) --context $(SMOLLM_CONTEXT)

ck-server-py:
	$(PYTHON) $(PYTHONFLAGS) tools/ck_server.py --model-dir $(SMOLLM_MODEL_DIR) --context $(SMOLLM_CONTEXT)

# ============================================================================
# System Configuration and Topology
# ============================================================================

SHOW_CONFIG := $(BUILD_DIR)/show_config

$(SHOW_CONFIG): $(BUILD_DIR) src/system_topology.c src/show_config.c include/system_topology.h
	$(CC) -O2 -Wall -Wno-format-truncation -Iinclude -o $@ src/system_topology.c src/show_config.c

show_config: $(SHOW_CONFIG)
	@./$(SHOW_CONFIG)

show-config: show_config

# ============================================================================
# Status and Coverage Reports
# ============================================================================

opt-status:
	@$(PYTHON) scripts/optimization_status.py

opt-pending:
	@$(PYTHON) scripts/optimization_status.py --pending

opt-inference:
	@$(PYTHON) scripts/optimization_status.py --inference

opt-training:
	@$(PYTHON) scripts/optimization_status.py --training

opt-kernels:
	@$(PYTHON) scripts/optimization_status.py --kernels

opt-targets:
	@$(PYTHON) scripts/optimization_status.py --targets

opt-md:
	@$(PYTHON) scripts/optimization_status.py --markdown

kernel-coverage:
	@$(PYTHON) scripts/kernel_coverage.py

kernel-coverage-md:
	@$(PYTHON) scripts/kernel_coverage.py --markdown

test-coverage:
	@$(PYTHON) scripts/test_coverage.py

test-coverage-md:
	@$(PYTHON) scripts/test_coverage.py --markdown

# ============================================================================
# Status Reports (reads from meta/kernel_meta.json)
# ============================================================================
# Usage:
#   make report        - Full comprehensive report (kernel status, roadmaps, tests)
#   make opt-status    - Quick kernel implementation table with opt levels
#   make opt-pending   - Show what's not done yet
#   make test-coverage - Show test file coverage
#
# To update the report:
#   1. Edit meta/kernel_meta.json when you add/modify kernels
#   2. Update "opt_level" arrays when adding SIMD/blocking/parallel
#   3. Run "make report" to see updated status
#
# Optional validation (checks JSON matches source code):
#   make meta-check    - Report discrepancies between JSON and code
# ============================================================================

meta-check:
	@$(PYTHON) scripts/sync_kernel_meta.py --check

meta-sync:
	@$(PYTHON) scripts/sync_kernel_meta.py --update

# Comprehensive report - runs all status reports
report:
	@echo ""
	@echo "╔══════════════════════════════════════════════════════════════════════════════════════════════════╗"
	@echo "║                              C-KERNEL-ENGINE COMPREHENSIVE REPORT                                ║"
	@echo "╚══════════════════════════════════════════════════════════════════════════════════════════════════╝"
	@echo ""
	@echo "┌──────────────────────────────────────────────────────────────────────────────────────────────────┐"
	@echo "│  1. KERNEL IMPLEMENTATION STATUS (with optimization levels)                                      │"
	@echo "│     Legend: A1=AVX1, A5=AVX512, BF=BF16, AM=AMX, +=blocked/parallel/fused, S=scalar              │"
	@echo "└──────────────────────────────────────────────────────────────────────────────────────────────────┘"
	@$(PYTHON) scripts/optimization_status.py --kernels
	@echo ""
	@echo "┌──────────────────────────────────────────────────────────────────────────────────────────────────┐"
	@echo "│  2. INFERENCE OPTIMIZATION ROADMAP                                                               │"
	@echo "└──────────────────────────────────────────────────────────────────────────────────────────────────┘"
	@$(PYTHON) scripts/optimization_status.py --inference
	@echo ""
	@echo "┌──────────────────────────────────────────────────────────────────────────────────────────────────┐"
	@echo "│  3. TRAINING OPTIMIZATION ROADMAP                                                                │"
	@echo "└──────────────────────────────────────────────────────────────────────────────────────────────────┘"
	@$(PYTHON) scripts/optimization_status.py --training
	@echo ""
	@echo "┌──────────────────────────────────────────────────────────────────────────────────────────────────┐"
	@echo "│  4. SINGLE-CORE PRIORITIES & PERFORMANCE TARGETS                                                 │"
	@echo "└──────────────────────────────────────────────────────────────────────────────────────────────────┘"
	@$(PYTHON) scripts/optimization_status.py --single-core
	@$(PYTHON) scripts/optimization_status.py --targets
	@echo ""
	@echo "┌──────────────────────────────────────────────────────────────────────────────────────────────────┐"
	@echo "│  5. TEST COVERAGE                                                                                │"
	@echo "└──────────────────────────────────────────────────────────────────────────────────────────────────┘"
	@$(PYTHON) scripts/test_coverage.py --summary
	@$(PYTHON) scripts/test_coverage.py --missing
	@echo ""
	@echo "┌──────────────────────────────────────────────────────────────────────────────────────────────────┐"
	@echo "│  6. HIGH-PRIORITY PENDING WORK                                                                   │"
	@echo "└──────────────────────────────────────────────────────────────────────────────────────────────────┘"
	@$(PYTHON) scripts/optimization_status.py --pending
	@echo ""
	@echo "╔══════════════════════════════════════════════════════════════════════════════════════════════════╗"
	@echo "║                                        END OF REPORT                                             ║"
	@echo "╚══════════════════════════════════════════════════════════════════════════════════════════════════╝"

# Generate markdown report for documentation
report-md:
	@echo "# C-Kernel-Engine Status Report"
	@echo ""
	@echo "Generated: $$(date)"
	@echo ""
	@$(PYTHON) scripts/kernel_coverage.py --markdown
	@echo ""
	@$(PYTHON) scripts/optimization_status.py --markdown

.PHONY: all clean test test-bf16 test-libs test-quant help litmus litmus-test test-quick test-full test-stress profile-memory profile-heap profile-cpu profile-cache flamegraph ck-cli ck-chat ck-server ck-chat-py ck-server-py generate-model gguf-inspect gguf-list gguf-to-bump opt-status opt-pending opt-inference opt-training opt-kernels opt-targets opt-md kernel-coverage kernel-coverage-md test-coverage test-coverage-md meta-check meta-sync meta-init report report-md show_config show-config
