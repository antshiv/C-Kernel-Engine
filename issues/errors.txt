Traceback (most recent call last):
  File "/opt/app-root/src/Ashivaku/C-Kernel-Engine/scripts/smollm_forward_parity.py", line 241, in <module>
    main()
  File "/opt/app-root/src/Ashivaku/C-Kernel-Engine/scripts/smollm_forward_parity.py", line 214, in main
    model = AutoModelForCausalLM.from_pretrained(
  File "/opt/app-root/lib64/python3.9/site-packages/transformers/models/auto/auto_factory.py", line 601, in from_pretrained
    model_class = _get_model_class(config, cls._model_mapping)
  File "/opt/app-root/lib64/python3.9/site-packages/transformers/models/auto/auto_factory.py", line 394, in _get_model_class
    supported_models = model_mapping[type(config)]
  File "/opt/app-root/lib64/python3.9/site-packages/transformers/models/auto/auto_factory.py", line 807, in __getitem__
    return self._load_attr_from_module(model_type, model_name)
  File "/opt/app-root/lib64/python3.9/site-packages/transformers/models/auto/auto_factory.py", line 821, in _load_attr_from_module
    return getattribute_from_module(self._modules[module_name], attr)
  File "/opt/app-root/lib64/python3.9/site-packages/transformers/models/auto/auto_factory.py", line 743, in getattribute_from_module
    raise ValueError(f"Could not find {attr} neither in {module} nor in {transformers_module}!")
ValueError: Could not find LlamaForCausalLM neither in <module 'transformers.models.llama' from '/opt/app-root/lib64/python3.9/site-packages/transformers/models/llama/__init__.py'> nor in <module 'transformers' from '/opt/app-root/lib64/python3.9/site-packages/transformers/__init__.py'>!
make: *** [Makefile:263: smollm-forward] Error 1
(app-root) (app-root) (app-root) 