#!/usr/bin/env python3
"""
codegen_v4.py - C source emitter for IR v4 layouts.

Emits forward implementations that call real kernels (fp32-only for now).
"""

from datetime import datetime
import os
from typing import List

import build_ir_v3 as v3


def _layer_buffer_names(section) -> List[str]:
    if not section.layers:
        return []
    return [buf.name for buf in section.layers[0].buffers]


def _layer_has_field(layer_names: List[str], field: str) -> bool:
    suffix = f".{field}"
    for name in layer_names:
        if name.endswith(suffix):
            return True
    return False


def emit_c_source_v4(layout: v3.ModelLayout,
                     output_path: str,
                     header_name: str,
                     mode: str,
                     emit_main: bool = False) -> None:
    """Emit generated_model.c with real kernel calls (fp32-only)."""
    if mode not in ("prefill", "decode"):
        raise ValueError(f"v4 codegen only supports prefill/decode (got: {mode})")
    config = layout.config
    section = layout.sections[0]

    safe_name = layout.name.upper().replace("-", "_").replace(".", "_")
    safe_name_lower = safe_name.lower()

    layer_names = _layer_buffer_names(section)

    required_prefill = [
        "ln1_gamma", "ln1_out", "wq", "wk", "wv",
        "q", "k", "v", "attn_out", "wo", "proj_tmp",
        "residual1", "ln2_gamma", "ln2_out",
        "w1", "fc1_out", "swiglu_out", "w2",
        "mlp_out", "output",
    ]
    if config["num_heads"] > 1:
        required_prefill.append("proj_scratch")

    required_decode = [
        "ln1_gamma", "ln1_out", "wq", "wk", "wv",
        "k", "v", "wo", "proj_tmp", "residual1",
        "ln2_gamma", "ln2_out", "w1", "fc1_out",
        "swiglu_out", "w2", "mlp_out", "output",
    ]

    required = required_prefill if mode == "prefill" else required_decode
    missing = [field for field in required if not _layer_has_field(layer_names, field)]
    if missing:
        missing_list = ", ".join(missing)
        raise ValueError(
            f"v4 codegen needs unfused buffers ({missing_list}). "
            "Re-run build_ir_v4.py with --fusion=off."
        )

    has_scores = _layer_has_field(layer_names, "scores")
    has_proj_scratch = _layer_has_field(layer_names, "proj_scratch")
    has_rope = any(buf.name in {"rope_cos_cache", "rope_sin_cache"} for buf in section.globals)

    lines = []
    def add(s=""):
        lines.append(s)

    add("/**")
    add(f" * @file {os.path.basename(output_path)}")
    add(f" * @brief AUTO-GENERATED: {layout.name} Implementation (IR v4)")
    add(f" *")
    add(f" * Generated: {datetime.utcnow().isoformat()} UTC")
    add(f" * Total Memory: {layout.total_bytes / 1e9:.2f} GB")
    add(f" *")
    add(f" * DO NOT EDIT - Regenerate with build_ir_v4.py")
    add(f" */")
    add()
    add("#define _GNU_SOURCE  /* For MAP_ANONYMOUS, MAP_HUGETLB */")
    add()
    add(f'#include "{header_name}"')
    add()
    add('#include "ckernel_engine.h"')
    add('#include "ckernel_orchestration.h"')
    add()
    add("#include <stdio.h>")
    add("#include <stdlib.h>")
    add("#include <string.h>")
    add("#include <stdint.h>")
    add("#include <math.h>")
    add()
    add("#ifdef __linux__")
    add("#include <sys/mman.h>")
    add("#endif")
    add()
    add(f"#if {safe_name}_DTYPE_BYTES != 4")
    add(f'#error "{layout.name}: v4 codegen currently supports fp32 only. Use --dtype=fp32."')
    add("#endif")
    add()

    add("/* Forward declarations */")
    add(f"static void {safe_name_lower}_init_canaries({safe_name}Model *model);")
    if mode == "decode":
        add(f"static void {safe_name_lower}_decode_token({safe_name}Model *model, const int *token, int token_index);")
    add()

    # Magic header
    add("/* ============================================================================")
    add(" * MAGIC HEADER")
    add(" * ============================================================================ */")
    add()
    add("typedef struct __attribute__((packed)) {")
    add(f"    uint32_t magic;           /* 0x{v3.MAGIC_PREFIX:08X} */")
    add("    uint32_t version;          /* IR version */")
    add("    uint64_t total_bytes;")
    add("    uint64_t weight_bytes;")
    add("    uint64_t activation_bytes;")
    add("    uint32_t num_layers;")
    add("    uint32_t embed_dim;")
    add("    uint32_t num_heads;")
    add("    uint32_t vocab_size;")
    add("    uint32_t max_seq_len;")
    add("    uint32_t canary_count;")
    add("    uint8_t  reserved[8];       /* Pad to 64 bytes */")
    add("} MagicHeader;")
    add()
    add("_Static_assert(sizeof(MagicHeader) == 64, \"MagicHeader must be 64 bytes\");")
    add()

    # Allocation
    add("/* ============================================================================")
    add(" * ALLOCATION")
    add(" * ============================================================================ */")
    add()
    add(f"int {safe_name_lower}_model_allocate({safe_name}Model *model) {{")
    add(f"    size_t total = {safe_name}_TOTAL_BYTES;")
    add()
    add("#ifdef __linux__")
    add("    /* Try hugepages first */")
    add("    model->base = mmap(NULL, total,")
    add("                       PROT_READ | PROT_WRITE,")
    add("                       MAP_PRIVATE | MAP_ANONYMOUS | MAP_HUGETLB,")
    add("                       -1, 0);")
    add("    if (model->base == MAP_FAILED) {")
    add("        /* Fall back to regular pages */")
    add("        model->base = mmap(NULL, total,")
    add("                           PROT_READ | PROT_WRITE,")
    add("                           MAP_PRIVATE | MAP_ANONYMOUS,")
    add("                           -1, 0);")
    add("    }")
    add("    if (model->base == MAP_FAILED) {")
    add("        perror(\"mmap failed\");")
    add("        return -1;")
    add("    }")
    add("#else")
    add("    model->base = aligned_alloc(64, total);")
    add("    if (!model->base) {")
    add("        perror(\"aligned_alloc failed\");")
    add("        return -1;")
    add("    }")
    add("#endif")
    add()
    add("    model->total_bytes = total;")
    add()
    add("    /* Initialize magic header */")
    add("    MagicHeader *header = (MagicHeader *)model->base;")
    add(f"    header->magic = {safe_name}_MAGIC;")
    add("    header->version = 4;")
    add(f"    header->total_bytes = {safe_name}_TOTAL_BYTES;")
    add(f"    header->weight_bytes = {safe_name}_WEIGHT_BYTES;")
    add(f"    header->activation_bytes = {safe_name}_ACTIVATION_BYTES;")
    add(f"    header->num_layers = {safe_name}_NUM_LAYERS;")
    add(f"    header->embed_dim = {safe_name}_EMBED_DIM;")
    add(f"    header->num_heads = {safe_name}_NUM_HEADS;")
    add(f"    header->vocab_size = {safe_name}_VOCAB_SIZE;")
    add(f"    header->max_seq_len = {safe_name}_MAX_SEQ_LEN;")
    add(f"    header->canary_count = {safe_name}_CANARY_COUNT;")
    add()
    add("    /* Initialize canaries */")
    add(f"    {safe_name_lower}_init_canaries(model);")
    add()
    add("    return 0;")
    add("}")
    add()

    # Free function
    add(f"void {safe_name_lower}_model_free({safe_name}Model *model) {{")
    add("    if (!model || !model->base) return;")
    add()
    add("#ifdef __linux__")
    add("    munmap(model->base, model->total_bytes);")
    add("#else")
    add("    free(model->base);")
    add("#endif")
    add()
    add("    model->base = NULL;")
    add("    model->total_bytes = 0;")
    add("}")
    add()

    # Canary init
    add("/* ============================================================================")
    add(" * CANARY SYSTEM")
    add(" * ============================================================================ */")
    add()
    add(f"static void {safe_name_lower}_init_canaries({safe_name}Model *model) {{")
    add("    uint32_t *ptr;")
    add()
    add(f"    /* Write 0x{v3.CANARY_VALUE:08X} to each canary slot */")
    add(f"    for (int i = 0; i < {safe_name}_CANARY_COUNT; i++) {{")
    add(f"        ptr = (uint32_t*)((char*)model->base + {safe_name}_CANARIES[i].offset);")
    add(f"        for (int j = 0; j < {v3.CANARY_SIZE // 4}; j++) {{")
    add(f"            ptr[j] = {safe_name}_CANARY_VALUE;")
    add("        }")
    add("    }")
    add("}")
    add()

    # Canary verify
    add(f"int {safe_name_lower}_verify_canaries({safe_name}Model *model) {{")
    add("    int errors = 0;")
    add("    uint32_t *ptr;")
    add()
    add(f"    for (int i = 0; i < {safe_name}_CANARY_COUNT; i++) {{")
    add(f"        ptr = (uint32_t*)((char*)model->base + {safe_name}_CANARIES[i].offset);")
    add(f"        for (int j = 0; j < {v3.CANARY_SIZE // 4}; j++) {{")
    add(f"            if (ptr[j] != {safe_name}_CANARY_VALUE) {{")
    add(f'                fprintf(stderr, "CANARY CORRUPTION: %s at offset 0x%lX\\n",')
    add(f"                        {safe_name}_CANARIES[i].name,")
    add(f"                        {safe_name}_CANARIES[i].offset);")
    add("                errors++;")
    add("                break;")
    add("            }")
    add("        }")
    add("    }")
    add()
    add("    return errors;")
    add("}")
    add()

    # Alignment helper
    add("/* ============================================================================")
    add(" * ALIGNMENT HELPERS")
    add(" * ============================================================================ */")
    add()
    add(f"static int {safe_name_lower}_align_elems(int elems, int elem_bytes, int align_bytes) {{")
    add("    int bytes = elems * elem_bytes;")
    add("    int aligned = (bytes + align_bytes - 1) / align_bytes * align_bytes;")
    add("    return aligned / elem_bytes;")
    add("}")
    add()

    # RoPE precompute
    add("/* ============================================================================")
    add(" * ROPE PRECOMPUTE")
    add(" * ============================================================================ */")
    add()
    add(f"void {safe_name_lower}_precompute_rope({safe_name}Model *model) {{")
    if has_rope:
        add(f"    const int T = {safe_name}_MAX_SEQ_LEN;")
        add(f"    const int D = {safe_name}_HEAD_DIM / 2;")
        add(f"    const float theta = {config.get('rope_theta', 10000.0)}f;")
        add()
        add(f"    float *cos_ptr = {safe_name}_PTR(model, {safe_name}_GLOBALS.rope_cos_cache);")
        add(f"    float *sin_ptr = {safe_name}_PTR(model, {safe_name}_GLOBALS.rope_sin_cache);")
        add()
        add("    for (int pos = 0; pos < T; pos++) {")
        add("        for (int i = 0; i < D; i++) {")
        add("            float freq = 1.0f / powf(theta, (float)(2 * i) / (float)(D * 2));")
        add("            float angle = (float)pos * freq;")
        add("            cos_ptr[pos * D + i] = cosf(angle);")
        add("            sin_ptr[pos * D + i] = sinf(angle);")
        add("        }")
        add("    }")
    else:
        add("    /* No RoPE globals defined */")
        add("    (void)model;")
    add("}")
    add()

    if mode == "prefill":
        add("/* ============================================================================")
        add(" * LAYER FORWARD (PREFILL)")
        add(" * ============================================================================ */")
        add()
        add(f"static void {safe_name_lower}_layer_forward_prefill(")
        add(f"    {safe_name}Model *model,")
        add("    int layer_id,")
        add("    int num_tokens,")
        add("    int aligned_embed_dim,")
        add("    int aligned_head_dim,")
        add("    int aligned_intermediate_dim,")
        add("    int aligned_context_window")
        add(") {")
        add(f"    const {safe_name}LayerOffsets *L = &{safe_name}_LAYERS[layer_id];")
        add("    CKLayerForwardParams p = {0};")
        add()
        add("    p.tokens = num_tokens;")
        add(f"    p.embed_dim = {safe_name}_EMBED_DIM;")
        add("    p.aligned_embed_dim = aligned_embed_dim;")
        add(f"    p.num_heads = {safe_name}_NUM_HEADS;")
        add(f"    p.num_kv_heads = {safe_name}_NUM_KV_HEADS;")
        add(f"    p.head_dim = {safe_name}_HEAD_DIM;")
        add("    p.aligned_head_dim = aligned_head_dim;")
        add("    p.aligned_context_window = aligned_context_window;")
        add(f"    p.intermediate_dim = {safe_name}_INTERMEDIATE;")
        add("    p.aligned_intermediate_dim = aligned_intermediate_dim;")
        add(f"    p.eps = {config.get('rms_norm_eps', 1e-6)}f;")
        add("    p.rope_pos_offset = 0;")
        add()
        add("    if (layer_id == 0) {")
        add(f"        p.input = {safe_name}_PTR(model, {safe_name}_HEADER.embedded_input);")
        add("    } else {")
        add(f"        p.input = {safe_name}_PTR(model, {safe_name}_LAYERS[layer_id - 1].output);")
        add("    }")
        add()
        add(f"    p.ln1_gamma = {safe_name}_PTR(model, L->ln1_gamma);")
        add(f"    p.ln2_gamma = {safe_name}_PTR(model, L->ln2_gamma);")
        add()
        if has_rope:
            add(f"    p.rope_cos = {safe_name}_PTR(model, {safe_name}_GLOBALS.rope_cos_cache);")
            add(f"    p.rope_sin = {safe_name}_PTR(model, {safe_name}_GLOBALS.rope_sin_cache);")
        else:
            add("    p.rope_cos = NULL;")
            add("    p.rope_sin = NULL;")
        add()
        add(f"    p.wq = {safe_name}_PTR(model, L->wq);")
        add("    p.bq = NULL;")
        add(f"    p.wk = {safe_name}_PTR(model, L->wk);")
        add("    p.bk = NULL;")
        add(f"    p.wv = {safe_name}_PTR(model, L->wv);")
        add("    p.bv = NULL;")
        add(f"    p.wo = {safe_name}_PTR(model, L->wo);")
        add("    p.bo = NULL;")
        add(f"    p.w1 = {safe_name}_PTR(model, L->w1);")
        add("    p.b1 = NULL;")
        add(f"    p.w2 = {safe_name}_PTR(model, L->w2);")
        add("    p.b2 = NULL;")
        add()
        add(f"    p.ln1_out = {safe_name}_PTR(model, L->ln1_out);")
        add("    p.ln1_rstd = NULL;")
        add(f"    p.q = {safe_name}_PTR(model, L->q);")
        add(f"    p.k = {safe_name}_PTR(model, L->k);")
        add(f"    p.v = {safe_name}_PTR(model, L->v);")
        if has_scores:
            add(f"    p.scores = {safe_name}_PTR(model, L->scores);")
        else:
            add("    p.scores = NULL;")
        add(f"    p.attn_out = {safe_name}_PTR(model, L->attn_out);")
        add(f"    p.proj_tmp = {safe_name}_PTR(model, L->proj_tmp);")
        if has_proj_scratch:
            add(f"    p.proj_scratch = {safe_name}_PTR(model, L->proj_scratch);")
        else:
            add("    p.proj_scratch = NULL;")
        add(f"    p.residual1 = {safe_name}_PTR(model, L->residual1);")
        add(f"    p.ln2_out = {safe_name}_PTR(model, L->ln2_out);")
        add("    p.ln2_rstd = NULL;")
        add(f"    p.fc1_out = {safe_name}_PTR(model, L->fc1_out);")
        add(f"    p.swiglu_out = {safe_name}_PTR(model, L->swiglu_out);")
        add(f"    p.mlp_out = {safe_name}_PTR(model, L->mlp_out);")
        add(f"    p.output = {safe_name}_PTR(model, L->output);")
        add()
        add("    ck_layer_forward_rmsnorm_swiglu(&p);")
        add()
        add("    kv_cache_repack_head_major_inplace(p.k,")
        add("                                       p.num_kv_heads,")
        add("                                       num_tokens,")
        add("                                       aligned_context_window,")
        add("                                       aligned_head_dim);")
        add("    kv_cache_repack_head_major_inplace(p.v,")
        add("                                       p.num_kv_heads,")
        add("                                       num_tokens,")
        add("                                       aligned_context_window,")
        add("                                       aligned_head_dim);")
        add("}")
        add()

        add("/* ============================================================================")
        add(" * FORWARD PASS (PREFILL)")
        add(" * ============================================================================ */")
        add()
        add(f"void {safe_name_lower}_forward(")
        add(f"    {safe_name}Model *model,")
        add("    const int *tokens,")
        add("    int num_tokens")
        add(") {")
        add("    if (!model || !tokens || num_tokens <= 0) {")
        add("        return;")
        add("    }")
        add()
        add(f"    const int elem_bytes = {safe_name}_DTYPE_BYTES;")
        add(f"    const int aligned_embed_dim = {safe_name_lower}_align_elems({safe_name}_EMBED_DIM, elem_bytes, 64);")
        add(f"    const int aligned_head_dim = {safe_name_lower}_align_elems({safe_name}_HEAD_DIM, elem_bytes, 64);")
        add(f"    const int aligned_intermediate_dim = {safe_name_lower}_align_elems({safe_name}_INTERMEDIATE, elem_bytes, 64);")
        add(f"    const int aligned_context_window = {safe_name_lower}_align_elems({safe_name}_MAX_SEQ_LEN, elem_bytes, 64);")
        add()
        add(f"    float *embed_weight = {safe_name}_PTR(model, {safe_name}_HEADER.token_emb);")
        add(f"    float *embed_out = {safe_name}_PTR(model, {safe_name}_HEADER.embedded_input);")
        add("    embedding_forward((const int32_t *)tokens,")
        add("                      num_tokens,")
        add(f"                      {safe_name}_VOCAB_SIZE,")
        add("                      embed_weight,")
        add("                      NULL,")
        add("                      embed_out,")
        add(f"                      {safe_name}_EMBED_DIM,")
        add("                      aligned_embed_dim,")
        add("                      num_tokens,")
        add("                      0);")
        add()
        add(f"    for (int layer_id = 0; layer_id < {safe_name}_NUM_LAYERS; ++layer_id) {{")
        add(f"        {safe_name_lower}_layer_forward_prefill(")
        add("            model,")
        add("            layer_id,")
        add("            num_tokens,")
        add("            aligned_embed_dim,")
        add("            aligned_head_dim,")
        add("            aligned_intermediate_dim,")
        add("            aligned_context_window);")
        add("    }")
        add()
        add(f"    float *last_hidden = {safe_name}_PTR(model, {safe_name}_LAYERS[{safe_name}_NUM_LAYERS - 1].output);")
        add(f"    float *final_ln_weight = {safe_name}_PTR(model, {safe_name}_FOOTER.final_ln_weight);")
        add(f"    float *final_out = {safe_name}_PTR(model, {safe_name}_FOOTER.final_output);")
        add("    rmsnorm_forward(last_hidden,")
        add("                   final_ln_weight,")
        add("                   final_out,")
        add("                   NULL,")
        add("                   num_tokens,")
        add(f"                   {safe_name}_EMBED_DIM,")
        add("                   aligned_embed_dim,")
        add(f"                   {config.get('rms_norm_eps', 1e-6)}f);")
        add()
        add(f"    float *lm_head = {safe_name}_PTR(model, {safe_name}_FOOTER.lm_head_weight);")
        add(f"    float *logits = {safe_name}_PTR(model, {safe_name}_FOOTER.logits);")
        add("    gemm_blocked_serial(final_out,")
        add("                        lm_head,")
        add("                        NULL,")
        add("                        logits,")
        add("                        num_tokens,")
        add(f"                        {safe_name}_VOCAB_SIZE,")
        add("                        aligned_embed_dim);")
        add("}")
        add()

    else:
        add("/* ============================================================================")
        add(" * LAYER FORWARD (DECODE, SINGLE TOKEN)")
        add(" * ============================================================================ */")
        add()
        add(f"static void {safe_name_lower}_layer_forward_decode_token(")
        add(f"    {safe_name}Model *model,")
        add("    int layer_id,")
        add("    int token_index,")
        add("    int aligned_embed_dim,")
        add("    int aligned_head_dim,")
        add("    int aligned_intermediate_dim,")
        add("    int aligned_context_window")
        add(") {")
        add(f"    const {safe_name}LayerOffsets *L = &{safe_name}_LAYERS[layer_id];")
        add()
        add(f"    const int H = {safe_name}_NUM_HEADS;")
        add(f"    const int H_kv = {safe_name}_NUM_KV_HEADS;")
        add(f"    const int head_dim = {safe_name}_HEAD_DIM;")
        add()
        add("    float *input_row;")
        add("    if (layer_id == 0) {")
        add(f"        input_row = {safe_name}_PTR(model, {safe_name}_HEADER.embedded_input);")
        add("    } else {")
        add(f"        input_row = {safe_name}_PTR(model, {safe_name}_LAYERS[layer_id - 1].output);")
        add("    }")
        add()
        add(f"    float *ln1_out = {safe_name}_PTR(model, L->ln1_out);")
        add(f"    float *ln2_out = {safe_name}_PTR(model, L->ln2_out);")
        add(f"    float *proj_tmp = {safe_name}_PTR(model, L->proj_tmp);")
        add(f"    float *residual1 = {safe_name}_PTR(model, L->residual1);")
        add(f"    float *fc1_out = {safe_name}_PTR(model, L->fc1_out);")
        add(f"    float *swiglu_out = {safe_name}_PTR(model, L->swiglu_out);")
        add(f"    float *mlp_out = {safe_name}_PTR(model, L->mlp_out);")
        add(f"    float *output = {safe_name}_PTR(model, L->output);")
        add()
        add(f"    float *k_cache = {safe_name}_PTR(model, L->k);")
        add(f"    float *v_cache = {safe_name}_PTR(model, L->v);")
        add()
        add("    float q_token[H * aligned_head_dim];")
        add("    float k_token[H_kv * aligned_head_dim];")
        add("    float v_token[H_kv * aligned_head_dim];")
        add("    float attn_token[H * aligned_head_dim];")
        add()
        add("    rmsnorm_forward(input_row,")
        add(f"                   {safe_name}_PTR(model, L->ln1_gamma),")
        add("                   ln1_out,")
        add("                   NULL,")
        add("                   1,")
        add(f"                   {safe_name}_EMBED_DIM,")
        add("                   aligned_embed_dim,")
        add(f"                   {config.get('rms_norm_eps', 1e-6)}f);")
        add()
        add("    ck_qkv_project_head_major_token(ln1_out,")
        add(f"                                    {safe_name}_PTR(model, L->wq),")
        add("                                    NULL,")
        add(f"                                    {safe_name}_PTR(model, L->wk),")
        add("                                    NULL,")
        add(f"                                    {safe_name}_PTR(model, L->wv),")
        add("                                    NULL,")
        add("                                    q_token,")
        add("                                    k_token,")
        add("                                    v_token,")
        add("                                    aligned_embed_dim,")
        add("                                    H,")
        add("                                    H_kv,")
        add("                                    aligned_head_dim);")
        add()
        if has_rope:
            add("    rope_forward_qk(q_token,")
            add("                    k_token,")
            add(f"                    {safe_name}_PTR(model, {safe_name}_GLOBALS.rope_cos_cache),")
            add(f"                    {safe_name}_PTR(model, {safe_name}_GLOBALS.rope_sin_cache),")
            add("                    H,")
            add("                    H_kv,")
            add("                    1,")
            add("                    head_dim,")
            add("                    aligned_head_dim,")
            add("                    token_index);")
            add()
        add("    kv_cache_write_head_major(k_token,")
        add("                             v_token,")
        add("                             k_cache,")
        add("                             v_cache,")
        add("                             H_kv,")
        add("                             token_index,")
        add("                             aligned_context_window,")
        add("                             head_dim,")
        add("                             aligned_head_dim);")
        add()
        add("    attention_forward_decode_head_major_gqa_flash(q_token,")
        add("                                                  k_cache,")
        add("                                                  v_cache,")
        add("                                                  attn_token,")
        add("                                                  H,")
        add("                                                  H_kv,")
        add("                                                  token_index + 1,")
        add("                                                  aligned_context_window,")
        add("                                                  head_dim,")
        add("                                                  aligned_head_dim);")
        add()
        add("    ck_attention_project_head_major_decode_token(attn_token,")
        add(f"                                                 {safe_name}_PTR(model, L->wo),")
        add("                                                 NULL,")
        add("                                                 proj_tmp,")
        add(f"                                                 {safe_name}_EMBED_DIM,")
        add("                                                 aligned_embed_dim,")
        add("                                                 H,")
        add("                                                 aligned_head_dim);")
        add()
        add("    ck_residual_add_token_major(input_row,")
        add("                                proj_tmp,")
        add("                                residual1,")
        add("                                1,")
        add("                                aligned_embed_dim);")
        add()
        add("    rmsnorm_forward(residual1,")
        add(f"                   {safe_name}_PTR(model, L->ln2_gamma),")
        add("                   ln2_out,")
        add("                   NULL,")
        add("                   1,")
        add(f"                   {safe_name}_EMBED_DIM,")
        add("                   aligned_embed_dim,")
        add(f"                   {config.get('rms_norm_eps', 1e-6)}f);")
        add()
        add("    ck_mlp_swiglu_forward(ln2_out,")
        add(f"                          {safe_name}_PTR(model, L->w1),")
        add("                          NULL,")
        add(f"                          {safe_name}_PTR(model, L->w2),")
        add("                          NULL,")
        add("                          fc1_out,")
        add("                          swiglu_out,")
        add("                          mlp_out,")
        add("                          1,")
        add("                          aligned_embed_dim,")
        add("                          aligned_intermediate_dim);")
        add()
        add("    ck_residual_add_token_major(residual1,")
        add("                                mlp_out,")
        add("                                output,")
        add("                                1,")
        add("                                aligned_embed_dim);")
        add("}")
        add()

        add("/* ============================================================================")
        add(" * DECODE HELPERS")
        add(" * ============================================================================ */")
        add()
        add(f"static void {safe_name_lower}_decode_token(")
        add(f"    {safe_name}Model *model,")
        add("    const int *token,")
        add("    int token_index")
        add(") {")
        add("    if (!model || !token) {")
        add("        return;")
        add("    }")
        add()
        add(f"    const int elem_bytes = {safe_name}_DTYPE_BYTES;")
        add(f"    const int aligned_embed_dim = {safe_name_lower}_align_elems({safe_name}_EMBED_DIM, elem_bytes, 64);")
        add(f"    const int aligned_head_dim = {safe_name_lower}_align_elems({safe_name}_HEAD_DIM, elem_bytes, 64);")
        add(f"    const int aligned_intermediate_dim = {safe_name_lower}_align_elems({safe_name}_INTERMEDIATE, elem_bytes, 64);")
        add(f"    const int aligned_context_window = {safe_name_lower}_align_elems({safe_name}_MAX_SEQ_LEN, elem_bytes, 64);")
        add("    if (token_index < 0 || token_index >= aligned_context_window) {")
        add("        return;")
        add("    }")
        add()
        add(f"    float *embed_weight = {safe_name}_PTR(model, {safe_name}_HEADER.token_emb);")
        add(f"    float *embed_out = {safe_name}_PTR(model, {safe_name}_HEADER.embedded_input);")
        add("    embedding_forward((const int32_t *)token,")
        add("                      1,")
        add(f"                      {safe_name}_VOCAB_SIZE,")
        add("                      embed_weight,")
        add("                      NULL,")
        add("                      embed_out,")
        add(f"                      {safe_name}_EMBED_DIM,")
        add("                      aligned_embed_dim,")
        add("                      1,")
        add("                      0);")
        add()
        add(f"    for (int layer_id = 0; layer_id < {safe_name}_NUM_LAYERS; ++layer_id) {{")
        add(f"        {safe_name_lower}_layer_forward_decode_token(")
        add("            model,")
        add("            layer_id,")
        add("            token_index,")
        add("            aligned_embed_dim,")
        add("            aligned_head_dim,")
        add("            aligned_intermediate_dim,")
        add("            aligned_context_window);")
        add("    }")
        add()
        add(f"    float *last_hidden = {safe_name}_PTR(model, {safe_name}_LAYERS[{safe_name}_NUM_LAYERS - 1].output);")
        add(f"    float *final_ln_weight = {safe_name}_PTR(model, {safe_name}_FOOTER.final_ln_weight);")
        add(f"    float *final_out = {safe_name}_PTR(model, {safe_name}_FOOTER.final_output);")
        add("    rmsnorm_forward(last_hidden,")
        add("                   final_ln_weight,")
        add("                   final_out,")
        add("                   NULL,")
        add("                   1,")
        add(f"                   {safe_name}_EMBED_DIM,")
        add("                   aligned_embed_dim,")
        add(f"                   {config.get('rms_norm_eps', 1e-6)}f);")
        add()
        add(f"    float *lm_head = {safe_name}_PTR(model, {safe_name}_FOOTER.lm_head_weight);")
        add(f"    float *logits = {safe_name}_PTR(model, {safe_name}_FOOTER.logits);")
        add("    gemm_blocked_serial(final_out,")
        add("                        lm_head,")
        add("                        NULL,")
        add("                        logits,")
        add("                        1,")
        add(f"                        {safe_name}_VOCAB_SIZE,")
        add("                        aligned_embed_dim);")
        add("}")
        add()

        add("/* ============================================================================")
        add(" * FORWARD PASS (DECODE)")
        add(" * ============================================================================ */")
        add()
        add(f"void {safe_name_lower}_forward(")
        add(f"    {safe_name}Model *model,")
        add("    const int *tokens,")
        add("    int num_tokens")
        add(") {")
        add("    if (!model || !tokens || num_tokens <= 0) {")
        add("        return;")
        add("    }")
        add()
        add("    for (int i = 0; i < num_tokens; ++i) {")
        add(f"        {safe_name_lower}_decode_token(model, tokens + i, i);")
        add("    }")
        add("}")
        add()

        add("/* Explicit decode API for cache-aware generation */")
        add(f"void {safe_name_lower}_decode({safe_name}Model *model, const int *token, int token_index) {{")
        add(f"    {safe_name_lower}_decode_token(model, token, token_index);")
        add("}")
        add()

    if emit_main:
        add("/* ============================================================================")
        add(" * STANDALONE MAIN (STUB)")
        add(" * ============================================================================ */")
        add()
        add("int main(int argc, char **argv) {")
        add("    printf(\"%s: generated runtime (%s)\\n\", argv[0], \"" + mode + "\");")
        add(f"    printf(\"total_bytes=%zu weight_bytes=%zu activation_bytes=%zu\\n\",")
        add(f"           (size_t){safe_name}_TOTAL_BYTES,")
        add(f"           (size_t){safe_name}_WEIGHT_BYTES,")
        add(f"           (size_t){safe_name}_ACTIVATION_BYTES);")
        add("    if (argc > 1 && strcmp(argv[1], \"--alloc\") == 0) {")
        add(f"        {safe_name}Model model = {{0}};")
        add(f"        if ({safe_name_lower}_model_allocate(&model) != 0) {{")
        add("            return 1;")
        add("        }")
        add(f"        {safe_name_lower}_precompute_rope(&model);")
        add(f"        {safe_name_lower}_model_free(&model);")
        add("        return 0;")
        add("    }")
        add("    fprintf(stderr, \"Run with --alloc to allocate buffers.\\n\");")
        add("    fprintf(stderr, \"Load weights and call *_forward() from your host app.\\n\");")
        add("    return 0;")
        add("}")
        add()

    with open(output_path, "w") as f:
        f.write("\n".join(lines))

    print(f"[.c] Written: {output_path}")
