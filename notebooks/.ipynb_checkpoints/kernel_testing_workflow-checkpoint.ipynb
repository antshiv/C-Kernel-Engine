{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C Kernel Engine: PyTorch-Based Kernel Testing\n",
    "\n",
    "This notebook shows how to:\n",
    "\n",
    "1. Build the `libckernel_engine.so` shared library.\n",
    "2. Call C kernels (LayerNorm, GELU, softmax, GEMM) from Python via `ctypes`.\n",
    "3. Compare their outputs and gradients against PyTorch references.\n",
    "\n",
    "The workflow is:\n",
    "\n",
    "- **Write/modify a kernel** in `src/kernels/*.c`.\n",
    "- `make` to rebuild the shared library.\n",
    "- Run the tests from this notebook, which re-use the helpers in `unittest/*.py`.\n",
    "\n",
    "You can keep this notebook open while iterating on C code to get fast feedback on correctness (max diff, RMSE, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If this notebook is opened from the C-Kernel-Engine root, this cell\n",
    "# will show the expected layout.\n",
    "import os, subprocess, sys\n",
    "\n",
    "print(\"CWD:\", os.getcwd())\n",
    "print(\"Contents:\", os.listdir(\".\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Build the Shared Library\n",
    "\n",
    "This runs the `Makefile` to produce `libckernel_engine.so`.\n",
    "Re-run this cell after changing any C kernel implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "set -e\n",
    "make"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. LayerNorm: Forward and Backward Tests\n",
    "\n",
    "We reuse the helpers from `unittest/test_layernorm.py`:\n",
    "\n",
    "- Forward: compare naive, rolled, unrolled C kernels vs `torch.layer_norm`.\n",
    "- Backward: compare C `layernorm_backward_kernel` vs PyTorch autograd for `x`, `gamma`, `beta`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "from unittest import test_layernorm\n",
    "\n",
    "# Reload in case you've edited unittest/test_layernorm.py\n",
    "importlib.reload(test_layernorm)\n",
    "\n",
    "print(\"=== LayerNorm forward tests ===\")\n",
    "test_layernorm.run_single_test(T=32, D=128)\n",
    "\n",
    "print(\"\\n=== LayerNorm backward tests ===\")\n",
    "test_layernorm.run_backward_test(T=16, D=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. GELU Forward Test\n",
    "\n",
    "Compare `gelu_fast_inplace` vs PyTorch's `F.gelu(approximate=\"tanh\")`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unittest import test_gelu\n",
    "importlib.reload(test_gelu)\n",
    "\n",
    "print(\"=== GELU forward test ===\")\n",
    "test_gelu.run_single_test(N=1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Causal Softmax: Forward and Backward\n",
    "\n",
    "Forward:\n",
    "- `causal_softmax_head_major` vs a PyTorch row-wise masked softmax.\n",
    "\n",
    "Backward:\n",
    "- `backward_causal_softmax_head_major` vs a pure-PyTorch Jacobian-vector product implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unittest import test_softmax, test_softmax_backward\n",
    "importlib.reload(test_softmax)\n",
    "importlib.reload(test_softmax_backward)\n",
    "\n",
    "print(\"=== Softmax forward test ===\")\n",
    "test_softmax.run_single_test(H=2, T=8)\n",
    "\n",
    "print(\"\\n=== Softmax backward test ===\")\n",
    "test_softmax_backward.run_single_test(H=2, T=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. GEMM Tests for LLM Shapes\n",
    "\n",
    "We test all GEMM variants (`naive`, `avx512`, `fine_grained`, `blocked_serial`) against PyTorch matmul + bias, for shapes that matter to LLMs:\n",
    "\n",
    "- `[T, D] · [D, 4D]` (MLP1)\n",
    "- `[T, d] · [T, d]` (QK^T-style)\n",
    "- `[T, T] · [T, d]` (SV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unittest import test_gemm\n",
    "importlib.reload(test_gemm)\n",
    "\n",
    "print(\"=== GEMM tests ===\")\n",
    "test_gemm.run_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Exploring Intermediate Values\n",
    "\n",
    "Because these tests are just Python modules, you can import them and inspect intermediate tensors directly. For example, to inspect LayerNorm intermediate mean/rstd:\n",
    "\n",
    "```python\n",
    "import torch\n",
    "from unittest import test_layernorm\n",
    "\n",
    "x = torch.randn(4, 8)\n",
    "gamma = torch.randn(8)\n",
    "beta = torch.randn(8)\n",
    "\n",
    "out, mean, rstd = test_layernorm.run_c_layernorm_naive(x, gamma, beta)\n",
    "print(\"mean:\", mean)\n",
    "print(\"rstd:\", rstd)\n",
    "```\n",
    "\n",
    "You can follow the same pattern for any new kernel you add (e.g., RMSNorm):\n",
    "\n",
    "1. Implement the kernel in `src/kernels/*.c` and expose it in `include/ckernel_engine.h`.\n",
    "2. Add a small `unittest/test_<kernel>.py` that:\n",
    "   - Builds random tensors.\n",
    "   - Computes a PyTorch reference.\n",
    "   - Calls the C kernel and reports max diff / RMSE.\n",
    "3. Add a section here that imports and runs that test.\n",
    "\n",
    "This keeps the workflow interactive and repeatable as you design new kernels."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

